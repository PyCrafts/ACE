{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"================================ ACE is a detection system and automation framework. At ACE\u2019s foundation are its engines for recursive analysis and its delivery of an intuitive presentation to the analyst. ACE's goal is to reduce the analyst's time-to-disposition to as close to zero as humanly possible. While ACE is a powerful detection system, and does have built in detections, ACE does not ship with all of the yara signatures and intel detections that teams have built around it. However, ACE makes it easy to load your own yara signatures and atomic indicator detections. Alerts are sent to ACE, and ACE handles the ordinary, manual, redundant, and repetitive tasks of collecting, combining, and relating data. ACE will then contextually and intuitively present all the right data to the analyst, allowing for a quick, high confidence determination to be made. Got some new analysis that can be automated? Awesome! Add your automation, and let ACE keep working for you. For the most part, custom hunting tools send alerts to ACE using ACE\u2019s client library (API wrapper). ACE then gets to work by taking whatever detectable conditions it\u2019s given and spiraling out through its recursive analysis of observables, hitting as many detection points as possible across the attack surface. Regardless of skill level, ACE greatly reduces the time it takes an analyst to make a high confidence determination, or as we call it, disposition. This reduction in time-to-disposition, coupled with the appropriate hunting and tuning mindset, means that security teams can greatly increase the attack surface they cover, all while utilizing the same amount of analyst time and practically eliminating alert fatigue. Optimization good, alert fatigue bad. Major Features \u00b6 ACE is the implementation of a proven detection strategy, a framework for automating analysis, a central platform to launch and manage incident response activates, an email scanner, and much more. Email Scanning Recursive File Scanning URL Crawling and Content Caching Intuitive Alert Presentation Recursive Data Analysis & Correlation Central Analyst Interface Event/Incident management Intel Ingestion Modular Design for extending automation","title":"Home"},{"location":"#major-features","text":"ACE is the implementation of a proven detection strategy, a framework for automating analysis, a central platform to launch and manage incident response activates, an email scanner, and much more. Email Scanning Recursive File Scanning URL Crawling and Content Caching Intuitive Alert Presentation Recursive Data Analysis & Correlation Central Analyst Interface Event/Incident management Intel Ingestion Modular Design for extending automation","title":"Major Features"},{"location":"development/","text":"Development Guide \u00b6 A guide to help with ACE development. Collectors \u00b6 Parsing Data \u00b6 ACE has a couple built in classes that can aid you in parsing and transforming data into a format that can be used to submit an alert to your collector. RegexObservableParserGroup The RegexObservableParserGroup can be used to add various regular expressions ``` {.sourceCode .python} from saq.constants import ( F_URL, F_IPV4, F_IPV4_CONVERSATION, F_IPV4_FULL_CONVERSATION, DIRECTIVE_CRAWL, ) from saq.util import RegexObservableParserGroup sample_log = \"dst_ip: 10.0.0.2, port: 8080, src_ip: 10.0.0.1, port: 5555, url: https://hello.local/malicious/payload.exe\\n\" Tags you add here will be applied to all observables you find unless \u00b6 you override the tags value at the individual parser being added. \u00b6 parser_group = RegexObservableParserGroup(tags=['my_custom_parser']) This will capture ALL matching strings. If any are duplicates, \u00b6 they will be filtered out when you generate the observables. \u00b6 parser_group.add(r'ip: ([0-9.]+)', F_IPV4) You can also add multiple capture groups and determine in which \u00b6 Order the items are extracted. \u00b6 For example, the source IP comes second in our test string, but the \u00b6 F_IPV4_CONVERSATION is in this format: src-ip_dst-ip. \u00b6 Note the capture groups are in reverse order to accomodate: \u00b6 parser_group.add(r'ip: ([0-9.]+).*ip: ([0-9.]+)', F_IPV4_CONVERSATION, capture_groups=[2, 1]) You can also change the delimiter for how all the capture groups \u00b6 are joined together. For example, the F_IPV4_FULL_CONVERSATION \u00b6 is delimited by colons. \u00b6 parser_group.add( r'dst_ip: ([0-9.]+), port: ([0-9]+), src_ip: ([0-9.]+), port: ([0-9]+)', F_IPV4_FULL_CONVERSATION, delimiter=':', capture_groups=[3, 4, 1, 2] ) You can also add directives to control analysis/actions taken on your \u00b6 observable. \u00b6 parser_group.add(r'url: ([^\\n]+)', F_URL, directives=[DIRECTIVE_CRAWL]) Once you've added your parsers, you can parse the data: ``` {.sourceCode .python} parser_group.parse_content(sample_log) Then you can access the observables: ``` {.sourceCode .python} observables = parser_group.observables The extractions from this example would create the following list of dictionairies which are in an appropriate format to be submitted to the collector: ``` {.sourceCode .python} [ { \"type\": \"ipv4\", \"value\": \"10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4\", \"value\": \"10.0.0.1\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_conversation\", \"value\": \"10.0.0.1_10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_full_conversation\", \"value\": \"10.0.0.1:5555:10.0.0.2:8080\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"url\", \"value\": \"https://hello.local/malicious/payload.exe\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [ \"crawl\" ] } ] What if you've created your own observable type? Or maybe you want to change the way the parser groups work. You can make subclass the saq.util.RegexObservableParser , override the parsing logic, and then pass it into the parser group: ``` {.sourceCode .python} from saq.constants import F_CUSTOM_TYPE from saq.util import RegexObservableParserGroup, RegexObservableParser class MyParser(RegexObservableParser): def init (self, args, kwargs): super(). init ( args, **kwargs) # override the RegexObservableParser.parse() method def parse(self, text): # My custom parsing logic pass parser_group = RegexObservableParserGroup() parser_group.add(r'ip: ([0-9.]+)', override_class=MyParser) parser_group.parse_content(my_log) When you're ready to submit to the collector: ``` {.sourceCode .python} observables = parser_group.observables from saq.collectors import Submission from saq.constanants import ANALYSIS_MODE_CORRELATION submission = Submission( description=\"My custom alert\", analysis_mode=ANALYSIS_MODE_CORRELATION, tool = 'my custom tool', tool_instance = 'my custom tool instance', type = 'custom_type', event_time = 'datetime_from_alert', details = [], observables = observables, tags=[], files=[], )","title":"Overview"},{"location":"development/#development-guide","text":"A guide to help with ACE development.","title":"Development Guide"},{"location":"development/#collectors","text":"","title":"Collectors"},{"location":"development/#parsing-data","text":"ACE has a couple built in classes that can aid you in parsing and transforming data into a format that can be used to submit an alert to your collector. RegexObservableParserGroup The RegexObservableParserGroup can be used to add various regular expressions ``` {.sourceCode .python} from saq.constants import ( F_URL, F_IPV4, F_IPV4_CONVERSATION, F_IPV4_FULL_CONVERSATION, DIRECTIVE_CRAWL, ) from saq.util import RegexObservableParserGroup sample_log = \"dst_ip: 10.0.0.2, port: 8080, src_ip: 10.0.0.1, port: 5555, url: https://hello.local/malicious/payload.exe\\n\"","title":"Parsing Data"},{"location":"development/#tags-you-add-here-will-be-applied-to-all-observables-you-find-unless","text":"","title":"Tags you add here will be applied to all observables you find unless"},{"location":"development/#you-override-the-tags-value-at-the-individual-parser-being-added","text":"parser_group = RegexObservableParserGroup(tags=['my_custom_parser'])","title":"you override the tags value at the individual parser being added."},{"location":"development/#this-will-capture-all-matching-strings-if-any-are-duplicates","text":"","title":"This will capture ALL matching strings. If any are duplicates,"},{"location":"development/#they-will-be-filtered-out-when-you-generate-the-observables","text":"parser_group.add(r'ip: ([0-9.]+)', F_IPV4)","title":"they will be filtered out when you generate the observables."},{"location":"development/#you-can-also-add-multiple-capture-groups-and-determine-in-which","text":"","title":"You can also add multiple capture groups and determine in which"},{"location":"development/#order-the-items-are-extracted","text":"","title":"Order the items are extracted."},{"location":"development/#for-example-the-source-ip-comes-second-in-our-test-string-but-the","text":"","title":"For example, the source IP comes second in our test string, but the"},{"location":"development/#f_ipv4_conversation-is-in-this-format-src-ip_dst-ip","text":"","title":"F_IPV4_CONVERSATION is in this format: src-ip_dst-ip."},{"location":"development/#note-the-capture-groups-are-in-reverse-order-to-accomodate","text":"parser_group.add(r'ip: ([0-9.]+).*ip: ([0-9.]+)', F_IPV4_CONVERSATION, capture_groups=[2, 1])","title":"Note the capture groups are in reverse order to accomodate:"},{"location":"development/#you-can-also-change-the-delimiter-for-how-all-the-capture-groups","text":"","title":"You can also change the delimiter for how all the capture groups"},{"location":"development/#are-joined-together-for-example-the-f_ipv4_full_conversation","text":"","title":"are joined together. For example, the F_IPV4_FULL_CONVERSATION"},{"location":"development/#is-delimited-by-colons","text":"parser_group.add( r'dst_ip: ([0-9.]+), port: ([0-9]+), src_ip: ([0-9.]+), port: ([0-9]+)', F_IPV4_FULL_CONVERSATION, delimiter=':', capture_groups=[3, 4, 1, 2] )","title":"is delimited by colons."},{"location":"development/#you-can-also-add-directives-to-control-analysisactions-taken-on-your","text":"","title":"You can also add directives to control analysis/actions taken on your"},{"location":"development/#observable","text":"parser_group.add(r'url: ([^\\n]+)', F_URL, directives=[DIRECTIVE_CRAWL]) Once you've added your parsers, you can parse the data: ``` {.sourceCode .python} parser_group.parse_content(sample_log) Then you can access the observables: ``` {.sourceCode .python} observables = parser_group.observables The extractions from this example would create the following list of dictionairies which are in an appropriate format to be submitted to the collector: ``` {.sourceCode .python} [ { \"type\": \"ipv4\", \"value\": \"10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4\", \"value\": \"10.0.0.1\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_conversation\", \"value\": \"10.0.0.1_10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_full_conversation\", \"value\": \"10.0.0.1:5555:10.0.0.2:8080\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"url\", \"value\": \"https://hello.local/malicious/payload.exe\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [ \"crawl\" ] } ] What if you've created your own observable type? Or maybe you want to change the way the parser groups work. You can make subclass the saq.util.RegexObservableParser , override the parsing logic, and then pass it into the parser group: ``` {.sourceCode .python} from saq.constants import F_CUSTOM_TYPE from saq.util import RegexObservableParserGroup, RegexObservableParser class MyParser(RegexObservableParser): def init (self, args, kwargs): super(). init ( args, **kwargs) # override the RegexObservableParser.parse() method def parse(self, text): # My custom parsing logic pass parser_group = RegexObservableParserGroup() parser_group.add(r'ip: ([0-9.]+)', override_class=MyParser) parser_group.parse_content(my_log) When you're ready to submit to the collector: ``` {.sourceCode .python} observables = parser_group.observables from saq.collectors import Submission from saq.constanants import ANALYSIS_MODE_CORRELATION submission = Submission( description=\"My custom alert\", analysis_mode=ANALYSIS_MODE_CORRELATION, tool = 'my custom tool', tool_instance = 'my custom tool instance', type = 'custom_type', event_time = 'datetime_from_alert', details = [], observables = observables, tags=[], files=[], )","title":"observable."},{"location":"development/api/","text":"ACE API Examples \u00b6 Let's go through a few examples using the ACE API. We will specifically use the ace_api python library. Connect to a Server \u00b6 By default, the ace_api library will attempt to connect to localhost. Use the ace_api.set_default_remote_host function to have the library connect to a different server. The OS's certificate store is used to validate the server. See ace_api.set_default_ssl_ca_path to change this behavior. >>> import ace_api >>> server = 'ace.integraldefense.com' >>> ace_api.set_default_remote_host(server) >>> ace_api.ping() {'result': 'pong'} You can over-ride this default in the ace_api.Analysis class with the ace_api.Analysis.set_remote_host method and you can also manually specify a remote host with any submit. >>> analysis = ace_api.Analysis('this is the analysis description') >>> analysis.remote_host 'ace.integraldefense.com' >>> analysis.set_remote_host('something.else.com').remote_host 'something.else.com' >>> ace_api.default_remote_host 'ace.integraldefense.com' If your ACE instance is listening on a port other than 443, specify it like so: >>> ace_api.set_default_remote_host('ace.integraldefense.com:24443') >>> ace_api.default_remote_host 'ace.integraldefense.com:24443' Submitting data to ACE \u00b6 You should submit data to ace by first creating an Analysis object and loading it with the data you want to submit for analysis and/or correlation. The below examples show how to perform some common submissions. Submit a File \u00b6 Say we have a suspect file in our current working director named \"Business.doc\" that we want to submit to ACE. First, we create an analysis object and then we pass the path to the file to the ace_api.Analysis.add_file method. We will also include some tags and check the status (ace_api.Analysis.status) of the analysis as ACE works on the submission. >>> path_to_file = 'Business.doc' >>> analysis.add_file(path_to_file) <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.add_tag('Business.doc').add_tag('suspicious doc') <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.submit() <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.status 'NEW' >>> analysis.status 'ANALYZING' >>> analysis.status 'COMPLETE (Alerted with 8 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=137842ac-9d53-4a25-8066-ad2a1f6cfa17 Submit a URL \u00b6 Two examples of submitting a URL to ACE follow. The first example shows how to submit a URL by adding the URL as an observable to an Analysis object. This also allows us to demontrate the use of directives. The second example shows how simple it is to submit a URL for analysis directly to Cloudphish. As an observable \u00b6 You can submit as many observables <observable> as you desire in a submission to ACE, but they won't neccessarily get passed to every analysis module that can work on them by default. This is the case for URL observables, which by themselves, require the crawl directive to tell ACE you want to download the conent from the URL for further analysis. Submititing a request for a suspicious URL to be analyzed, note the use of the crawl directive and how to get a list of the valid directives. >>> suspicious_url = 'http://davidcizek.cz/Invoice/ifKgg-jrzA_PvC-a7' >>> analysis = ace_api.Analysis('Suspicious URL') >>> analysis.add_tag('suspicious_url') <ace_api.Analysis object at 0x7f23d57e7588> >>> for d in ace_api.get_valid_directives()['result']: ... if d['name'] == 'crawl': ... print(d['description']) ... crawl the URL >>> analysis.add_url(suspicious_url, directives=['crawl']).submit() <ace_api.Analysis object at 0x7f23d57e7588> >>> analysis.status 'COMPLETE (Alerted with 9 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=de66b2d3-f273-4bdd-a05b-771ecf5c8a76 Using Cloudphish \u00b6 If you just want ACE to analyze a single URL, it's best to submit directly to Cloudphish. In this example, a URL is submitted to cloudphish that cloudphish has never seen before and a 'NEW' status is returned. After cloudphish has finished analyzing the URL, the status changes to 'ANALYZED' and the analysis_result tells us at least one detection was found (as we alerted). >>> another_url = 'http://medicci.ru/myATT/tu8794_QcbkoEsv_Xw20pYh7ij' >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'NEW' >>> # Query again, a moment later: ... >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'ANALYZED' >>> cp_result['analysis_result'] 'ALERT' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(ace_api.default_remote_host, cp_result['uuid']) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=732ec396-ce20-463f-82b0-6b043b07f941 Forcing Alert Creation \u00b6 By default, ACE alerts are only created if an detection is made in the initially submitted analysis. You can force alert creation by changing the default analysis mode from analysis to correlation . This is accomplished like so: >>> analysis = ace_api.Analysis('This is an analysis with no detections', analysis_mode='correlation') >>> analysis.submit() <ace_api.Analysis object at 0x7fbe81af66a0> >>> analysis.status 'COMPLETE (Alerted with 0 detections)' Downloading Cloudphish Results \u00b6 Cloudphish keeps a cache of the URL content it downloads. In this example we will download the results of the URL submitted in the previous example, which in this case is a malicious word document. >>> ace_api.cloudphish_download(another_url, output_path='cp_result.raw') True >>> os.path.exists('cp_result.raw') True Downloading an Alert \u00b6 You can use the ace_api.download function to download an entire Alert. Below, we download an entire Alert and have it written to a directory named by the Alert's UUID.: >>> uuid = cp_result['uuid'] >>> >>> uuid '732ec396-ce20-463f-82b0-6b043b07f941' >>> ace_api.download(uuid, target_dir=uuid) Now, there is a new directory named '732ec396-ce20-463f-82b0-6b043b07f941' in our current working directory that contians all of the files and data from the alert with uuid 732ec396-ce20-463f-82b0-6b043b07f941. Use the ace_api.load_analysis function to load an alert into a new Analysis object. ACE API \u00b6 Python Library \u00b6 A python library exits for intereacting with the ACE API. You can install it wil pip: pip3 install ace_api . Common API \u00b6 Alert API \u00b6 submit \u00b6 Submits a new alert to ACE. These go directly into the correlation engine for analysis and show up to analysts as alerts. Parameters: alert - JSON dict with the following schema : { 'tool': tool_name, 'tool_instance': tool_instance_name, 'type': alert_type, 'description': alert description, 'event_time': time of the alert/event (in %Y-%m-%dT%H:%M:%S.%f%z format), 'details': free-form JSON dict of anything you want to include, 'observables': (see below), 'tags': a list of tags to add to the alert, } The observables field is a list of zero or more dicts with the following format : { 'type': The type of the observable, 'value': The value of the observable, 'time': The optional time of the observable (can be null), 'tags': Optional list of tags to add to the observable, 'directives': Optional list of directives to add to the observable, } To attach files to the alert use the field named file .","title":"API"},{"location":"development/api/#ace-api-examples","text":"Let's go through a few examples using the ACE API. We will specifically use the ace_api python library.","title":"ACE API Examples"},{"location":"development/api/#connect-to-a-server","text":"By default, the ace_api library will attempt to connect to localhost. Use the ace_api.set_default_remote_host function to have the library connect to a different server. The OS's certificate store is used to validate the server. See ace_api.set_default_ssl_ca_path to change this behavior. >>> import ace_api >>> server = 'ace.integraldefense.com' >>> ace_api.set_default_remote_host(server) >>> ace_api.ping() {'result': 'pong'} You can over-ride this default in the ace_api.Analysis class with the ace_api.Analysis.set_remote_host method and you can also manually specify a remote host with any submit. >>> analysis = ace_api.Analysis('this is the analysis description') >>> analysis.remote_host 'ace.integraldefense.com' >>> analysis.set_remote_host('something.else.com').remote_host 'something.else.com' >>> ace_api.default_remote_host 'ace.integraldefense.com' If your ACE instance is listening on a port other than 443, specify it like so: >>> ace_api.set_default_remote_host('ace.integraldefense.com:24443') >>> ace_api.default_remote_host 'ace.integraldefense.com:24443'","title":"Connect to a Server"},{"location":"development/api/#submitting-data-to-ace","text":"You should submit data to ace by first creating an Analysis object and loading it with the data you want to submit for analysis and/or correlation. The below examples show how to perform some common submissions.","title":"Submitting data to ACE"},{"location":"development/api/#submit-a-file","text":"Say we have a suspect file in our current working director named \"Business.doc\" that we want to submit to ACE. First, we create an analysis object and then we pass the path to the file to the ace_api.Analysis.add_file method. We will also include some tags and check the status (ace_api.Analysis.status) of the analysis as ACE works on the submission. >>> path_to_file = 'Business.doc' >>> analysis.add_file(path_to_file) <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.add_tag('Business.doc').add_tag('suspicious doc') <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.submit() <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.status 'NEW' >>> analysis.status 'ANALYZING' >>> analysis.status 'COMPLETE (Alerted with 8 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=137842ac-9d53-4a25-8066-ad2a1f6cfa17","title":"Submit a File"},{"location":"development/api/#submit-a-url","text":"Two examples of submitting a URL to ACE follow. The first example shows how to submit a URL by adding the URL as an observable to an Analysis object. This also allows us to demontrate the use of directives. The second example shows how simple it is to submit a URL for analysis directly to Cloudphish.","title":"Submit a URL"},{"location":"development/api/#as-an-observable","text":"You can submit as many observables <observable> as you desire in a submission to ACE, but they won't neccessarily get passed to every analysis module that can work on them by default. This is the case for URL observables, which by themselves, require the crawl directive to tell ACE you want to download the conent from the URL for further analysis. Submititing a request for a suspicious URL to be analyzed, note the use of the crawl directive and how to get a list of the valid directives. >>> suspicious_url = 'http://davidcizek.cz/Invoice/ifKgg-jrzA_PvC-a7' >>> analysis = ace_api.Analysis('Suspicious URL') >>> analysis.add_tag('suspicious_url') <ace_api.Analysis object at 0x7f23d57e7588> >>> for d in ace_api.get_valid_directives()['result']: ... if d['name'] == 'crawl': ... print(d['description']) ... crawl the URL >>> analysis.add_url(suspicious_url, directives=['crawl']).submit() <ace_api.Analysis object at 0x7f23d57e7588> >>> analysis.status 'COMPLETE (Alerted with 9 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=de66b2d3-f273-4bdd-a05b-771ecf5c8a76","title":"As an observable"},{"location":"development/api/#using-cloudphish","text":"If you just want ACE to analyze a single URL, it's best to submit directly to Cloudphish. In this example, a URL is submitted to cloudphish that cloudphish has never seen before and a 'NEW' status is returned. After cloudphish has finished analyzing the URL, the status changes to 'ANALYZED' and the analysis_result tells us at least one detection was found (as we alerted). >>> another_url = 'http://medicci.ru/myATT/tu8794_QcbkoEsv_Xw20pYh7ij' >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'NEW' >>> # Query again, a moment later: ... >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'ANALYZED' >>> cp_result['analysis_result'] 'ALERT' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(ace_api.default_remote_host, cp_result['uuid']) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=732ec396-ce20-463f-82b0-6b043b07f941","title":"Using Cloudphish"},{"location":"development/api/#forcing-alert-creation","text":"By default, ACE alerts are only created if an detection is made in the initially submitted analysis. You can force alert creation by changing the default analysis mode from analysis to correlation . This is accomplished like so: >>> analysis = ace_api.Analysis('This is an analysis with no detections', analysis_mode='correlation') >>> analysis.submit() <ace_api.Analysis object at 0x7fbe81af66a0> >>> analysis.status 'COMPLETE (Alerted with 0 detections)'","title":"Forcing Alert Creation"},{"location":"development/api/#downloading-cloudphish-results","text":"Cloudphish keeps a cache of the URL content it downloads. In this example we will download the results of the URL submitted in the previous example, which in this case is a malicious word document. >>> ace_api.cloudphish_download(another_url, output_path='cp_result.raw') True >>> os.path.exists('cp_result.raw') True","title":"Downloading Cloudphish Results"},{"location":"development/api/#downloading-an-alert","text":"You can use the ace_api.download function to download an entire Alert. Below, we download an entire Alert and have it written to a directory named by the Alert's UUID.: >>> uuid = cp_result['uuid'] >>> >>> uuid '732ec396-ce20-463f-82b0-6b043b07f941' >>> ace_api.download(uuid, target_dir=uuid) Now, there is a new directory named '732ec396-ce20-463f-82b0-6b043b07f941' in our current working directory that contians all of the files and data from the alert with uuid 732ec396-ce20-463f-82b0-6b043b07f941. Use the ace_api.load_analysis function to load an alert into a new Analysis object.","title":"Downloading an Alert"},{"location":"development/api/#ace-api","text":"","title":"ACE API"},{"location":"development/api/#python-library","text":"A python library exits for intereacting with the ACE API. You can install it wil pip: pip3 install ace_api .","title":"Python Library"},{"location":"development/api/#common-api","text":"","title":"Common API"},{"location":"development/api/#alert-api","text":"","title":"Alert API"},{"location":"development/api/#submit","text":"Submits a new alert to ACE. These go directly into the correlation engine for analysis and show up to analysts as alerts. Parameters: alert - JSON dict with the following schema : { 'tool': tool_name, 'tool_instance': tool_instance_name, 'type': alert_type, 'description': alert description, 'event_time': time of the alert/event (in %Y-%m-%dT%H:%M:%S.%f%z format), 'details': free-form JSON dict of anything you want to include, 'observables': (see below), 'tags': a list of tags to add to the alert, } The observables field is a list of zero or more dicts with the following format : { 'type': The type of the observable, 'value': The value of the observable, 'time': The optional time of the observable (can be null), 'tags': Optional list of tags to add to the observable, 'directives': Optional list of directives to add to the observable, } To attach files to the alert use the field named file .","title":"submit"},{"location":"development/history/","text":"Developer README \u00b6 This document explains the reasons behind some of the stranger design decisions made for this project. SAQ = ACE \u00b6 When the project first started we called it the Simple Alert Queue (SAQ). It was later renamed to the Analysis Correlation Engine (ACE). There are still a lot of references to SAQ left, including the name of the core library ( import saq ) and the SAQ_HOME environment variable. Eveything was initially command line driven. \u00b6 The original UI of the project was CLI. So there's still a lot of that left. Most of what you can do can also be done via the command line, including full analysis of observables. Along those lines, it was also meant to be able to be executed from any directory. This is probably no longer true, but there are a number of times where the code assumes it is running in some other directory. This was an internal project. \u00b6 There's a number of basic things that you would expect would exist that don't. For example, there's no way to manage users from the GUI. It must be done from the command line. And even then, there's no support to delete a user. We didn't have any turnover for 5 years so this was never a requirement. And the along those lines there's little effort put into account security internally. There are no \"roles\" or \"administrators\". The database came later. \u00b6 Very little of the analysis data is stored in the database. From the beginning of the project I wanted the data to be stored in a schema-less JSON structure on the filesystem. This would allow analysts to simply grep the files for whatever they were looking for. I (reluctantly) looked at MongoDB as a way to index the data and speed up the searches. This was quickly abandoned (it was slowing down development for various reasons.) Later when the GUI was added to the project we started storing data in MySQL. I knew that we would be modifying this system a lot. So trying to create a database schema that encompassed everything we would ever want to do was not realisitic. Making major changes to large database schemas is no easy task. Today the database is used to manage the workload of the collectors and engines, and to provide the GUI (and API) for the analysts. The data.json JSON files that hold the results of the analysis are actually the official records of the analysis. The database is kept in sync with these files. At some point it would make sense to index these JSON files in a system like Elasticsearch. Unit testing. \u00b6 My one regret with this project was not creating unit tests as I went. I didn't start adding unit tests until we were \\~4 years into the project. Unit test coverage is not what it shoud be, but I would expect that to improve over time. Final words. \u00b6 I think it's worth noting that this project was created to enable and improve our analysts. We were not designing a product . We were also moving as quickly as we saw threat actors change tactics. As soon as we saw a new techique being used, we would quickly implement a feature to ACE that would allow us to detect that. So there's a number of places where the code looks hastily thrown together. Hopefully this file helps to explain some of the oddness you may see in the code.","title":"History"},{"location":"development/history/#developer-readme","text":"This document explains the reasons behind some of the stranger design decisions made for this project.","title":"Developer README"},{"location":"development/history/#saq-ace","text":"When the project first started we called it the Simple Alert Queue (SAQ). It was later renamed to the Analysis Correlation Engine (ACE). There are still a lot of references to SAQ left, including the name of the core library ( import saq ) and the SAQ_HOME environment variable.","title":"SAQ = ACE"},{"location":"development/history/#eveything-was-initially-command-line-driven","text":"The original UI of the project was CLI. So there's still a lot of that left. Most of what you can do can also be done via the command line, including full analysis of observables. Along those lines, it was also meant to be able to be executed from any directory. This is probably no longer true, but there are a number of times where the code assumes it is running in some other directory.","title":"Eveything was initially command line driven."},{"location":"development/history/#this-was-an-internal-project","text":"There's a number of basic things that you would expect would exist that don't. For example, there's no way to manage users from the GUI. It must be done from the command line. And even then, there's no support to delete a user. We didn't have any turnover for 5 years so this was never a requirement. And the along those lines there's little effort put into account security internally. There are no \"roles\" or \"administrators\".","title":"This was an internal project."},{"location":"development/history/#the-database-came-later","text":"Very little of the analysis data is stored in the database. From the beginning of the project I wanted the data to be stored in a schema-less JSON structure on the filesystem. This would allow analysts to simply grep the files for whatever they were looking for. I (reluctantly) looked at MongoDB as a way to index the data and speed up the searches. This was quickly abandoned (it was slowing down development for various reasons.) Later when the GUI was added to the project we started storing data in MySQL. I knew that we would be modifying this system a lot. So trying to create a database schema that encompassed everything we would ever want to do was not realisitic. Making major changes to large database schemas is no easy task. Today the database is used to manage the workload of the collectors and engines, and to provide the GUI (and API) for the analysts. The data.json JSON files that hold the results of the analysis are actually the official records of the analysis. The database is kept in sync with these files. At some point it would make sense to index these JSON files in a system like Elasticsearch.","title":"The database came later."},{"location":"development/history/#unit-testing","text":"My one regret with this project was not creating unit tests as I went. I didn't start adding unit tests until we were \\~4 years into the project. Unit test coverage is not what it shoud be, but I would expect that to improve over time.","title":"Unit testing."},{"location":"development/history/#final-words","text":"I think it's worth noting that this project was created to enable and improve our analysts. We were not designing a product . We were also moving as quickly as we saw threat actors change tactics. As soon as we saw a new techique being used, we would quickly implement a feature to ACE that would allow us to detect that. So there's a number of places where the code looks hastily thrown together. Hopefully this file helps to explain some of the oddness you may see in the code.","title":"Final words."},{"location":"installation/","text":"Installation + Adding Data \u00b6 Super fast How-To \u00b6 Clean Ubuntu 18 install. Create username/group ace/ace. Add ace to sudo. Login as user ace. sudo mkdir /opt/ace && sudo chown ace:ace /opt/ace && cd /opt/ace git clone https://github.com/ace-ecosystem/ACE.git . ./installer/source_install source load_environment ./ace add-user username email_address Goto https://127.0.0.1/ace/ or whatever IP address you're using. If you run into certificate / SSL issues, see the Troubleshooting and Help section below. Detailed Installation \u00b6 Install Ubuntu Server 18.04 LST \u00b6 The size specifications for your server need to be based on your needs. At a minimum, the server should have 4 GB RAM and 20 GB storage drive. When installing the server, all of the default configurations are fine. Getting Everything Ready \u00b6 The ace User \u00b6 $ sudo adduser ace $ sudo adduser ace sudo $ sudo su - ace $ sudo chown ace:ace /opt Cloning ACE \u00b6 As the ace user you previously created, cd into /opt and git clone the ace-ecosystem ACE master branch: https://github.com/ace-ecosystem/ACE.git : $ cd /opt $ git clone https://github.com/ace-ecosystem/ACE.git ace Run the Installer \u00b6 With everything ready <get-enviro-ready>, you can now run the ACE installer. Run the installer as the ace user. You will be prompted for the password when certain things are run using sudo. This will take a little while to complete. $ cd /opt/ace $ ./installer/source_install Set Up Environment \u00b6 Next, you will need to load the default environment variables ACE depends on. This load needs to be sourced from bash with the following command: $ source load_environment This should already be added to the ace account bashrc, so the next login should automatically load it. Create Users \u00b6 Users are managed from the ACE command line with the following ace commands: add-user Add a new user to the system. modify-user Modifies an existing user on the system. delete-user Deletes an existing user from the system. Create your first user so that you can log into the ACE GUI: ./ace add-user <username> <email_address> Log into the GUI \u00b6 You should now be able to browse to https://your_ip/ace/ and log into ACE with the user you previously created. Troubleshooting & Help \u00b6 There are a couple snags and gotchas that you can run into when installing ACE. This section will detail a few, but it's still a work in process. SSL Errors \u00b6 You may run into an SSL error that will include the following text: Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),)) This error is less common when running on a local VM; However, it is fairly common when ACE is installed on a server with a domain. For example, AWS EC2 assigns a hostname such as ip-10-10-10-10.ec2.internal to their EC2 instances. Two quick options to fix this error if you are planning on using your ACE machine with the default installation: Add the FQDN of your host as the ServerName in /opt/ace/etc/saq_apache.conf Add the FQDN of your host as a ServerAlias in /opt/ace/etc/saq_apache.conf Then, restart the apache service (authentication required): $ service apache2 restart Example 1: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ip-10-10-10-10.ec2.internal # Rest of the config... Example 2: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ace.local ServerAlias ip-10-10-10-10.ec2.internal # Rest of the config... No Web GUI? \u00b6 Make sure apache2 is running and the /etc/apache2/sites-enabled/ace.conf configuration is loaded. The ace.conf should be a symlink in /etc/apache2/sites-available that points to /opt/ace/etc/saq_apache.conf . Alerts staying in 'NEW' status? \u00b6 Make sure the ACE engine is running. You can do this by running the following: cd /opt/ace && ace service start engine --daemon Start ACE \u00b6 You should now have a working installation, but you need to start the correlation engine. This is accomplished with the ace service start engine --daemon command. Getting Data into ACE \u00b6 A bare-bones ACE install is not going to be very effective without data. You can use the Manual Analysis section to submit observables to ACE. However, you're going to want to turn on some of the additional Integrations and Modules that come with ACE by default. Manual Analysis \u00b6 Via the Manual Analysis page, an analyst can submit an observable for ACE to analyze. Observables can be submitted for analysis via the Manual Analysis page By default, the Insert Date is set to the current time, and the Description is set to 'Manual Correlation'. You can change the description to something meaningful. The Target Company will also be set to default, which should be fine for most ACE installations. Select the type of observable you wish to correlate and then provide the value. Click the Add button to correlate more than one observable type and/or value at a time. Shortly after you've submitted your observable(s) for correlation, you will see your alert appear on the Manage Alerts page with the description you provided. The alert status will change to 'Complete' once ACE is finished performing its analysis. You must currently refresh the Manage Alerts page to see the alert status updates. Using the API \u00b6 ACE has an API that makes it simple to submit data to ACE for analysis and/or correlation. Check out the ACE API Examples and ACE API section for more information.","title":"Overview"},{"location":"installation/#installation-adding-data","text":"","title":"Installation + Adding Data"},{"location":"installation/#super-fast-how-to","text":"Clean Ubuntu 18 install. Create username/group ace/ace. Add ace to sudo. Login as user ace. sudo mkdir /opt/ace && sudo chown ace:ace /opt/ace && cd /opt/ace git clone https://github.com/ace-ecosystem/ACE.git . ./installer/source_install source load_environment ./ace add-user username email_address Goto https://127.0.0.1/ace/ or whatever IP address you're using. If you run into certificate / SSL issues, see the Troubleshooting and Help section below.","title":"Super fast How-To"},{"location":"installation/#detailed-installation","text":"","title":"Detailed Installation"},{"location":"installation/#install-ubuntu-server-1804-lst","text":"The size specifications for your server need to be based on your needs. At a minimum, the server should have 4 GB RAM and 20 GB storage drive. When installing the server, all of the default configurations are fine.","title":"Install Ubuntu Server 18.04 LST"},{"location":"installation/#getting-everything-ready","text":"","title":"Getting Everything Ready"},{"location":"installation/#the-ace-user","text":"$ sudo adduser ace $ sudo adduser ace sudo $ sudo su - ace $ sudo chown ace:ace /opt","title":"The ace User"},{"location":"installation/#cloning-ace","text":"As the ace user you previously created, cd into /opt and git clone the ace-ecosystem ACE master branch: https://github.com/ace-ecosystem/ACE.git : $ cd /opt $ git clone https://github.com/ace-ecosystem/ACE.git ace","title":"Cloning ACE"},{"location":"installation/#run-the-installer","text":"With everything ready <get-enviro-ready>, you can now run the ACE installer. Run the installer as the ace user. You will be prompted for the password when certain things are run using sudo. This will take a little while to complete. $ cd /opt/ace $ ./installer/source_install","title":"Run the Installer"},{"location":"installation/#set-up-environment","text":"Next, you will need to load the default environment variables ACE depends on. This load needs to be sourced from bash with the following command: $ source load_environment This should already be added to the ace account bashrc, so the next login should automatically load it.","title":"Set Up Environment"},{"location":"installation/#create-users","text":"Users are managed from the ACE command line with the following ace commands: add-user Add a new user to the system. modify-user Modifies an existing user on the system. delete-user Deletes an existing user from the system. Create your first user so that you can log into the ACE GUI: ./ace add-user <username> <email_address>","title":"Create Users"},{"location":"installation/#log-into-the-gui","text":"You should now be able to browse to https://your_ip/ace/ and log into ACE with the user you previously created.","title":"Log into the GUI"},{"location":"installation/#troubleshooting-help","text":"There are a couple snags and gotchas that you can run into when installing ACE. This section will detail a few, but it's still a work in process.","title":"Troubleshooting &amp; Help"},{"location":"installation/#ssl-errors","text":"You may run into an SSL error that will include the following text: Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),)) This error is less common when running on a local VM; However, it is fairly common when ACE is installed on a server with a domain. For example, AWS EC2 assigns a hostname such as ip-10-10-10-10.ec2.internal to their EC2 instances. Two quick options to fix this error if you are planning on using your ACE machine with the default installation: Add the FQDN of your host as the ServerName in /opt/ace/etc/saq_apache.conf Add the FQDN of your host as a ServerAlias in /opt/ace/etc/saq_apache.conf Then, restart the apache service (authentication required): $ service apache2 restart Example 1: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ip-10-10-10-10.ec2.internal # Rest of the config... Example 2: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ace.local ServerAlias ip-10-10-10-10.ec2.internal # Rest of the config...","title":"SSL Errors"},{"location":"installation/#no-web-gui","text":"Make sure apache2 is running and the /etc/apache2/sites-enabled/ace.conf configuration is loaded. The ace.conf should be a symlink in /etc/apache2/sites-available that points to /opt/ace/etc/saq_apache.conf .","title":"No Web GUI?"},{"location":"installation/#alerts-staying-in-new-status","text":"Make sure the ACE engine is running. You can do this by running the following: cd /opt/ace && ace service start engine --daemon","title":"Alerts staying in 'NEW' status?"},{"location":"installation/#start-ace","text":"You should now have a working installation, but you need to start the correlation engine. This is accomplished with the ace service start engine --daemon command.","title":"Start ACE"},{"location":"installation/#getting-data-into-ace","text":"A bare-bones ACE install is not going to be very effective without data. You can use the Manual Analysis section to submit observables to ACE. However, you're going to want to turn on some of the additional Integrations and Modules that come with ACE by default.","title":"Getting Data into ACE"},{"location":"installation/#manual-analysis","text":"Via the Manual Analysis page, an analyst can submit an observable for ACE to analyze. Observables can be submitted for analysis via the Manual Analysis page By default, the Insert Date is set to the current time, and the Description is set to 'Manual Correlation'. You can change the description to something meaningful. The Target Company will also be set to default, which should be fine for most ACE installations. Select the type of observable you wish to correlate and then provide the value. Click the Add button to correlate more than one observable type and/or value at a time. Shortly after you've submitted your observable(s) for correlation, you will see your alert appear on the Manage Alerts page with the description you provided. The alert status will change to 'Complete' once ACE is finished performing its analysis. You must currently refresh the Manage Alerts page to see the alert status updates.","title":"Manual Analysis"},{"location":"installation/#using-the-api","text":"ACE has an API that makes it simple to submit data to ACE for analysis and/or correlation. Check out the ACE API Examples and ACE API section for more information.","title":"Using the API"},{"location":"overview/","text":"Some Background \u00b6 The following topics cover some concepts (at a high level) that should be first understood if you're curious about where ACE comes from or the bigger picture of how ACE is meant to be used. I gave a talk on the development of the tool at BSides Cincinnati in 2015 which covers these topics in detail. You can watch his presentation here: Driving Behavior \u00b6 The metric this tool attempts to drive is called time to disposition , or, how long does it take to figure out you're wasting your time? Most people would phrase this as how long does it take to work an alert? but I've taken a different approach to how we look at alerts. Generally, analysts tend to open up whatever alert they're looking at and try to figure out if it's \"bad\" or not. And by \"bad\", they usually mean \"Is this an attack?\", or, \"Is this something I need to worry about?\" Over the years as an analyst, I came to realize that most of the alerts I worked ended up being dispositioned as false positive, with the following definition of what a false postive actually is. False Positive - An alert you're not going to respond to. \u00b6 I think many people define a false positive more in terms of mismatching something, such as a network-based packet alert that pattern matched something, but that pattern matched something it wasn't intended to match. This is obvious. But I would expand that definition even further. For example, suppose you get an alert from your IDS that a remote MySQL exploit was attempted against a server you protect, but that server does not run MySQL. How would you disposition the alert? It was an attack for sure, but it doesn't matter if the target of the attack is not running the software the attack is meant to exploit. It would matter even less if you didn't run MySQL at all. (Note that I would consider the intelligence gained from the attack as another matter entirely and not the focus of this topic.) So when I applied that definition of what a false positive is, I found that almost all of the alerts I worked ended up being false positive. In my presentation at the Cincinnati b-sides, I even stated that 99% of alerts are false positive . For the sake of argument here, we assume this is the case. This changes how alerts are analyzed. If 99% of them are false positive, then what is the chance that what is being analyzed is something to worry about? So rather than answer the question \"Is this bad?\" we answer the question \"Am I wasting my time?\" because 99% of the time that's exactly what is happening! This idea gets some resistance. I've seen many security organizations take the approach of always attempting to minimize the number of false positives their tools are generating, (rightfully) thinking their analysts waste their time working them, and thus the only alerts that should be generated are the true positive alerts that are accurately identifying an attack. I disagree with that and here is why. Algorithms and Metrics \u00b6 An analyst can work a certain number of alerts in a given day. How large that number is depends on a number of factors such as how skilled they are. how good their tools are. how focused they are. how much analysis paralysis has set in (aka alert burn-out.) Say we measure the number of alerts the analysts can work in a given day as measurement W . Now, there are a number of tools deployed that are generating alerts. Say we measure the number of alerts generated in a given day as measurement N . All of the signatures, rules, engines and algorithms they use to generate these alerts gives the network being protected coverage . We define coverage as \"this is all the places and things we're constantly monitoring for evidence of a compromise or attack.\" Say we measure this coverage as measurement C . So now we have tools generating N alerts giving us a coverage of C , with the analysts able to work W of the N alerts. The metric to drive here is C . The higher the value of C , the more difficult it is for an attacker to have a successful attack without being detected. The issue is that an increase in C usually results in an increase in N , but W stays the same. The goal is to always be increasing C while keeping N less than or equal to W . In English that would read as \"always increasing coverage while keeping the number of alerts manageable.\" This is accomplished by a continuous process of hunting, automation and tuning. Hunting is how C increases. You look somewhere you were not looking before. This can be literally anything that makes sense: a new snort rule, a new saved search in splunk, a new script that runs that checks some weird system you have. Anything that would generate new alerts to be analyzed by the analysts. Automation is how the increase to C is actually made permanent. This means running the hunt continuously, forever or until it doesn't make sense any more. For some tools this is natural, for example, a snort rule always runs after it's deployed. But you may need to build something to run that splunk search every so often, or to run that script on a cron job as certain times. Finally, tuning is how to manage the increase in N . This is the action of preventing the tools from generating false positive alerts. This is accomplished in one of two ways: modifying the hunt to avoid the false positives. For example, tweaking the signature to be more specific in the match, or adding additional boolean logic clauses to queries (such as NOT this AND NOT that AND NOT that.) turning off the alert entirely in the case where the hunt is either wrong or not tunable. At this point, we're constantly increasing C by following the process of hunting, automating and tuning, keeping the number of alerts N manageable to a team of analysts that can handle W alerts in a given day. So where does ACE come into play here? It drives the one metric not covered yet: W . ACE increases the number of alerts an analyst can work in a given day. The higher the value of W is, the more aggressive a team can get with C . Teams with a low value of W are easily overwhelmed by very small increases to C . Teams with a high value of W can handle large increases to C . If viewed on a chart over time, the value of N should look more like a sine wave, fluctuating as new hunts are automated and tuning is performed on the old hunts. The value of C should always be rising, even if only gradually. Finally, it's worth noting that in this scenario I'm describing the number of false positive alerts is very close to N , because 99% of all alerts are false positive (assuming our definition.) Thus, the effort to reduce false positive alerts is merely a function of the process, and not a goal in itself.","title":"Overview"},{"location":"overview/#some-background","text":"The following topics cover some concepts (at a high level) that should be first understood if you're curious about where ACE comes from or the bigger picture of how ACE is meant to be used. I gave a talk on the development of the tool at BSides Cincinnati in 2015 which covers these topics in detail. You can watch his presentation here:","title":"Some Background"},{"location":"overview/#driving-behavior","text":"The metric this tool attempts to drive is called time to disposition , or, how long does it take to figure out you're wasting your time? Most people would phrase this as how long does it take to work an alert? but I've taken a different approach to how we look at alerts. Generally, analysts tend to open up whatever alert they're looking at and try to figure out if it's \"bad\" or not. And by \"bad\", they usually mean \"Is this an attack?\", or, \"Is this something I need to worry about?\" Over the years as an analyst, I came to realize that most of the alerts I worked ended up being dispositioned as false positive, with the following definition of what a false postive actually is.","title":"Driving Behavior"},{"location":"overview/#false-positive-an-alert-youre-not-going-to-respond-to","text":"I think many people define a false positive more in terms of mismatching something, such as a network-based packet alert that pattern matched something, but that pattern matched something it wasn't intended to match. This is obvious. But I would expand that definition even further. For example, suppose you get an alert from your IDS that a remote MySQL exploit was attempted against a server you protect, but that server does not run MySQL. How would you disposition the alert? It was an attack for sure, but it doesn't matter if the target of the attack is not running the software the attack is meant to exploit. It would matter even less if you didn't run MySQL at all. (Note that I would consider the intelligence gained from the attack as another matter entirely and not the focus of this topic.) So when I applied that definition of what a false positive is, I found that almost all of the alerts I worked ended up being false positive. In my presentation at the Cincinnati b-sides, I even stated that 99% of alerts are false positive . For the sake of argument here, we assume this is the case. This changes how alerts are analyzed. If 99% of them are false positive, then what is the chance that what is being analyzed is something to worry about? So rather than answer the question \"Is this bad?\" we answer the question \"Am I wasting my time?\" because 99% of the time that's exactly what is happening! This idea gets some resistance. I've seen many security organizations take the approach of always attempting to minimize the number of false positives their tools are generating, (rightfully) thinking their analysts waste their time working them, and thus the only alerts that should be generated are the true positive alerts that are accurately identifying an attack. I disagree with that and here is why.","title":"False Positive - An alert you're not going to respond to."},{"location":"overview/#algorithms-and-metrics","text":"An analyst can work a certain number of alerts in a given day. How large that number is depends on a number of factors such as how skilled they are. how good their tools are. how focused they are. how much analysis paralysis has set in (aka alert burn-out.) Say we measure the number of alerts the analysts can work in a given day as measurement W . Now, there are a number of tools deployed that are generating alerts. Say we measure the number of alerts generated in a given day as measurement N . All of the signatures, rules, engines and algorithms they use to generate these alerts gives the network being protected coverage . We define coverage as \"this is all the places and things we're constantly monitoring for evidence of a compromise or attack.\" Say we measure this coverage as measurement C . So now we have tools generating N alerts giving us a coverage of C , with the analysts able to work W of the N alerts. The metric to drive here is C . The higher the value of C , the more difficult it is for an attacker to have a successful attack without being detected. The issue is that an increase in C usually results in an increase in N , but W stays the same. The goal is to always be increasing C while keeping N less than or equal to W . In English that would read as \"always increasing coverage while keeping the number of alerts manageable.\" This is accomplished by a continuous process of hunting, automation and tuning. Hunting is how C increases. You look somewhere you were not looking before. This can be literally anything that makes sense: a new snort rule, a new saved search in splunk, a new script that runs that checks some weird system you have. Anything that would generate new alerts to be analyzed by the analysts. Automation is how the increase to C is actually made permanent. This means running the hunt continuously, forever or until it doesn't make sense any more. For some tools this is natural, for example, a snort rule always runs after it's deployed. But you may need to build something to run that splunk search every so often, or to run that script on a cron job as certain times. Finally, tuning is how to manage the increase in N . This is the action of preventing the tools from generating false positive alerts. This is accomplished in one of two ways: modifying the hunt to avoid the false positives. For example, tweaking the signature to be more specific in the match, or adding additional boolean logic clauses to queries (such as NOT this AND NOT that AND NOT that.) turning off the alert entirely in the case where the hunt is either wrong or not tunable. At this point, we're constantly increasing C by following the process of hunting, automating and tuning, keeping the number of alerts N manageable to a team of analysts that can handle W alerts in a given day. So where does ACE come into play here? It drives the one metric not covered yet: W . ACE increases the number of alerts an analyst can work in a given day. The higher the value of W is, the more aggressive a team can get with C . Teams with a low value of W are easily overwhelmed by very small increases to C . Teams with a high value of W can handle large increases to C . If viewed on a chart over time, the value of N should look more like a sine wave, fluctuating as new hunts are automated and tuning is performed on the old hunts. The value of C should always be rising, even if only gradually. Finally, it's worth noting that in this scenario I'm describing the number of false positive alerts is very close to N , because 99% of all alerts are false positive (assuming our definition.) Thus, the effort to reduce false positive alerts is merely a function of the process, and not a goal in itself.","title":"Algorithms and Metrics"},{"location":"user/","text":"Analyst Orientation - Start Here \u00b6 Keep this in mind when working ACE alerts: ACE is meant to enable the analyst to QUICKLY disposition false positive alerts and recognize true positives. For convenience, here is a video recording that provides a tour of the ACE GUI and demonstrates how to work some ACE alerts. Many of the concepts in this orientation are covered. Quick Concept Touchpoint \u00b6 There are two core concepts an analyst must be familiar with when working ACE alerts: Observables and Dispositioning. Observables \u00b6 Observables are anything an analyst might \"observe\" or take note of during an investigation or when performing Alert Triage. For instance, an IP address is an observable, and a file name is a different type of observable. Some more observable types are: URLs, domain names, usernames, file hashes, file names, file paths, email addresses, and Yara signatures. ACE knows what kind of analysis to perform for a given observable type and how to correlate the value of an observable across all available data sources. In the process of correlating observables with other data sources, ACE will discover more observables to analyze and correlate. When an ACE alert is created from an initial detection point, the alert's 'root' level observables are found in the output of that initial detection. ACE then gets to work on those root observables. An ACE alert's status is complete when ACE is finished with its recursive analysis, correlation, discovery, and relational combination of observables. The result is an ACE alert with intuitive context ready for the analyst's consumption. The figure below is meant to give a visual representation ACE's recursive observable analysis and correlation. Recursive Observable Analysis ACE\u2019s recursive analysis of observables reduces and simplifies the analyst\u2019s workload by providing the analyst with as much available context as reasonably possible. A complete list of currently defined observable types can be viewed in the table below. Alert Dispositioning \u00b6 When investigating an alert, there is a categorization model for analysts to follow called dispositioning. No matter if an alert requires a response or not, analysts need to disposition them correctly. Sometimes, especially for true positive alerts that get escalated, more information may lead a change in an alert\u2019s disposition. The disposition model that ACE uses is based on Lockheed Martin's Cyber Kill Chain model for identifying and describing the stages of an adversary\u2019s attack. The table below describes each of the different dispositions used by ACE. +----------+-----------------------------------------------------------+ | Disposit | Description / Example | | ion | | +==========+===========================================================+ | FALSE\\_P | Something matched a detection signature, but that | | OSITIVE | something turned out to be nothing malicious. | | | | | | - A signature was designed to detect something | | | specific, and this wasn't it. | | | - A signature was designed in a broad manner and, after | | | analysis, what it detected turned out to be benign. | | | - A response is not required. | +----------+-----------------------------------------------------------+ | IGNORE | This alert should have never fired. A match was made on | | | something a detection was looking for but it was expected | | | or an error. | | | | | | - Security information was being transferred | | | - An error occurred in the detection software | | | generating invalid alerts | | | - Someone on the security team was testing something or | | | working on something | | | | | | It is important to make the distinction between | | | FALSE\\_POSITIVE and IGNORE dispositions, as alerts marked | | | FALSE\\_POSITIVE are used to tune detection signatures, | | | while alerts marked as IGNORE are not. IGNORE alerts are | | | deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | UNKNOWN | Not enough information is available to make a good | | | decision because of a lack of visibility. | +----------+-----------------------------------------------------------+ | REVIEWED | This is a special disposition to be used for alerts that | | | were manually generated for analysis or serve an | | | informational purpose. For example, if someone uploaded a | | | malware sample from a third party to ACE, you would set | | | the disposition to REVIEWED after reviewing the analysis | | | results. Alerts set to REVIEWED do not count for metrics | | | and are not deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | GRAYWARE | Software that is not inherently malicious but exhibits | | | potentially unwanted or obtrusive behavior. | | | | | | - Adware | | | - Spyware | | | | | | [If desired, this disposition can be used to categorize s | | | pam emails.]{.strike} | +----------+-----------------------------------------------------------+ | POLICY\\_ | In the course of an investigation, general risky user | | VIOLATIO | behavior or behavior against an official policy or | | N | standard is discovered. | | | | | | - Installing unsupported software | | | - Connecting a USB drive with pirated software | | | - Browsing to pornographic sites | +----------+-----------------------------------------------------------+ | RECONNAI | Catching the adversary planning, gathering intel, or | | SSANCE | researching what attacks may work against you. | | | | | | - Vulnerability and port scanning | | | - Attempts to establish trust with a user | +----------+-----------------------------------------------------------+ | WEAPONIZ | The detection of an attempt to build a cyber attack | | ATION | weapon. | | | | | | - Detecting an advesary building a malicious document | | | using VT threat hunting | +----------+-----------------------------------------------------------+ | DELIVERY | An attack was attempted, and the attack's destination was | | | reached. Even with no indication the attack worked. | | | | | | - A user browsed to an exploit kit | | | - A phish was delivered to the email inbox | | | - AV detected and remediated malware after the malware | | | was written to disk | +----------+-----------------------------------------------------------+ | EXPLOITA | An attack was DELIVERED and there is evidence that the | | TION | EXPLOITATION worked in whole or in part. | | | | | | - A user clicked on a malicious link from a phish | | | - A user opened and ran a malicious email attachment | | | - A user hit an exploit kit, a Flash exploit was | | | attempted | +----------+-----------------------------------------------------------+ | INSTALLA | An attack was DELIVERED and the attack resulted in the | | TION | INSTALLATION of something to maintain persistence on an | | | asset/endpoint/system. | | | | | | - A user browsed to an exploit kit and got malware | | | installed on their system | | | - A user executed a malicious email attachment and | | | malware was installed | | | - Malware executed off a USB and installed persistence | | | on an endpoint | +----------+-----------------------------------------------------------+ | COMMAND\\ | An attacker was able to communicate between their control | | _AND\\_CO | system and a compromised asset. The adversary has been | | NTROL | able to establish a control channel with an asset. | | | | | | Example Scenario: A phish is DELIVERED to an inbox, and a | | | user opens a malicious Word document that was attached. | | | The Word document EXPLOITS a vulnerability and leads to | | | the INSTALLATION of malware. The malware is able to | | | communicate back to the attackers COMMAND\\_AND\\_CONTROL | | | server. | +----------+-----------------------------------------------------------+ | EXFIL | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. EXFIL indicates the | | | loss of something important. | | | | | | - Adversaries steals information by uploading files to | | | their control server | | | - A user submits login credentials to a phishing | | | website | +----------+-----------------------------------------------------------+ | DAMAGE | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. DAMAGE indicates that | | | damage or disruption was made to an asset, the network, | | | the company, or business operations. | | | | | | - An attacker steals money by tricking an employee to | | | change the bank account number of a customer | | | - Ransomware encrypts multiple files on an asset | | | - PLC code is modified and warehouse equipment is | | | broken | | | - Process Control Systems are tampered with and a | | | facility must shutdown until repairs are made | | | - A public facing website is compromised and defaced or | | | serves malware to other victums | +----------+-----------------------------------------------------------+ | INSIDER\\ | Employee has the data and is attempting to send it out of | | _DATA\\_C | the bank. | | ONTROL | | +----------+-----------------------------------------------------------+ | INSIDER\\ | Sensitive data leaves the bank below privacy impact | | _DATA\\_E | threshold. | | XFIL | | +----------+-----------------------------------------------------------+ GUI Overview \u00b6 Analysts interact with ACE through its graphical interface and specifically use the Manage Alerts page. After you're logged into ACE (Assuming you already have an account), you'll see a navigation bar that looks like the following image. A a simple breakdown of each page on that navigation bar is provided below. Page Function ---- --------- Overview General ACE information, performance, statistics, etc. Manual Analysis Where analysts can manually upload or submit observables for ACE to analyze Manage Alerts The alert queue - where the magic happens Events Where events are managed Metrics For creating and tracking metrics from the data ACE generates Working Alerts \u00b6 This section covers the basics for working and managing ACE alerts. If you're comfortable, skip ahead to the Examples section to find a walkthrough of a few ACE alerts being worked. The Manage Alerts Page \u00b6 ACE alerts will queue up on the Manage Alerts page. By default, only alerts that are open (not dispositioned ) and not owned by another analyst are displayed. When working an alert, analysts should take ownership of it to prevent other analysts from starting to work on the same alert. This prevents re-work and saves analyst time. You can take ownership of one or more alerts on the Manage Alerts page by selecting alert checkboxes and clicking the 'Take Ownership' button. You can also take ownership when viewing an individual alert. Below is an example of the Manage Alerts page with 32 open and unowned alerts. Manage Alerts page Viewing Observable Summary \u00b6 On the Manage Alerts page, each alert can be expanded via its dropdown button. Once expanded, all the observables in the alert can be viewed. The observables are grouped and listed by their observable type. The numbers in parentheses show a count of how many times ACE has seen that observable. Each observable is clickable, and when clicked, ACE will add that observable to the current alert filter. You don't need to worry about alert filtering to work alerts, however, the Filtering and Grouping <filtering and grouping> section covers Alert filtering. Expand/Collapse Observables - email_address - fakeuser@fakecompany.com (21) - tfry@kennyross.com (2) - email_conversation - tfry@kennyross.com|fakeuser@fakecompany.com (1) - file - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/1LSZPI0TG6C82HTABETK.temp (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/Kenny_Ross_Inquiry.LNK (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/index.dat (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/urlref_httpvezopilan.comtstindex.phpl_soho7.tkn_.Split (1) - Kenny_Ross_Inquiry.doc (9) - Kenny_Ross_Inquiry.doc.officeparser/iYzcZYMdfv.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/oUDOGruwp.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_10_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_11_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_12_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_13_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_14_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_15_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_16_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_17_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_18_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_19_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_1_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIK1.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIKManager.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/_pPOR/WXRIKManager.lrA.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/[Content_Types].lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/_pPOR/.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_3_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_4_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_5_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_8_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_9_0.dat (2) - Kenny_Ross_Inquiry.doc.olevba/macro_0.bas (2) - Kenny_Ross_Inquiry.doc.olevba/macro_1.bas (2) - Kenny_Ross_Inquiry.doc.pcode.bas (2) - email.rfc822 (37952) - email.rfc822.headers (37949) - email.rfc822.unknown_text_html_000 (3229) - email.rfc822.unknown_text_html_000_000.png (2482) - email.rfc822.unknown_text_plain_000 (37354) - filename.PNG (11) - indicator - 55c36786bcb87f2d54cf15da (369) - 57ffd02cbcb87fbb1464b1ce (88) - 58c9708aad951d7387c65be2 (274) - 58e3e8dfad951d49aabb1622 (384) - 58ee209dad951d09a1ee3860 (92) - 58ee221dad951d09a0b13e99 (92) - 5937f5d4ad951d4fe8787c63 (672) - 599db056ad951d5cb2c4768b (302) - 599dd8abad951d5cb3204569 (155) - 59a7fcc7ad951d522eeef8ed (380) - ipv4 - 104.118.208.249 (24) - md5 - 2307a1a403c6326509d4d9546e5f32ab (2) - 267b1bd0ae8194781c373f93c9df02fa (2) - 39ee938f6fa351f94a2cbf8835bb454f (2) - 5c4c76cbb739c04fb3838aff5b2c25bb (2) - 65811d8f7c6a1b94eab03ba1072a3a7e (2) - b3b8bf4ed2c5cb26883661911487d642 (2) - d8a7ea6ba4ab9541e628452e2ad6014a (2) - message_id - <8de41f6eb57ac01b2a90d3466890b0a1@127.0.0.1> (1) - sha1 - 03484a568871d494ad144ac9597e9717a2ae5601 (2) - 2e3b95bb9b0beb5db3487646d772363004505df6 (2) - 33b9d3de33adc5bd5954c1e9f9e48f10eabe7c49 (2) - 62837876eb5ec321e6d8dbd6babd0d5789230b60 (2) - b3024c6f598b1745ca352ac3a24cc3603b814cad (2) - cfe4f07fbf042b4f7dce44f9e6e3f449e02c123a (2) - fa47ebc1026bbe8952f129480f38a011f9faf47d (2) - sha256 - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98 (2) - 50aef060b9192d5230be21df821acb4495f7dc90416b2edfd68ebebde40562be (2) - 62be2fe5e5ad79f62671ba4b846a63352d324bb693ee7c0f663f488e25f05fe0 (2) - 8159227eb654ef2f60eb4c575f4a218bb76919ea15fdd625c2d01d151e4973f3 (2) - 9c7e06164ec59e76d6f3e01fa0129607be1d98af270a09fd0f126ee8e16da306 (2) - ae67f33b6ff45aecf91ff6cac71b290c27f791ccbe4829be44bd64468cbe3f5d (2) - ca797ec10341aebaed1130c4dbf9a5b036945f17dd94d71d46f2f81d9937504f (2) - url - http://schemas.openxmlformats.org/drawingml/2006/main (3796) - user - fake_user_id (17) - yara_rule - CRITS_EmailContent (4478) - CRITS_StringOffice (1685) - CRITS_StringVBS (6592) - CRITS_StringWindowsShell (1770) - macro_code_snippet (1013) - macro_overused_legit_functions (82) Above, you can click to expand a text based example of an alerts observable structure when expanded on the Manage Alerts page. Filtering and Grouping \u00b6 On the manage-alerts-page, alerts are filtered by default to show open alerts that are not currently owned by any other analysts. The current filter state is always displayed at the top of the page, in a human readable format. You can select 'Edit Filters' to modify the alert filter and display alerts based on several different conditions. For example, you can change the filters to see alerts dispositioned as DELIVERY over the past seven days by a specific analyst. Alerts can also be filtered by observables. Conveniently, when viewing an alert's Observable Summary <observable-summary> on the Manage Alerts page, you can click any of those observables to add it to the currently defined alert filter. So, with the default filter applied, if you clicked on an MD5 observable with value 10EFE4369EA344308416FB59051D5947 then the page would refresh and you'd see that the new filter became: filter: open alerts AND not owned by others AND with observable type md5 value b'10EFE4369EA344308416FB59051D5947'` The Alert Page \u00b6 Once an alert is opened, the full analysis results will be displayed. It's usually a good idea to go ahead and view <analysis-views> all of the alert's analysis. Views \u00b6 There are two different modes in which you can view ACE alerts: 'Critical' and 'All'. By default, ACE alerts will be displayed in critical mode. Critical mode will only display 'root' level alert observable analysis. This is helpful for alerts with a lot of observables, although it's generally helpful to view all alert analysis. At the top right of every alert you will see a button to \"View All Analysis\" or \"View Critical Analysis\". Whichever mode you have enabled will be persistent across your ACE session. Be mindful of these different views, as it's possible for an analyst to miss helpful information if viewing an alert in critical mode compared to all mode. Analysis Overview \u00b6 Each standard ACE alert will have analysis overview section where the analysis results for every Observable <observable> will be found. The observables displayed at the 'root' level are the ones that were directly discovered in the data provided to ACE at the time of the alert's creation. Underneath each observable you will find the analysis results for that respective observable. You may also find new observables that were added to the alert from the recursive analysis of other observables. This observable nesting on the alert page provides a visual representation of how alert observables are related. The figure below shows the analysis overview section of an ACE Mailbox (email) alert. You can see that a user observable of value 'fake-user-id' was discovered from the analysis results of the email_address Observable. Alert Tags \u00b6 ACE has a tagging system by which observables and analysis are tagged for the purpose of providing additional context. If you review the previous figure of manage-alerts-page, you will notice tags such as phish, new_sender, and frequent_conversation associated to various alerts. All observable tags get associated with their respective alert and show up on the alert management page. Any observable can be tagged and can have any number of tags. For instance, an email conversation between two addresses that ACE has seen a lot will be tagged as 'frequent_conversation'. Tags can also be added directly to alerts from the Manage Alerts page. This can be helpful for Filtering and Grouping alerts if an analyst needs a way to group alerts that don\u2019t otherwise have a commonly shared tag or observable. Examples \u00b6 The following are examples of a snarky analyst working ACE alerts. Think about the first intuition you get from what you see in these alerts. Check out this Email Alert \u00b6 We just got this alert in the queue. Huh, looks like this email might be related to a potentially malicious zip file. Let's open the alert and look at the Analysis Overview section to see the results ACE brought us. In the case of email alerts like this one, the 'email.rfc882' file is what ACE was given when told to create this alert. Under that email.rfc882 file observable you will see the output of the Email Analysis module, and underneath Email Analysis you will see where ACE discovered more observables, such as the email addresses. With respect to this alert, ACE conveniently rendered us a visual image of the email's HTML body. That rendering lets us quickly see that the sender is thanking the recipient for their purchase. It seems doubtful to me that the user really purchased anything, so this email seems awfully suspicious. Note that we can also view or download that 'email.rfc822.unknown_text_html_000' file by using the dropdown next to it. Scrolling down on the same alert from the example above, we see the \u2018URL Extraction Analysis\u2019 found some URL observables. Moreover, we see that ACE found additional observables in the analysis output of those URL observables. Specifically, ACE downloaded that '66524177012457.zip' file and extracted it to reveal an executable named '66524177012457.exe'. Hm, this email doesn't seem friendly at all. Perhaps that malicious tag was onto something... where did that tag come from? Oh, it's next to the MD5 observable of the file, which I know ACE checks VirusTotal for, and one of the analysis results under that MD5 observable shows the VT result summary. Got it. Definitely malicious. Someone should do something about this. We got another Email Alert \u00b6 We just got this email alert. Judging by the tags, I'm assuming an office document is attached. It's probably an open xml office document too, since the zip tag is present. I\u2019m assuming this because I know open xml office documents are zipped up. Of course, there could be a stand-alone zip file in the email too. Let's look and see. When we open the alert, we see the alert header at the top. Hmm, this email alert only has one detection. Either this is really good phish and something we barely catch, or it's a false positive. Let's scroll down and find that single detection. Oh, I just noticed that we're only viewing this alert's critical analysis. We could click on the \"View All Analysis\" button if we wanted to view all of its analysis results. However, for this alert, the critical view makes it easy to find the single detection. Detections are marked by a little red flame icon. Here we see that the flame is highlighting a yara rule that detected something in the analysis of the \"Glenn Resume.docx\" file. Speaking of that file, we were right about assuming it was an open xml office document. Look at this, ACE tagged the rels_remote_references yara rule with high_fp_frequency. That tells us that this specific yara rule has a high frequency of showing up in false positive alerts. Below the rule, we see that the \"Malicious Frequency Analysis\" module found the rels_remote_references yara rule only appeared in four true-positive alerts out of two hundred and ten! I don't know about you, but my gut is telling me this email alert is a false positive. Let's make sure though and click to view the \"Yara Scan Results\". Above we can see what the yara rule detected in this docx file. And what do we see? A target reference to a file, and when looking closer we see that the file being referenced was named \"Resume Template.dotm\". I bet this dotm file is a leftover artifact from Glenn using a resume template when creating this \"Glenn Resume.docx\" file. I'm already clicking the \"Disposition\" button and marking this alert FALSE_POSITIVE. Now that we've reviewed this email alert, I want to harp on how QUICKLY we should be able to disposition it. If you\u2019re curious, the rels_remote_references yara rule was created to detect references to URLs or files in an open xml document's template. Such references can and have been malicious. An example would be a Microsoft Word document that references a URL and causes word to display an authentication dialog to the end-user for the purpose of harvesting the user\u2019s credentials. This repository contains a GO script that makes it easy to do that very thing: https://github.com/ryhanson/phishery","title":"Overview"},{"location":"user/#analyst-orientation-start-here","text":"Keep this in mind when working ACE alerts: ACE is meant to enable the analyst to QUICKLY disposition false positive alerts and recognize true positives. For convenience, here is a video recording that provides a tour of the ACE GUI and demonstrates how to work some ACE alerts. Many of the concepts in this orientation are covered.","title":"Analyst Orientation - Start Here"},{"location":"user/#quick-concept-touchpoint","text":"There are two core concepts an analyst must be familiar with when working ACE alerts: Observables and Dispositioning.","title":"Quick Concept Touchpoint"},{"location":"user/#observables","text":"Observables are anything an analyst might \"observe\" or take note of during an investigation or when performing Alert Triage. For instance, an IP address is an observable, and a file name is a different type of observable. Some more observable types are: URLs, domain names, usernames, file hashes, file names, file paths, email addresses, and Yara signatures. ACE knows what kind of analysis to perform for a given observable type and how to correlate the value of an observable across all available data sources. In the process of correlating observables with other data sources, ACE will discover more observables to analyze and correlate. When an ACE alert is created from an initial detection point, the alert's 'root' level observables are found in the output of that initial detection. ACE then gets to work on those root observables. An ACE alert's status is complete when ACE is finished with its recursive analysis, correlation, discovery, and relational combination of observables. The result is an ACE alert with intuitive context ready for the analyst's consumption. The figure below is meant to give a visual representation ACE's recursive observable analysis and correlation. Recursive Observable Analysis ACE\u2019s recursive analysis of observables reduces and simplifies the analyst\u2019s workload by providing the analyst with as much available context as reasonably possible. A complete list of currently defined observable types can be viewed in the table below.","title":"Observables"},{"location":"user/#alert-dispositioning","text":"When investigating an alert, there is a categorization model for analysts to follow called dispositioning. No matter if an alert requires a response or not, analysts need to disposition them correctly. Sometimes, especially for true positive alerts that get escalated, more information may lead a change in an alert\u2019s disposition. The disposition model that ACE uses is based on Lockheed Martin's Cyber Kill Chain model for identifying and describing the stages of an adversary\u2019s attack. The table below describes each of the different dispositions used by ACE. +----------+-----------------------------------------------------------+ | Disposit | Description / Example | | ion | | +==========+===========================================================+ | FALSE\\_P | Something matched a detection signature, but that | | OSITIVE | something turned out to be nothing malicious. | | | | | | - A signature was designed to detect something | | | specific, and this wasn't it. | | | - A signature was designed in a broad manner and, after | | | analysis, what it detected turned out to be benign. | | | - A response is not required. | +----------+-----------------------------------------------------------+ | IGNORE | This alert should have never fired. A match was made on | | | something a detection was looking for but it was expected | | | or an error. | | | | | | - Security information was being transferred | | | - An error occurred in the detection software | | | generating invalid alerts | | | - Someone on the security team was testing something or | | | working on something | | | | | | It is important to make the distinction between | | | FALSE\\_POSITIVE and IGNORE dispositions, as alerts marked | | | FALSE\\_POSITIVE are used to tune detection signatures, | | | while alerts marked as IGNORE are not. IGNORE alerts are | | | deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | UNKNOWN | Not enough information is available to make a good | | | decision because of a lack of visibility. | +----------+-----------------------------------------------------------+ | REVIEWED | This is a special disposition to be used for alerts that | | | were manually generated for analysis or serve an | | | informational purpose. For example, if someone uploaded a | | | malware sample from a third party to ACE, you would set | | | the disposition to REVIEWED after reviewing the analysis | | | results. Alerts set to REVIEWED do not count for metrics | | | and are not deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | GRAYWARE | Software that is not inherently malicious but exhibits | | | potentially unwanted or obtrusive behavior. | | | | | | - Adware | | | - Spyware | | | | | | [If desired, this disposition can be used to categorize s | | | pam emails.]{.strike} | +----------+-----------------------------------------------------------+ | POLICY\\_ | In the course of an investigation, general risky user | | VIOLATIO | behavior or behavior against an official policy or | | N | standard is discovered. | | | | | | - Installing unsupported software | | | - Connecting a USB drive with pirated software | | | - Browsing to pornographic sites | +----------+-----------------------------------------------------------+ | RECONNAI | Catching the adversary planning, gathering intel, or | | SSANCE | researching what attacks may work against you. | | | | | | - Vulnerability and port scanning | | | - Attempts to establish trust with a user | +----------+-----------------------------------------------------------+ | WEAPONIZ | The detection of an attempt to build a cyber attack | | ATION | weapon. | | | | | | - Detecting an advesary building a malicious document | | | using VT threat hunting | +----------+-----------------------------------------------------------+ | DELIVERY | An attack was attempted, and the attack's destination was | | | reached. Even with no indication the attack worked. | | | | | | - A user browsed to an exploit kit | | | - A phish was delivered to the email inbox | | | - AV detected and remediated malware after the malware | | | was written to disk | +----------+-----------------------------------------------------------+ | EXPLOITA | An attack was DELIVERED and there is evidence that the | | TION | EXPLOITATION worked in whole or in part. | | | | | | - A user clicked on a malicious link from a phish | | | - A user opened and ran a malicious email attachment | | | - A user hit an exploit kit, a Flash exploit was | | | attempted | +----------+-----------------------------------------------------------+ | INSTALLA | An attack was DELIVERED and the attack resulted in the | | TION | INSTALLATION of something to maintain persistence on an | | | asset/endpoint/system. | | | | | | - A user browsed to an exploit kit and got malware | | | installed on their system | | | - A user executed a malicious email attachment and | | | malware was installed | | | - Malware executed off a USB and installed persistence | | | on an endpoint | +----------+-----------------------------------------------------------+ | COMMAND\\ | An attacker was able to communicate between their control | | _AND\\_CO | system and a compromised asset. The adversary has been | | NTROL | able to establish a control channel with an asset. | | | | | | Example Scenario: A phish is DELIVERED to an inbox, and a | | | user opens a malicious Word document that was attached. | | | The Word document EXPLOITS a vulnerability and leads to | | | the INSTALLATION of malware. The malware is able to | | | communicate back to the attackers COMMAND\\_AND\\_CONTROL | | | server. | +----------+-----------------------------------------------------------+ | EXFIL | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. EXFIL indicates the | | | loss of something important. | | | | | | - Adversaries steals information by uploading files to | | | their control server | | | - A user submits login credentials to a phishing | | | website | +----------+-----------------------------------------------------------+ | DAMAGE | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. DAMAGE indicates that | | | damage or disruption was made to an asset, the network, | | | the company, or business operations. | | | | | | - An attacker steals money by tricking an employee to | | | change the bank account number of a customer | | | - Ransomware encrypts multiple files on an asset | | | - PLC code is modified and warehouse equipment is | | | broken | | | - Process Control Systems are tampered with and a | | | facility must shutdown until repairs are made | | | - A public facing website is compromised and defaced or | | | serves malware to other victums | +----------+-----------------------------------------------------------+ | INSIDER\\ | Employee has the data and is attempting to send it out of | | _DATA\\_C | the bank. | | ONTROL | | +----------+-----------------------------------------------------------+ | INSIDER\\ | Sensitive data leaves the bank below privacy impact | | _DATA\\_E | threshold. | | XFIL | | +----------+-----------------------------------------------------------+","title":"Alert Dispositioning"},{"location":"user/#gui-overview","text":"Analysts interact with ACE through its graphical interface and specifically use the Manage Alerts page. After you're logged into ACE (Assuming you already have an account), you'll see a navigation bar that looks like the following image. A a simple breakdown of each page on that navigation bar is provided below. Page Function ---- --------- Overview General ACE information, performance, statistics, etc. Manual Analysis Where analysts can manually upload or submit observables for ACE to analyze Manage Alerts The alert queue - where the magic happens Events Where events are managed Metrics For creating and tracking metrics from the data ACE generates","title":"GUI Overview"},{"location":"user/#working-alerts","text":"This section covers the basics for working and managing ACE alerts. If you're comfortable, skip ahead to the Examples section to find a walkthrough of a few ACE alerts being worked.","title":"Working Alerts"},{"location":"user/#the-manage-alerts-page","text":"ACE alerts will queue up on the Manage Alerts page. By default, only alerts that are open (not dispositioned ) and not owned by another analyst are displayed. When working an alert, analysts should take ownership of it to prevent other analysts from starting to work on the same alert. This prevents re-work and saves analyst time. You can take ownership of one or more alerts on the Manage Alerts page by selecting alert checkboxes and clicking the 'Take Ownership' button. You can also take ownership when viewing an individual alert. Below is an example of the Manage Alerts page with 32 open and unowned alerts. Manage Alerts page","title":"The Manage Alerts Page"},{"location":"user/#viewing-observable-summary","text":"On the Manage Alerts page, each alert can be expanded via its dropdown button. Once expanded, all the observables in the alert can be viewed. The observables are grouped and listed by their observable type. The numbers in parentheses show a count of how many times ACE has seen that observable. Each observable is clickable, and when clicked, ACE will add that observable to the current alert filter. You don't need to worry about alert filtering to work alerts, however, the Filtering and Grouping <filtering and grouping> section covers Alert filtering. Expand/Collapse Observables - email_address - fakeuser@fakecompany.com (21) - tfry@kennyross.com (2) - email_conversation - tfry@kennyross.com|fakeuser@fakecompany.com (1) - file - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/1LSZPI0TG6C82HTABETK.temp (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/Kenny_Ross_Inquiry.LNK (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/index.dat (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/urlref_httpvezopilan.comtstindex.phpl_soho7.tkn_.Split (1) - Kenny_Ross_Inquiry.doc (9) - Kenny_Ross_Inquiry.doc.officeparser/iYzcZYMdfv.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/oUDOGruwp.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_10_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_11_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_12_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_13_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_14_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_15_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_16_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_17_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_18_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_19_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_1_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIK1.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIKManager.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/_pPOR/WXRIKManager.lrA.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/[Content_Types].lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/_pPOR/.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_3_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_4_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_5_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_8_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_9_0.dat (2) - Kenny_Ross_Inquiry.doc.olevba/macro_0.bas (2) - Kenny_Ross_Inquiry.doc.olevba/macro_1.bas (2) - Kenny_Ross_Inquiry.doc.pcode.bas (2) - email.rfc822 (37952) - email.rfc822.headers (37949) - email.rfc822.unknown_text_html_000 (3229) - email.rfc822.unknown_text_html_000_000.png (2482) - email.rfc822.unknown_text_plain_000 (37354) - filename.PNG (11) - indicator - 55c36786bcb87f2d54cf15da (369) - 57ffd02cbcb87fbb1464b1ce (88) - 58c9708aad951d7387c65be2 (274) - 58e3e8dfad951d49aabb1622 (384) - 58ee209dad951d09a1ee3860 (92) - 58ee221dad951d09a0b13e99 (92) - 5937f5d4ad951d4fe8787c63 (672) - 599db056ad951d5cb2c4768b (302) - 599dd8abad951d5cb3204569 (155) - 59a7fcc7ad951d522eeef8ed (380) - ipv4 - 104.118.208.249 (24) - md5 - 2307a1a403c6326509d4d9546e5f32ab (2) - 267b1bd0ae8194781c373f93c9df02fa (2) - 39ee938f6fa351f94a2cbf8835bb454f (2) - 5c4c76cbb739c04fb3838aff5b2c25bb (2) - 65811d8f7c6a1b94eab03ba1072a3a7e (2) - b3b8bf4ed2c5cb26883661911487d642 (2) - d8a7ea6ba4ab9541e628452e2ad6014a (2) - message_id - <8de41f6eb57ac01b2a90d3466890b0a1@127.0.0.1> (1) - sha1 - 03484a568871d494ad144ac9597e9717a2ae5601 (2) - 2e3b95bb9b0beb5db3487646d772363004505df6 (2) - 33b9d3de33adc5bd5954c1e9f9e48f10eabe7c49 (2) - 62837876eb5ec321e6d8dbd6babd0d5789230b60 (2) - b3024c6f598b1745ca352ac3a24cc3603b814cad (2) - cfe4f07fbf042b4f7dce44f9e6e3f449e02c123a (2) - fa47ebc1026bbe8952f129480f38a011f9faf47d (2) - sha256 - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98 (2) - 50aef060b9192d5230be21df821acb4495f7dc90416b2edfd68ebebde40562be (2) - 62be2fe5e5ad79f62671ba4b846a63352d324bb693ee7c0f663f488e25f05fe0 (2) - 8159227eb654ef2f60eb4c575f4a218bb76919ea15fdd625c2d01d151e4973f3 (2) - 9c7e06164ec59e76d6f3e01fa0129607be1d98af270a09fd0f126ee8e16da306 (2) - ae67f33b6ff45aecf91ff6cac71b290c27f791ccbe4829be44bd64468cbe3f5d (2) - ca797ec10341aebaed1130c4dbf9a5b036945f17dd94d71d46f2f81d9937504f (2) - url - http://schemas.openxmlformats.org/drawingml/2006/main (3796) - user - fake_user_id (17) - yara_rule - CRITS_EmailContent (4478) - CRITS_StringOffice (1685) - CRITS_StringVBS (6592) - CRITS_StringWindowsShell (1770) - macro_code_snippet (1013) - macro_overused_legit_functions (82) Above, you can click to expand a text based example of an alerts observable structure when expanded on the Manage Alerts page.","title":"Viewing Observable Summary"},{"location":"user/#filtering-and-grouping","text":"On the manage-alerts-page, alerts are filtered by default to show open alerts that are not currently owned by any other analysts. The current filter state is always displayed at the top of the page, in a human readable format. You can select 'Edit Filters' to modify the alert filter and display alerts based on several different conditions. For example, you can change the filters to see alerts dispositioned as DELIVERY over the past seven days by a specific analyst. Alerts can also be filtered by observables. Conveniently, when viewing an alert's Observable Summary <observable-summary> on the Manage Alerts page, you can click any of those observables to add it to the currently defined alert filter. So, with the default filter applied, if you clicked on an MD5 observable with value 10EFE4369EA344308416FB59051D5947 then the page would refresh and you'd see that the new filter became: filter: open alerts AND not owned by others AND with observable type md5 value b'10EFE4369EA344308416FB59051D5947'`","title":"Filtering and Grouping"},{"location":"user/#the-alert-page","text":"Once an alert is opened, the full analysis results will be displayed. It's usually a good idea to go ahead and view <analysis-views> all of the alert's analysis.","title":"The Alert Page"},{"location":"user/#views","text":"There are two different modes in which you can view ACE alerts: 'Critical' and 'All'. By default, ACE alerts will be displayed in critical mode. Critical mode will only display 'root' level alert observable analysis. This is helpful for alerts with a lot of observables, although it's generally helpful to view all alert analysis. At the top right of every alert you will see a button to \"View All Analysis\" or \"View Critical Analysis\". Whichever mode you have enabled will be persistent across your ACE session. Be mindful of these different views, as it's possible for an analyst to miss helpful information if viewing an alert in critical mode compared to all mode.","title":"Views"},{"location":"user/#analysis-overview","text":"Each standard ACE alert will have analysis overview section where the analysis results for every Observable <observable> will be found. The observables displayed at the 'root' level are the ones that were directly discovered in the data provided to ACE at the time of the alert's creation. Underneath each observable you will find the analysis results for that respective observable. You may also find new observables that were added to the alert from the recursive analysis of other observables. This observable nesting on the alert page provides a visual representation of how alert observables are related. The figure below shows the analysis overview section of an ACE Mailbox (email) alert. You can see that a user observable of value 'fake-user-id' was discovered from the analysis results of the email_address Observable.","title":"Analysis Overview"},{"location":"user/#alert-tags","text":"ACE has a tagging system by which observables and analysis are tagged for the purpose of providing additional context. If you review the previous figure of manage-alerts-page, you will notice tags such as phish, new_sender, and frequent_conversation associated to various alerts. All observable tags get associated with their respective alert and show up on the alert management page. Any observable can be tagged and can have any number of tags. For instance, an email conversation between two addresses that ACE has seen a lot will be tagged as 'frequent_conversation'. Tags can also be added directly to alerts from the Manage Alerts page. This can be helpful for Filtering and Grouping alerts if an analyst needs a way to group alerts that don\u2019t otherwise have a commonly shared tag or observable.","title":"Alert Tags"},{"location":"user/#examples","text":"The following are examples of a snarky analyst working ACE alerts. Think about the first intuition you get from what you see in these alerts.","title":"Examples"},{"location":"user/#check-out-this-email-alert","text":"We just got this alert in the queue. Huh, looks like this email might be related to a potentially malicious zip file. Let's open the alert and look at the Analysis Overview section to see the results ACE brought us. In the case of email alerts like this one, the 'email.rfc882' file is what ACE was given when told to create this alert. Under that email.rfc882 file observable you will see the output of the Email Analysis module, and underneath Email Analysis you will see where ACE discovered more observables, such as the email addresses. With respect to this alert, ACE conveniently rendered us a visual image of the email's HTML body. That rendering lets us quickly see that the sender is thanking the recipient for their purchase. It seems doubtful to me that the user really purchased anything, so this email seems awfully suspicious. Note that we can also view or download that 'email.rfc822.unknown_text_html_000' file by using the dropdown next to it. Scrolling down on the same alert from the example above, we see the \u2018URL Extraction Analysis\u2019 found some URL observables. Moreover, we see that ACE found additional observables in the analysis output of those URL observables. Specifically, ACE downloaded that '66524177012457.zip' file and extracted it to reveal an executable named '66524177012457.exe'. Hm, this email doesn't seem friendly at all. Perhaps that malicious tag was onto something... where did that tag come from? Oh, it's next to the MD5 observable of the file, which I know ACE checks VirusTotal for, and one of the analysis results under that MD5 observable shows the VT result summary. Got it. Definitely malicious. Someone should do something about this.","title":"Check out this Email Alert"},{"location":"user/#we-got-another-email-alert","text":"We just got this email alert. Judging by the tags, I'm assuming an office document is attached. It's probably an open xml office document too, since the zip tag is present. I\u2019m assuming this because I know open xml office documents are zipped up. Of course, there could be a stand-alone zip file in the email too. Let's look and see. When we open the alert, we see the alert header at the top. Hmm, this email alert only has one detection. Either this is really good phish and something we barely catch, or it's a false positive. Let's scroll down and find that single detection. Oh, I just noticed that we're only viewing this alert's critical analysis. We could click on the \"View All Analysis\" button if we wanted to view all of its analysis results. However, for this alert, the critical view makes it easy to find the single detection. Detections are marked by a little red flame icon. Here we see that the flame is highlighting a yara rule that detected something in the analysis of the \"Glenn Resume.docx\" file. Speaking of that file, we were right about assuming it was an open xml office document. Look at this, ACE tagged the rels_remote_references yara rule with high_fp_frequency. That tells us that this specific yara rule has a high frequency of showing up in false positive alerts. Below the rule, we see that the \"Malicious Frequency Analysis\" module found the rels_remote_references yara rule only appeared in four true-positive alerts out of two hundred and ten! I don't know about you, but my gut is telling me this email alert is a false positive. Let's make sure though and click to view the \"Yara Scan Results\". Above we can see what the yara rule detected in this docx file. And what do we see? A target reference to a file, and when looking closer we see that the file being referenced was named \"Resume Template.dotm\". I bet this dotm file is a leftover artifact from Glenn using a resume template when creating this \"Glenn Resume.docx\" file. I'm already clicking the \"Disposition\" button and marking this alert FALSE_POSITIVE. Now that we've reviewed this email alert, I want to harp on how QUICKLY we should be able to disposition it. If you\u2019re curious, the rels_remote_references yara rule was created to detect references to URLs or files in an open xml document's template. Such references can and have been malicious. An example would be a Microsoft Word document that references a URL and causes word to display an authentication dialog to the end-user for the purpose of harvesting the user\u2019s credentials. This repository contains a GO script that makes it easy to do that very thing: https://github.com/ryhanson/phishery","title":"We got another Email Alert"},{"location":"user/hunting/","text":"Hunting \u00b6 ACE has a service called hunter that executes hunts at a specific time or frequency. Each type of hunter is designed to execute a specific type of hunt. Currently the following hunting services are available in ACE. Fundamentals \u00b6 A hunt is compromised of a configuration and an execution . The configuration defines the various aspects of the hunt such as what it should be identified as, what hunting system it's a part of, when it should run, how the results should be interpreted, etc. The execution defines what exactly the hunt is looking for. In the case of query-based hunts the execution would be a search string or a database query. The results of the execution then feed ACE with an analysis submission. Supported Hunting Systems \u00b6 At this time the following hunting system are supported. Splunk IBM QRadar Hunt Configuration \u00b6 Hunts are defined in individual ini files, one hunt per file. There is no limit on the number of hunts that can be defined. The following provides an example as well as the documentation of the fields in each hunt configuration. ``` {.sourceCode .ini} [rule] ; set this to either yes or no to enable or disable the hunt ; hunts that are disabled are not executed enabled = yes ; OPTIONAL ; a unique name for the hunt ; this also becomes the prefix for the name of the alert in ACE ; by default the name for the hunt is derived from the file name name = Super Awesome Hunt ; a useful description for the hunt ; this is a free-form text value that will be included as the ; \"instructions\" field in the alert ; you can use this field to describe what this hunt is looking for ; as well as ways to analyze the results description = This looks for a thing and then you have to analyze it. ; OPTIONAL ; defines what analysis mode submissions will be in when they reach ACE ; this controls what analysis modules run on the hunt results ; by default this is set to correlation which forces the hunts results ; to become alerts by default analysis_mode = correlation ; defines what hunter should execute this hunt ; use the ace hunt list-types to get the full list of supported types type = qradar ; OPTIONAL ; this maps to the type field of an ACE alert ; this is used to define custom templates to view the data in ACE ; by default this takes the value of hunter - [type] alert_type = hunter - qradar - bluecoat ; how often to execute the hunt ; this can be either a timespec in HH:MM:SS format ; or it can be a crontab entry format if you want to hunt to execute ; at a specific time of the day ; keep in mind that ACE uses UTC frequency = 00:30:00 ; a comma separated list of tags to add to the analysis if submitted to ACE tags = tag1, tag2, tag3 ; ; the following configuration options are valid only for query-based hunts ; ; set to yes to enable full coverage, no to disable ; this option ensures that the starting time of the next execution will be ; equal to the ending time of the last time the hunt successfully executed ; this will effectively bypass the time_range setting and use the frequency ; to determine the search times, except for the first time it runs full_coverage = yes ; set to yes to enable, no to disable ; use whatever is considered the \"index time\" for this hunt type ; events that are collected by log systems often record when they received ; the event, which is separate from when the event actually occurred ; this can be very different when logs are sent by batch methods ; use this method in conjunction with full_coverage to ensure that you ; search all of the log records with your hunts use_index_time = yes ; set the time range for the query in HH:MM:SS format ; specifies how far back to look when performing a query (by controlling ; the time field in whatever system is executing the hunt) ; if full_coverage is yes, then this value is only used the first time the ; hunt is executed time_range = 00:30:00 ; specified a maximum time range for a single query in [DD:]HH:MM:SS format ; queries that would exceed this time range are split into chunks and ; executed in series with time ranges no larger than max_time_range max_time_range = 24:00:00 ; OPTIONAL ; specifies an offset in HH:MM:SS format to execute queries on ; the final time range of all queries is offset by this value ; this option is useful if your logging system is slow to index data offset = 00:05:00 ; when a query hunt executes and returns multiple results, these results ; are then grouped together by the field specified in this option ; this also becomes part of the name of the alert ; by appending the value of the grouped field to the name of the hunt group_by = root_domain ; OPTIONAL ; specifies a path (relative to SAQ_HOME) to a file that contains the ; actual hunt to execute search = hunts/site/qradar/bluecoat-malicious_domain.sql ; OPTIONAL ; specifies the actual hunt to execute query = SELECT * FROM whatever WHERE something = 'this_or_that' ; ; the following configuration options are valid only for SPLUNK hunts ; ; OPTIONAL ; puts the splunk search into the context of the given user and/or app ; by default the splunk hunter uses the default (wildcard) user and app namespace splunk_user_context = user_name splunk_app_context = app_name ; OPTIONAL ; maps the fields to observable types in ACE ; using the format field_name = observable_type [observable_mapping] root_domain = fqdn BluecoatProxy-URL = url userName = user sourceip = ipv4 destinationip = ipv4 ; OPTIONAL ; by default all observations are assumed to have happened when the hunt ; was executed which is probably not what you want ; any field set to yes in this section will also record the event time ; along with the added observable so that correlation targets the correct time [temporal_fields] sourceip = yes destinationip = yes ipv4_conversation = yes ; OPTIONAL ; use this section to add any number of directives (comma separated) to any ; observable that is added [directives] ipv4_conversation = extract_pcap Hunt Configuration Locations ------------------ The location of the hunts depends on the settings for each type of hunt. Typically these are located in the integration settings for the system the hunts execute against (for example etc/saq.qradar.default.ini). Look for the **rule_dirs** configuration settings in the [hunt_type_TYPE] configuration block. This specifies a comma separated list of directories to look for hunts in. ``` {.sourceCode .ini} [hunt_type_TYPE] module = saq.collectors.TYPE_hunter class = TYPEHunt rule_dirs = hunts/type concurrency_limit = type Debugging Hunts \u00b6 A hunt can be manually executed by using the execute subcommand of the hunt command. ``` {.sourceCode .bash} ace hunt execute --help For example, to execute the **query\\_stuff** hunt in the **splunk** hunting system you would issue the following command. ``` {.sourceCode .bash} ace hunt execute -s 04/17/2020:00:00:00 -e /04/18/2020:00:00:00 -z US/Eastern splunk:query_stuff By default what gets displayed is a list of the alerts that would have been generated. There are additional options to display more details of the alerts.","title":"Hunting"},{"location":"user/hunting/#hunting","text":"ACE has a service called hunter that executes hunts at a specific time or frequency. Each type of hunter is designed to execute a specific type of hunt. Currently the following hunting services are available in ACE.","title":"Hunting"},{"location":"user/hunting/#fundamentals","text":"A hunt is compromised of a configuration and an execution . The configuration defines the various aspects of the hunt such as what it should be identified as, what hunting system it's a part of, when it should run, how the results should be interpreted, etc. The execution defines what exactly the hunt is looking for. In the case of query-based hunts the execution would be a search string or a database query. The results of the execution then feed ACE with an analysis submission.","title":"Fundamentals"},{"location":"user/hunting/#supported-hunting-systems","text":"At this time the following hunting system are supported. Splunk IBM QRadar","title":"Supported Hunting Systems"},{"location":"user/hunting/#hunt-configuration","text":"Hunts are defined in individual ini files, one hunt per file. There is no limit on the number of hunts that can be defined. The following provides an example as well as the documentation of the fields in each hunt configuration. ``` {.sourceCode .ini} [rule] ; set this to either yes or no to enable or disable the hunt ; hunts that are disabled are not executed enabled = yes ; OPTIONAL ; a unique name for the hunt ; this also becomes the prefix for the name of the alert in ACE ; by default the name for the hunt is derived from the file name name = Super Awesome Hunt ; a useful description for the hunt ; this is a free-form text value that will be included as the ; \"instructions\" field in the alert ; you can use this field to describe what this hunt is looking for ; as well as ways to analyze the results description = This looks for a thing and then you have to analyze it. ; OPTIONAL ; defines what analysis mode submissions will be in when they reach ACE ; this controls what analysis modules run on the hunt results ; by default this is set to correlation which forces the hunts results ; to become alerts by default analysis_mode = correlation ; defines what hunter should execute this hunt ; use the ace hunt list-types to get the full list of supported types type = qradar ; OPTIONAL ; this maps to the type field of an ACE alert ; this is used to define custom templates to view the data in ACE ; by default this takes the value of hunter - [type] alert_type = hunter - qradar - bluecoat ; how often to execute the hunt ; this can be either a timespec in HH:MM:SS format ; or it can be a crontab entry format if you want to hunt to execute ; at a specific time of the day ; keep in mind that ACE uses UTC frequency = 00:30:00 ; a comma separated list of tags to add to the analysis if submitted to ACE tags = tag1, tag2, tag3 ; ; the following configuration options are valid only for query-based hunts ; ; set to yes to enable full coverage, no to disable ; this option ensures that the starting time of the next execution will be ; equal to the ending time of the last time the hunt successfully executed ; this will effectively bypass the time_range setting and use the frequency ; to determine the search times, except for the first time it runs full_coverage = yes ; set to yes to enable, no to disable ; use whatever is considered the \"index time\" for this hunt type ; events that are collected by log systems often record when they received ; the event, which is separate from when the event actually occurred ; this can be very different when logs are sent by batch methods ; use this method in conjunction with full_coverage to ensure that you ; search all of the log records with your hunts use_index_time = yes ; set the time range for the query in HH:MM:SS format ; specifies how far back to look when performing a query (by controlling ; the time field in whatever system is executing the hunt) ; if full_coverage is yes, then this value is only used the first time the ; hunt is executed time_range = 00:30:00 ; specified a maximum time range for a single query in [DD:]HH:MM:SS format ; queries that would exceed this time range are split into chunks and ; executed in series with time ranges no larger than max_time_range max_time_range = 24:00:00 ; OPTIONAL ; specifies an offset in HH:MM:SS format to execute queries on ; the final time range of all queries is offset by this value ; this option is useful if your logging system is slow to index data offset = 00:05:00 ; when a query hunt executes and returns multiple results, these results ; are then grouped together by the field specified in this option ; this also becomes part of the name of the alert ; by appending the value of the grouped field to the name of the hunt group_by = root_domain ; OPTIONAL ; specifies a path (relative to SAQ_HOME) to a file that contains the ; actual hunt to execute search = hunts/site/qradar/bluecoat-malicious_domain.sql ; OPTIONAL ; specifies the actual hunt to execute query = SELECT * FROM whatever WHERE something = 'this_or_that' ; ; the following configuration options are valid only for SPLUNK hunts ; ; OPTIONAL ; puts the splunk search into the context of the given user and/or app ; by default the splunk hunter uses the default (wildcard) user and app namespace splunk_user_context = user_name splunk_app_context = app_name ; OPTIONAL ; maps the fields to observable types in ACE ; using the format field_name = observable_type [observable_mapping] root_domain = fqdn BluecoatProxy-URL = url userName = user sourceip = ipv4 destinationip = ipv4 ; OPTIONAL ; by default all observations are assumed to have happened when the hunt ; was executed which is probably not what you want ; any field set to yes in this section will also record the event time ; along with the added observable so that correlation targets the correct time [temporal_fields] sourceip = yes destinationip = yes ipv4_conversation = yes ; OPTIONAL ; use this section to add any number of directives (comma separated) to any ; observable that is added [directives] ipv4_conversation = extract_pcap Hunt Configuration Locations ------------------ The location of the hunts depends on the settings for each type of hunt. Typically these are located in the integration settings for the system the hunts execute against (for example etc/saq.qradar.default.ini). Look for the **rule_dirs** configuration settings in the [hunt_type_TYPE] configuration block. This specifies a comma separated list of directories to look for hunts in. ``` {.sourceCode .ini} [hunt_type_TYPE] module = saq.collectors.TYPE_hunter class = TYPEHunt rule_dirs = hunts/type concurrency_limit = type","title":"Hunt Configuration"},{"location":"user/hunting/#debugging-hunts","text":"A hunt can be manually executed by using the execute subcommand of the hunt command. ``` {.sourceCode .bash} ace hunt execute --help For example, to execute the **query\\_stuff** hunt in the **splunk** hunting system you would issue the following command. ``` {.sourceCode .bash} ace hunt execute -s 04/17/2020:00:00:00 -e /04/18/2020:00:00:00 -z US/Eastern splunk:query_stuff By default what gets displayed is a list of the alerts that would have been generated. There are additional options to display more details of the alerts.","title":"Debugging Hunts"}]}