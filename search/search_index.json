{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analysis Correlation Engine \u00b6 ACE is a detection system and automation framework. The analysis engine analyzes data and presents the output to analysts in a manner that attempts to reduce the time to disposition to as close to zero as possible. ACE adheres to a defined design philosophy in an attempt to achieve this goal. ACE was designed to handle the ordinary, manual, redundant, and repetitive tasks of collecting, combining, and relating data. A contextual and intuitive presentation of all the important data is used to allow for a quick high confidence determination. Tools (some included in ACE) send analysis requests to ACE which then takes whatever is given and recursively analyzes the data. These requests may already be alerts that require additional correlation, or they may be something that could correlate into becoming an alert . ACE is the implementation of a proven detection strategy, a framework for automating analysis, a central platform to launch and manage incident response activates, an email scanner, and more. Base functionality provides the following: Recursive Data Analysis & Correlation Email Scanning Microsoft Office Document Analysis URL Crawling and Content Caching Intuitive Alert Presentation Central Analyst Interface Event/Incident management Intel Ingestion Modular Design for extending automation","title":"Analysis Correlation Engine"},{"location":"#analysis-correlation-engine","text":"ACE is a detection system and automation framework. The analysis engine analyzes data and presents the output to analysts in a manner that attempts to reduce the time to disposition to as close to zero as possible. ACE adheres to a defined design philosophy in an attempt to achieve this goal. ACE was designed to handle the ordinary, manual, redundant, and repetitive tasks of collecting, combining, and relating data. A contextual and intuitive presentation of all the important data is used to allow for a quick high confidence determination. Tools (some included in ACE) send analysis requests to ACE which then takes whatever is given and recursively analyzes the data. These requests may already be alerts that require additional correlation, or they may be something that could correlate into becoming an alert . ACE is the implementation of a proven detection strategy, a framework for automating analysis, a central platform to launch and manage incident response activates, an email scanner, and more. Base functionality provides the following: Recursive Data Analysis & Correlation Email Scanning Microsoft Office Document Analysis URL Crawling and Content Caching Intuitive Alert Presentation Central Analyst Interface Event/Incident management Intel Ingestion Modular Design for extending automation","title":"Analysis Correlation Engine"},{"location":"admin/","text":"Administrators Guide \u00b6 System Overview \u00b6 ACE is comprised of services , web applications and scheduled tasks . Services can be started and stopped . Some services have dependencies on other services. These dependencies are listed in the configuration of the service. The services that are part of the base ACE installation are documented here. Additional services may be available if additional integrations are installed and enabled. Core Services \u00b6 ecs : encryption password caching service network_semaphore : a global network semaphore for limiting access to resources yara_scanner : the local yara scanning services engine : local engine node remediation : service for performing remediation tasks hunter : service for running hunts at regular intervals","title":"Administrators Guide"},{"location":"admin/#administrators-guide","text":"","title":"Administrators Guide"},{"location":"admin/#system-overview","text":"ACE is comprised of services , web applications and scheduled tasks . Services can be started and stopped . Some services have dependencies on other services. These dependencies are listed in the configuration of the service. The services that are part of the base ACE installation are documented here. Additional services may be available if additional integrations are installed and enabled.","title":"System Overview"},{"location":"admin/#core-services","text":"ecs : encryption password caching service network_semaphore : a global network semaphore for limiting access to resources yara_scanner : the local yara scanning services engine : local engine node remediation : service for performing remediation tasks hunter : service for running hunts at regular intervals","title":"Core Services"},{"location":"admin/brotex_whitelisting/","text":"Brotex Whitelisting \u00b6 See here for design notes. Brotex whitelisting is used to whitelist scanned emails. cloudphish or crawlphish link retrieval. Matching is performed using simple case-insensitive substring comparison. Configuration \u00b6 The whitelist is stored in the file etc/brotex.whitelist and has the following format. ACE monitors this file and automatically reloads it when it is modified. # blank lines and lines starting with # are ignored type:value type is one of the supported types as defined below smtp_from : matches the from address of an email smtp_to : matches the to address of an email and any envelopment SMTP MAIL FROM smtp_subject : matches the decoded subject of an email (see below) http_host : matches the host part of a URL","title":"Brotex Whitelisting"},{"location":"admin/brotex_whitelisting/#brotex-whitelisting","text":"See here for design notes. Brotex whitelisting is used to whitelist scanned emails. cloudphish or crawlphish link retrieval. Matching is performed using simple case-insensitive substring comparison.","title":"Brotex Whitelisting"},{"location":"admin/brotex_whitelisting/#configuration","text":"The whitelist is stored in the file etc/brotex.whitelist and has the following format. ACE monitors this file and automatically reloads it when it is modified. # blank lines and lines starting with # are ignored type:value type is one of the supported types as defined below smtp_from : matches the from address of an email smtp_to : matches the to address of an email and any envelopment SMTP MAIL FROM smtp_subject : matches the decoded subject of an email (see below) http_host : matches the host part of a URL","title":"Configuration"},{"location":"admin/crawlphish_filter/","text":"Crawlphish Filtering \u00b6 The crawlphish analysis module has a custom filterting system designed to minimize the amount of URLs ACE automatically crawls, while also ensuring that suspect URL content is downloaded and analyzed. The system consists of the following components A list of patterns of URLs that should be whitelisted . A list of patterns of URLs that should be blacklisted . Criteria to force download of certain types of URLs. A database of visited URLs. An intelligence indicator cache. In this design we define whitelisted to mean URLs we want to analyze and blacklisted to mean URLs we do not want to analyze. Configuration \u00b6 The location of the files that contain the filtering patterns is stored in the [analysis_module_crawlphish] section of the configuration . All file paths are relative to the installation directory . ; path to whitelisted netloc whitelist_path = etc/crawlphish.whitelist ; path to whitelisted path regexes regex_path = etc/crawlphish.path_regex ; path to blacklisted netloc blacklist_path = etc/crawlphish.blacklist The whitelist_path file contains a list of domain names that should be whitelisted . Any URL that has a hostname that matches anything in this list will always be analyzed by ACE. The blacklist_path file contains a list of domain names that should never be analyzed. The regex_path file contains a list of python-compatible regular expressions. Each expression is applied to the full URL. A URL that matches any of these regular expressions will be analyzed by ACE. ACE monitors these files for changes and automatically reloads them when they are modified.","title":"Crawlphish Filtering"},{"location":"admin/crawlphish_filter/#crawlphish-filtering","text":"The crawlphish analysis module has a custom filterting system designed to minimize the amount of URLs ACE automatically crawls, while also ensuring that suspect URL content is downloaded and analyzed. The system consists of the following components A list of patterns of URLs that should be whitelisted . A list of patterns of URLs that should be blacklisted . Criteria to force download of certain types of URLs. A database of visited URLs. An intelligence indicator cache. In this design we define whitelisted to mean URLs we want to analyze and blacklisted to mean URLs we do not want to analyze.","title":"Crawlphish Filtering"},{"location":"admin/crawlphish_filter/#configuration","text":"The location of the files that contain the filtering patterns is stored in the [analysis_module_crawlphish] section of the configuration . All file paths are relative to the installation directory . ; path to whitelisted netloc whitelist_path = etc/crawlphish.whitelist ; path to whitelisted path regexes regex_path = etc/crawlphish.path_regex ; path to blacklisted netloc blacklist_path = etc/crawlphish.blacklist The whitelist_path file contains a list of domain names that should be whitelisted . Any URL that has a hostname that matches anything in this list will always be analyzed by ACE. The blacklist_path file contains a list of domain names that should never be analyzed. The regex_path file contains a list of python-compatible regular expressions. Each expression is applied to the full URL. A URL that matches any of these regular expressions will be analyzed by ACE. ACE monitors these files for changes and automatically reloads them when they are modified.","title":"Configuration"},{"location":"admin/encryption/","text":"Encryption \u00b6 See the design guide for how encryption is implemented and used in ACE. Configuration \u00b6 The encrypted_passwords_db option in the [global] configuration section specifies which database connection to use to access the encryption settings. The value of this option corresponds to the database configuration section. By default this is set to ace which uses the [database_ace] configuration settings. [global] encrypted_passwords_db = ace Setting the Encryption Password \u00b6 The primary encryption password can be set using the following command. The password is prompted for. ace enc set Changing the Encryption Password \u00b6 The same command can be used to change the password at any time. Listing Encrypted Passwords \u00b6 ace enc config list The current list of passwords can be viewed by listing them. If the encryption key is loaded then the actual (decrypted) values of the passwords are displayed. Adding Encrypted Passwords \u00b6 ace enc config set key Stores a password in the database using encryption. key is the name of the password to be stored. The value is prompted for. Removing Encrypted Passwords \u00b6 ace enc config delete key Removes an encrypted password from the database . key is the name of the password to be deleted. Importing and Exporting Passwords In Bulk \u00b6 ace enc config export output_file.json ace enc config import output_file.json The entire list of encrypted passwords can be exported into a JSON formatted file. This file can then be imported into ACE. Note that the exported data is plain text.","title":"Encryption"},{"location":"admin/encryption/#encryption","text":"See the design guide for how encryption is implemented and used in ACE.","title":"Encryption"},{"location":"admin/encryption/#configuration","text":"The encrypted_passwords_db option in the [global] configuration section specifies which database connection to use to access the encryption settings. The value of this option corresponds to the database configuration section. By default this is set to ace which uses the [database_ace] configuration settings. [global] encrypted_passwords_db = ace","title":"Configuration"},{"location":"admin/encryption/#setting-the-encryption-password","text":"The primary encryption password can be set using the following command. The password is prompted for. ace enc set","title":"Setting the Encryption Password"},{"location":"admin/encryption/#changing-the-encryption-password","text":"The same command can be used to change the password at any time.","title":"Changing the Encryption Password"},{"location":"admin/encryption/#listing-encrypted-passwords","text":"ace enc config list The current list of passwords can be viewed by listing them. If the encryption key is loaded then the actual (decrypted) values of the passwords are displayed.","title":"Listing Encrypted Passwords"},{"location":"admin/encryption/#adding-encrypted-passwords","text":"ace enc config set key Stores a password in the database using encryption. key is the name of the password to be stored. The value is prompted for.","title":"Adding Encrypted Passwords"},{"location":"admin/encryption/#removing-encrypted-passwords","text":"ace enc config delete key Removes an encrypted password from the database . key is the name of the password to be deleted.","title":"Removing Encrypted Passwords"},{"location":"admin/encryption/#importing-and-exporting-passwords-in-bulk","text":"ace enc config export output_file.json ace enc config import output_file.json The entire list of encrypted passwords can be exported into a JSON formatted file. This file can then be imported into ACE. Note that the exported data is plain text.","title":"Importing and Exporting Passwords In Bulk"},{"location":"admin/exchange_remediation/","text":"Exchange Remediation \u00b6 ACE can remove and restore emails in an on-prem Microsoft Exchange environment through the remediation system . Impersonation rights are used to log into target mailboxes and make the modifications. This functionality requires a highly privileged account to work. Configuration \u00b6 TODO.","title":"Exchange Remediation"},{"location":"admin/exchange_remediation/#exchange-remediation","text":"ACE can remove and restore emails in an on-prem Microsoft Exchange environment through the remediation system . Impersonation rights are used to log into target mailboxes and make the modifications. This functionality requires a highly privileged account to work.","title":"Exchange Remediation"},{"location":"admin/exchange_remediation/#configuration","text":"TODO.","title":"Configuration"},{"location":"admin/hunting/","text":"Hunting \u00b6 See the design guide for an overview of the hunting system. Hunt Manager Configuration \u00b6 A hunt manager is defined by a configuration section in the following format. [hunt_type_TYPE] module = saq.collectors.TYPE_hunter class = TYPEHunt rule_dirs = hunts/type concurrency_limit = type TYPE is the type of the hunts it manages and should match the type configuration setting for the hunt (see below.) There can only be one hunt manager per type. module and class follow the module-class specification . rule_dirs is a comma separated list of directories relative to the installation directory that contains the configurations for the hunts for this manager. When a hunt manager loads hunts from rule_dirs , it only loads hunts of the same type. This allows hunt managers to be able to share rule directories. concurrency_limit is an optional setting that specifies the type of concurrency that should be used for this type of hunting. If the value is a string then it is the name of a network semaphore to be acquired before performing the hunt. If the value is an integer then is a limit that is enforced using local semaphores . Hunt Configuration \u00b6 Each hunt is defined by an ini file. There is no limit on the number of hunts that can be defined. The following provides an example as well as the documentation of the fields in a hunt configuration. [rule] ; set this to either yes or no to enable or disable the hunt ; hunts that are disabled are not executed enabled = yes ; OPTIONAL ; a unique name for the hunt ; this also becomes the prefix for the name of the alert in ACE ; by default the name for the hunt is derived from the file name name = Super Awesome Hunt ; a useful description for the hunt ; this is a free-form text value that will be included as the ; \"instructions\" field in the alert ; you can use this field to describe what this hunt is looking for ; as well as ways to analyze the results description = This looks for a thing and then you have to analyze it. ; OPTIONAL ; defines what analysis mode submissions will be in when they reach ACE ; this controls what analysis modules run on the hunt results ; by default this is set to **correlation** which forces the hunts results ; to become alerts by default analysis_mode = correlation ; defines what hunter should execute this hunt ; use the ace hunt list-types to get the full list of supported types type = qradar ; OPTIONAL ; this maps to the type field of an ACE alert ; this is used to define custom templates to view the data in ACE ; by default this takes the value of hunter - [type] alert_type = hunter - qradar - bluecoat ; how often to execute the hunt ; this can be either a timespec in HH:MM:SS format ; or it can be a crontab entry format if you want to hunt to execute ; at a specific time of the day ; keep in mind that ACE uses UTC frequency = 00:30:00 ; a comma separated list of tags to add to the analysis if submitted to ACE tags = tag1, tag2, tag3 ; ; the following configuration options are valid only for query-based hunts ; ; set to yes to enable full coverage, no to disable ; this option ensures that the starting time of the next execution will be ; equal to the ending time of the last time the hunt successfully executed ; this will effectively bypass the time_range setting and use the frequency ; to determine the search times, except for the first time it runs full_coverage = yes ; set to yes to enable, no to disable ; use whatever is considered the \"index time\" for this hunt type ; events that are collected by log systems often record when they received ; the event, which is separate from when the event actually occurred ; this can be very different when logs are sent by batch methods ; use this method in conjunction with full_coverage to ensure that you ; search all of the log records with your hunts use_index_time = yes ; set the time range for the query in HH:MM:SS format ; specifies how far back to look when performing a query (by controlling ; the time field in whatever system is executing the hunt) ; if full_coverage is yes, then this value is only used the first time the ; hunt is executed time_range = 00:30:00 ; specified a maximum time range for a single query in [DD:]HH:MM:SS format ; queries that would exceed this time range are split into chunks and ; executed in series with time ranges no larger than max_time_range max_time_range = 24:00:00 ; OPTIONAL ; specifies an offset in HH:MM:SS format to execute queries on ; the final time range of all queries is offset by this value ; this option is useful if your logging system is slow to index data offset = 00:05:00 ; when a query hunt executes and returns multiple results, these results ; are then grouped together by the field specified in this option ; this also becomes part of the name of the alert ; by appending the value of the grouped field to the name of the hunt group_by = root_domain ; OPTIONAL ; specifies a path (relative to SAQ_HOME) to a file that contains the ; actual hunt to execute search = hunts/site/qradar/bluecoat-malicious_domain.sql ; OPTIONAL ; specifies the actual hunt to execute query = SELECT * FROM whatever WHERE something = 'this_or_that' ; ; the following configuration options are valid only for SPLUNK hunts ; ; OPTIONAL ; puts the splunk search into the context of the given user and/or app ; by default the splunk hunter uses the default (wildcard) user and app namespace splunk_user_context = user_name splunk_app_context = app_name ; OPTIONAL ; maps the fields to observable types in ACE ; using the format field_name = observable_type [observable_mapping] root_domain = fqdn BluecoatProxy-URL = url userName = user sourceip = ipv4 destinationip = ipv4 ; OPTIONAL ; by default all observations are assumed to have happened when the hunt ; was executed which is probably not what you want ; any field set to yes in this section will also record the event time ; along with the added observable so that correlation targets the correct time [temporal_fields] sourceip = yes destinationip = yes ipv4_conversation = yes ; OPTIONAL ; use this section to add any number of directives (comma separated) to any ; observable that is added [directives] ipv4_conversation = extract_pcap Managing Hunts \u00b6 See the Hunter's Guide for details on how to build, test and deploy hunts.","title":"Hunting"},{"location":"admin/hunting/#hunting","text":"See the design guide for an overview of the hunting system.","title":"Hunting"},{"location":"admin/hunting/#hunt-manager-configuration","text":"A hunt manager is defined by a configuration section in the following format. [hunt_type_TYPE] module = saq.collectors.TYPE_hunter class = TYPEHunt rule_dirs = hunts/type concurrency_limit = type TYPE is the type of the hunts it manages and should match the type configuration setting for the hunt (see below.) There can only be one hunt manager per type. module and class follow the module-class specification . rule_dirs is a comma separated list of directories relative to the installation directory that contains the configurations for the hunts for this manager. When a hunt manager loads hunts from rule_dirs , it only loads hunts of the same type. This allows hunt managers to be able to share rule directories. concurrency_limit is an optional setting that specifies the type of concurrency that should be used for this type of hunting. If the value is a string then it is the name of a network semaphore to be acquired before performing the hunt. If the value is an integer then is a limit that is enforced using local semaphores .","title":"Hunt Manager Configuration"},{"location":"admin/hunting/#hunt-configuration","text":"Each hunt is defined by an ini file. There is no limit on the number of hunts that can be defined. The following provides an example as well as the documentation of the fields in a hunt configuration. [rule] ; set this to either yes or no to enable or disable the hunt ; hunts that are disabled are not executed enabled = yes ; OPTIONAL ; a unique name for the hunt ; this also becomes the prefix for the name of the alert in ACE ; by default the name for the hunt is derived from the file name name = Super Awesome Hunt ; a useful description for the hunt ; this is a free-form text value that will be included as the ; \"instructions\" field in the alert ; you can use this field to describe what this hunt is looking for ; as well as ways to analyze the results description = This looks for a thing and then you have to analyze it. ; OPTIONAL ; defines what analysis mode submissions will be in when they reach ACE ; this controls what analysis modules run on the hunt results ; by default this is set to **correlation** which forces the hunts results ; to become alerts by default analysis_mode = correlation ; defines what hunter should execute this hunt ; use the ace hunt list-types to get the full list of supported types type = qradar ; OPTIONAL ; this maps to the type field of an ACE alert ; this is used to define custom templates to view the data in ACE ; by default this takes the value of hunter - [type] alert_type = hunter - qradar - bluecoat ; how often to execute the hunt ; this can be either a timespec in HH:MM:SS format ; or it can be a crontab entry format if you want to hunt to execute ; at a specific time of the day ; keep in mind that ACE uses UTC frequency = 00:30:00 ; a comma separated list of tags to add to the analysis if submitted to ACE tags = tag1, tag2, tag3 ; ; the following configuration options are valid only for query-based hunts ; ; set to yes to enable full coverage, no to disable ; this option ensures that the starting time of the next execution will be ; equal to the ending time of the last time the hunt successfully executed ; this will effectively bypass the time_range setting and use the frequency ; to determine the search times, except for the first time it runs full_coverage = yes ; set to yes to enable, no to disable ; use whatever is considered the \"index time\" for this hunt type ; events that are collected by log systems often record when they received ; the event, which is separate from when the event actually occurred ; this can be very different when logs are sent by batch methods ; use this method in conjunction with full_coverage to ensure that you ; search all of the log records with your hunts use_index_time = yes ; set the time range for the query in HH:MM:SS format ; specifies how far back to look when performing a query (by controlling ; the time field in whatever system is executing the hunt) ; if full_coverage is yes, then this value is only used the first time the ; hunt is executed time_range = 00:30:00 ; specified a maximum time range for a single query in [DD:]HH:MM:SS format ; queries that would exceed this time range are split into chunks and ; executed in series with time ranges no larger than max_time_range max_time_range = 24:00:00 ; OPTIONAL ; specifies an offset in HH:MM:SS format to execute queries on ; the final time range of all queries is offset by this value ; this option is useful if your logging system is slow to index data offset = 00:05:00 ; when a query hunt executes and returns multiple results, these results ; are then grouped together by the field specified in this option ; this also becomes part of the name of the alert ; by appending the value of the grouped field to the name of the hunt group_by = root_domain ; OPTIONAL ; specifies a path (relative to SAQ_HOME) to a file that contains the ; actual hunt to execute search = hunts/site/qradar/bluecoat-malicious_domain.sql ; OPTIONAL ; specifies the actual hunt to execute query = SELECT * FROM whatever WHERE something = 'this_or_that' ; ; the following configuration options are valid only for SPLUNK hunts ; ; OPTIONAL ; puts the splunk search into the context of the given user and/or app ; by default the splunk hunter uses the default (wildcard) user and app namespace splunk_user_context = user_name splunk_app_context = app_name ; OPTIONAL ; maps the fields to observable types in ACE ; using the format field_name = observable_type [observable_mapping] root_domain = fqdn BluecoatProxy-URL = url userName = user sourceip = ipv4 destinationip = ipv4 ; OPTIONAL ; by default all observations are assumed to have happened when the hunt ; was executed which is probably not what you want ; any field set to yes in this section will also record the event time ; along with the added observable so that correlation targets the correct time [temporal_fields] sourceip = yes destinationip = yes ipv4_conversation = yes ; OPTIONAL ; use this section to add any number of directives (comma separated) to any ; observable that is added [directives] ipv4_conversation = extract_pcap","title":"Hunt Configuration"},{"location":"admin/hunting/#managing-hunts","text":"See the Hunter's Guide for details on how to build, test and deploy hunts.","title":"Managing Hunts"},{"location":"admin/integration/","text":"Integrations \u00b6 See the design guide for an overview of integrations. Integrations work closely with how configuration files are loaded . When an integration is enabled two additional configuration files are loaded that have the filename etc/saq.INTEGRATION.default.ini and etc/saq.INTEGRATION.ini where INTEGRATION is the name of the integration the saq.INTEGRATION.default.ini file contains the global default settings for the integration. the saq.INTEGRATION.ini file contains local site settings that override the default. Viewing Available Integartions \u00b6 You can display the name and status of each available integartion. The status can be either enabled or disabled . ace integration list Enabling and Disabling Integrations \u00b6 You can enable and disable integrations. NAME is the name of the integration as shown by the list command. ace integration enable NAME ace integration disable NAME Adding and Removing Integrations \u00b6 The list of available integrations is stored in etc/saq.integrations.ini . The enabled and disable commands change the values in this file, and the list command simply formats a list of the contents of this file.","title":"Integrations"},{"location":"admin/integration/#integrations","text":"See the design guide for an overview of integrations. Integrations work closely with how configuration files are loaded . When an integration is enabled two additional configuration files are loaded that have the filename etc/saq.INTEGRATION.default.ini and etc/saq.INTEGRATION.ini where INTEGRATION is the name of the integration the saq.INTEGRATION.default.ini file contains the global default settings for the integration. the saq.INTEGRATION.ini file contains local site settings that override the default.","title":"Integrations"},{"location":"admin/integration/#viewing-available-integartions","text":"You can display the name and status of each available integartion. The status can be either enabled or disabled . ace integration list","title":"Viewing Available Integartions"},{"location":"admin/integration/#enabling-and-disabling-integrations","text":"You can enable and disable integrations. NAME is the name of the integration as shown by the list command. ace integration enable NAME ace integration disable NAME","title":"Enabling and Disabling Integrations"},{"location":"admin/integration/#adding-and-removing-integrations","text":"The list of available integrations is stored in etc/saq.integrations.ini . The enabled and disable commands change the values in this file, and the list command simply formats a list of the contents of this file.","title":"Adding and Removing Integrations"},{"location":"admin/network_semaphore/","text":"Network Semaphore \u00b6 The overview and design of the network semaphore service is documented in the design guide . Configuration Options \u00b6 The [service_network_semaphore] configuration section contains the settings for the network semaphore service. [service_network_semaphore] module = saq.network_semaphore class = NetworkSemaphoreServer description = Network Semaphore - global network service for controlling concurrent access to limited resources enabled = yes ; the address of the network semaphore server (used to bind and listen) bind_address = 127.0.0.1 bind_port = 53559 ; the address of the network semaphore server to the clients that want to use them ; could be the same as the bind_adress and bind_port above remote_address = 127.0.0.1 remote_port = 53559 ; comma separated list of source IP addresses that are allowed to connect allowed_ipv4 = 127.0.0.1 ; directory that contains metrics and current status of semaphores stats_dir = var/network_semaphore The service can be found to a specific interface and port as defined by the bind_address and bind_port options. A value of 0.0.0.0 for the bind_address option binds the service to all available network interfaces. ACE uses the remote_address and remote_port options when requesting network semaphore locks. Note that these settings are valid even if the enabled boolean option is set to False. You must define precisely what source addresses are allowed to connect to the service using the comma separated list of IP addresses in the allowed_ipv4 option. stats_dir defines a directory (relative to ../design/data_dir.md) that contains various statistical information regarding the usage of the semaphores. Logging and Monitoring \u00b6 The standard logging configuration options apply. You can view the current status of all defined semaphores by reading the semaphore.status file in the directory defined by the stats_dir configuration setting.","title":"Network Semaphore"},{"location":"admin/network_semaphore/#network-semaphore","text":"The overview and design of the network semaphore service is documented in the design guide .","title":"Network Semaphore"},{"location":"admin/network_semaphore/#configuration-options","text":"The [service_network_semaphore] configuration section contains the settings for the network semaphore service. [service_network_semaphore] module = saq.network_semaphore class = NetworkSemaphoreServer description = Network Semaphore - global network service for controlling concurrent access to limited resources enabled = yes ; the address of the network semaphore server (used to bind and listen) bind_address = 127.0.0.1 bind_port = 53559 ; the address of the network semaphore server to the clients that want to use them ; could be the same as the bind_adress and bind_port above remote_address = 127.0.0.1 remote_port = 53559 ; comma separated list of source IP addresses that are allowed to connect allowed_ipv4 = 127.0.0.1 ; directory that contains metrics and current status of semaphores stats_dir = var/network_semaphore The service can be found to a specific interface and port as defined by the bind_address and bind_port options. A value of 0.0.0.0 for the bind_address option binds the service to all available network interfaces. ACE uses the remote_address and remote_port options when requesting network semaphore locks. Note that these settings are valid even if the enabled boolean option is set to False. You must define precisely what source addresses are allowed to connect to the service using the comma separated list of IP addresses in the allowed_ipv4 option. stats_dir defines a directory (relative to ../design/data_dir.md) that contains various statistical information regarding the usage of the semaphores.","title":"Configuration Options"},{"location":"admin/network_semaphore/#logging-and-monitoring","text":"The standard logging configuration options apply. You can view the current status of all defined semaphores by reading the semaphore.status file in the directory defined by the stats_dir configuration setting.","title":"Logging and Monitoring"},{"location":"admin/office365_remediation/","text":"Office365 Email Remediation \u00b6 ACE can remove and restore emails in an Office365 environment through the remediation system . Impersonation rights are used to log into target mailboxes and make the modifications. This functionality requires a highly privileged account to work. Configuration \u00b6 TODO.","title":"Office365 Email Remediation"},{"location":"admin/office365_remediation/#office365-email-remediation","text":"ACE can remove and restore emails in an Office365 environment through the remediation system . Impersonation rights are used to log into target mailboxes and make the modifications. This functionality requires a highly privileged account to work.","title":"Office365 Email Remediation"},{"location":"admin/office365_remediation/#configuration","text":"TODO.","title":"Configuration"},{"location":"admin/persistence/","text":"Persistence Data \u00b6 See the design guide for an overview of persistence data. Viewing Persistence \u00b6 ace persistence list --help This command is used to view persistence data and sources . Viewing Persistence Sources \u00b6 ace persistence list Without any options this command lists all current defined persistence sources . Viewing Persistence Data \u00b6 # search for persistence key for a given source # NOTE that by default only volatile keys are displayed ace persistence list -s SOURCE # adding -k option also displays permanent keys ace persistence list -k -s SOURCE # search for keys by name using the -n option ace persistence list -k -s SOURCE -n NAME Managing Persistence Data \u00b6 ace persistence clear --help This command allows you to clear persistence keys according to some criteria. You can use the --dry-run option to test options before actually committing the changes. # clear a single persistence key (volatile or permanent) ace persistence clear SOURCE KEY # clear all persistence data for the given source (volatile and permanent) ace persistence clear SOURCE --all # clear volatile data that is older than some date # the format of the date can by any date specification that is understood by dateparser # see https://dateparser.readthedocs.io/en/latest/#features ace persistence clear SOURCE --older-than \"two weeks ago\"","title":"Persistence Data"},{"location":"admin/persistence/#persistence-data","text":"See the design guide for an overview of persistence data.","title":"Persistence Data"},{"location":"admin/persistence/#viewing-persistence","text":"ace persistence list --help This command is used to view persistence data and sources .","title":"Viewing Persistence"},{"location":"admin/persistence/#viewing-persistence-sources","text":"ace persistence list Without any options this command lists all current defined persistence sources .","title":"Viewing Persistence Sources"},{"location":"admin/persistence/#viewing-persistence-data","text":"# search for persistence key for a given source # NOTE that by default only volatile keys are displayed ace persistence list -s SOURCE # adding -k option also displays permanent keys ace persistence list -k -s SOURCE # search for keys by name using the -n option ace persistence list -k -s SOURCE -n NAME","title":"Viewing Persistence Data"},{"location":"admin/persistence/#managing-persistence-data","text":"ace persistence clear --help This command allows you to clear persistence keys according to some criteria. You can use the --dry-run option to test options before actually committing the changes. # clear a single persistence key (volatile or permanent) ace persistence clear SOURCE KEY # clear all persistence data for the given source (volatile and permanent) ace persistence clear SOURCE --all # clear volatile data that is older than some date # the format of the date can by any date specification that is understood by dateparser # see https://dateparser.readthedocs.io/en/latest/#features ace persistence clear SOURCE --older-than \"two weeks ago\"","title":"Managing Persistence Data"},{"location":"admin/service/","text":"Services \u00b6 Configuration Options \u00b6 Services are identified in the ACE configuration by sections formatted as [service_NAME] where NAME is unique. The following service configuration is used as an example. [service_email_collector] enabled = yes module = saq.collectors.email class = EmailCollector description = Email Collector (AMS) - collects emails from remote AMS systems dependencies = The enabled boolean options allows you to enable and disable services. The module and class specification follows the standard ACE module-class specification for identifying the python module and class that implements the saq.service.Service class. The description option contains a human readable description of the service. The dependencies option contains a comma separated list of services that should be started before this service is started. Services that are already started are ignored. This option is only used when the service is started in background (daemon) mode. Service Management \u00b6 You can start, stop and view the status of a service using the ace command. Listing Services \u00b6 ace service status All services are listed along with their current status. The status can be any of the following values. stopped : the service is not actively running running : the service is actively running stale : the service was started in daemon (background) mode but it died disabled : the service is disabled in the configuration Starting A Service \u00b6 ace service start NAME A service can be started in debug , foreground , or background (daemon) mode as documented here . Stopping A Service \u00b6 This only applies to services running in the background. ace service stop NAME Restarting A Service \u00b6 This only applies to services running in the background. ace service restart NAME Logging Configurations for Background Services \u00b6 When a service is started in the background, a special logging configuration is used. The etc/logging_configs directory contains logging configurations for the services that come standard with ACE. Each service has a configuration file formatted as etc/logging_configs/service_NAME.default.ini where NAME is the name of the service. The first time the service is started in background (daemon) mode, this file is copied to the same directory with the .default stripped out of the name: etc/logging_configs/service_NAME.ini . This is the logging configuration that is used for this service running in background mode. This allows customization of the logging for that particular service.","title":"Services"},{"location":"admin/service/#services","text":"","title":"Services"},{"location":"admin/service/#configuration-options","text":"Services are identified in the ACE configuration by sections formatted as [service_NAME] where NAME is unique. The following service configuration is used as an example. [service_email_collector] enabled = yes module = saq.collectors.email class = EmailCollector description = Email Collector (AMS) - collects emails from remote AMS systems dependencies = The enabled boolean options allows you to enable and disable services. The module and class specification follows the standard ACE module-class specification for identifying the python module and class that implements the saq.service.Service class. The description option contains a human readable description of the service. The dependencies option contains a comma separated list of services that should be started before this service is started. Services that are already started are ignored. This option is only used when the service is started in background (daemon) mode.","title":"Configuration Options"},{"location":"admin/service/#service-management","text":"You can start, stop and view the status of a service using the ace command.","title":"Service Management"},{"location":"admin/service/#listing-services","text":"ace service status All services are listed along with their current status. The status can be any of the following values. stopped : the service is not actively running running : the service is actively running stale : the service was started in daemon (background) mode but it died disabled : the service is disabled in the configuration","title":"Listing Services"},{"location":"admin/service/#starting-a-service","text":"ace service start NAME A service can be started in debug , foreground , or background (daemon) mode as documented here .","title":"Starting A Service"},{"location":"admin/service/#stopping-a-service","text":"This only applies to services running in the background. ace service stop NAME","title":"Stopping A Service"},{"location":"admin/service/#restarting-a-service","text":"This only applies to services running in the background. ace service restart NAME","title":"Restarting A Service"},{"location":"admin/service/#logging-configurations-for-background-services","text":"When a service is started in the background, a special logging configuration is used. The etc/logging_configs directory contains logging configurations for the services that come standard with ACE. Each service has a configuration file formatted as etc/logging_configs/service_NAME.default.ini where NAME is the name of the service. The first time the service is started in background (daemon) mode, this file is copied to the same directory with the .default stripped out of the name: etc/logging_configs/service_NAME.ini . This is the logging configuration that is used for this service running in background mode. This allows customization of the logging for that particular service.","title":"Logging Configurations for Background Services"},{"location":"admin/smtp_collector/","text":"SMTP Collector \u00b6 See the design guide for an overview of using ACE to scan emails extracted from SMTP session data. Installation \u00b6 Install zeek into /opt/bro . Symlink the /opt/ace/bro directory to /opt/bro/share/zeek/site/ace . Edit /opt/bro/share/zeek/site/local.zeek and add the following lines at the end of the file. redef ignore_checksums = T; @load ace/ace_local @load ace/ace_smtp Edit bro/ace_local.zeek and modify the record_smtp_stream if required. Start or restart zeek. Configuration \u00b6 The [bro] section of the configuration settings define what directory the SMTP session files are stored in relative to the data directory . [bro] ; the directory that contains the SMTP streams generated by bro/ace_smtp.bro (relative to DATA_DIR) smtp_dir = var/bro/smtp Monitoring and Maintenance \u00b6 The smtp_dir directory should be monitored for file age and file count. Files that are old should be investigated in the logs to discover why they were not processed. You can manually create the .ready files to force the system to process them. If the number of files in this directory is growing then the collector is either not running or unable to process the files fast enough.","title":"SMTP Collector"},{"location":"admin/smtp_collector/#smtp-collector","text":"See the design guide for an overview of using ACE to scan emails extracted from SMTP session data.","title":"SMTP Collector"},{"location":"admin/smtp_collector/#installation","text":"Install zeek into /opt/bro . Symlink the /opt/ace/bro directory to /opt/bro/share/zeek/site/ace . Edit /opt/bro/share/zeek/site/local.zeek and add the following lines at the end of the file. redef ignore_checksums = T; @load ace/ace_local @load ace/ace_smtp Edit bro/ace_local.zeek and modify the record_smtp_stream if required. Start or restart zeek.","title":"Installation"},{"location":"admin/smtp_collector/#configuration","text":"The [bro] section of the configuration settings define what directory the SMTP session files are stored in relative to the data directory . [bro] ; the directory that contains the SMTP streams generated by bro/ace_smtp.bro (relative to DATA_DIR) smtp_dir = var/bro/smtp","title":"Configuration"},{"location":"admin/smtp_collector/#monitoring-and-maintenance","text":"The smtp_dir directory should be monitored for file age and file count. Files that are old should be investigated in the logs to discover why they were not processed. You can manually create the .ready files to force the system to process them. If the number of files in this directory is growing then the collector is either not running or unable to process the files fast enough.","title":"Monitoring and Maintenance"},{"location":"admin/submission_filter/","text":"Submission Filtering \u00b6 Submissions are filtered in two places: by collectors prior to submission. by the api upon receiving a submission. Yara rules are used to filter out matching submissions. The locations of these yara rules are set in the [collection] section of the configuration . Options have the format tuning_dir_NAME where NAME is a unique value. Each of these options specifies a yara directory of rules to load for submission filtering. [collection] tuning_dir_default = etc/tuning/submission Submission Filtering Buffers \u00b6 ACE prepares buffers of data to present to the yara rules for filtering. These buffers are filled with various parts of the submission data and formatted in a documented way. Three types of buffers are available for scanning. Submission Buffer Format: submission \u00b6 The submission buffer contains the submission data in three sections: submission data observable data analysis details The submission data is formatted as follows. Note that this format is not JSON format. description = text analysis_mode = text tool = text tool_instance = text type = text event_time = YYYY-MM-DDTmm:HH:SS.FFFFFF+ZZZZ tags = tags is a comma separated list of tags. Observable data is formatted as a list of JSON objects. The JSON data is formatted in a human-readable format. [ { \"directives\": [], \"tags\": [], \"type\": \"type\", \"value\": \"text\" } ] Analysis data is formatted as the JSON object it is. The JSON data is formatted in a human-readable format. The exact contents of this data depend on the source of the submission. Submission Buffer Format: observable \u00b6 The observable buffer only contains the observables of the submission. Submission Buffer Format: files \u00b6 The files buffer contains the raw bytes of (optional) additional files included in the submission. Submission Buffer Format: all \u00b6 The all buffer contains a combination of the submission and files buffers together into one buffer. Submission Filtering with Yara Rules \u00b6 The meta directive targets of yara rules is used to apply a yara rule against a given buffer type. This value is a comma separated list of submission buffer types this rule should be used against. rule tuning_blah { meta: targets = \"submission,file\" author = \"John Davison\" description = \"Tuning out this thing blah blah\" In the above example the rule would apply to the submission and file buffers.","title":"Submission Filtering"},{"location":"admin/submission_filter/#submission-filtering","text":"Submissions are filtered in two places: by collectors prior to submission. by the api upon receiving a submission. Yara rules are used to filter out matching submissions. The locations of these yara rules are set in the [collection] section of the configuration . Options have the format tuning_dir_NAME where NAME is a unique value. Each of these options specifies a yara directory of rules to load for submission filtering. [collection] tuning_dir_default = etc/tuning/submission","title":"Submission Filtering"},{"location":"admin/submission_filter/#submission-filtering-buffers","text":"ACE prepares buffers of data to present to the yara rules for filtering. These buffers are filled with various parts of the submission data and formatted in a documented way. Three types of buffers are available for scanning.","title":"Submission Filtering Buffers"},{"location":"admin/submission_filter/#submission-buffer-format-submission","text":"The submission buffer contains the submission data in three sections: submission data observable data analysis details The submission data is formatted as follows. Note that this format is not JSON format. description = text analysis_mode = text tool = text tool_instance = text type = text event_time = YYYY-MM-DDTmm:HH:SS.FFFFFF+ZZZZ tags = tags is a comma separated list of tags. Observable data is formatted as a list of JSON objects. The JSON data is formatted in a human-readable format. [ { \"directives\": [], \"tags\": [], \"type\": \"type\", \"value\": \"text\" } ] Analysis data is formatted as the JSON object it is. The JSON data is formatted in a human-readable format. The exact contents of this data depend on the source of the submission.","title":"Submission Buffer Format: submission"},{"location":"admin/submission_filter/#submission-buffer-format-observable","text":"The observable buffer only contains the observables of the submission.","title":"Submission Buffer Format: observable"},{"location":"admin/submission_filter/#submission-buffer-format-files","text":"The files buffer contains the raw bytes of (optional) additional files included in the submission.","title":"Submission Buffer Format: files"},{"location":"admin/submission_filter/#submission-buffer-format-all","text":"The all buffer contains a combination of the submission and files buffers together into one buffer.","title":"Submission Buffer Format: all"},{"location":"admin/submission_filter/#submission-filtering-with-yara-rules","text":"The meta directive targets of yara rules is used to apply a yara rule against a given buffer type. This value is a comma separated list of submission buffer types this rule should be used against. rule tuning_blah { meta: targets = \"submission,file\" author = \"John Davison\" description = \"Tuning out this thing blah blah\" In the above example the rule would apply to the submission and file buffers.","title":"Submission Filtering with Yara Rules"},{"location":"admin/yara_scanning_config/","text":"Yara Scanning Configuration \u00b6 Yara configuration settings are in the [service_yara] section. ; relative directory where the unix sockets for the yara scanner server are located (relative to DATA_DIR) socket_dir = var/yss/sockets ; global configuration of yara rules (relative to SAQ_HOME or absolute path) ; each subdirectory in this directory has any yara rules loaded in the scanner signature_dir = etc/yara ; how often to check the yara rules for changes (in seconds) update_frequency = 60 ; parameter to the socket.listen() function (how many connections to backlog) backlog = 50 ; the blacklist contains a list of rule names (one per line) to exclude from the results blacklist_path = etc/yara.blacklist ; a directory that contains all the files that fail to scan (relative to DATA_DIR) scan_failure_dir = scan_failures Yara Rules \u00b6 Yara rules are stored in sub directories inside of the directory specified by the signature_dir option. Each sub directory is scanned for yara rules (files that end with .yar or .yara ). Only files in the sub directory are included. Files in nested sub directories are not included. Any number of sub directories inside of the signature_dir can be used. Rules are automatically reloaded when they change. If the sub directory is a git repository, rules will only be reloaded when the HEAD commit of the repository is changed. Blacklisting Yara Rules \u00b6 You can avoid loading certain yara rules by including the name of the yara rule in a file referenced by the blacklist_path configuration setting. This is useful if you have a repository of rules from an external source that changes often and you don't want to have to manage but you don't want to use all the rules. Scan Failures \u00b6 At times the process actually performing the scanning my unexpectedly die. This may happen for a number of reasons including local system stability. target buffer size. rule quality. When scanning fails ACE makes a copy of the file that was being scanned available in the directory specified by the scan_failure_dir configuration setting. This should be periodically reviewed to determine the cause of the failures.","title":"Yara Scanning Configuration"},{"location":"admin/yara_scanning_config/#yara-scanning-configuration","text":"Yara configuration settings are in the [service_yara] section. ; relative directory where the unix sockets for the yara scanner server are located (relative to DATA_DIR) socket_dir = var/yss/sockets ; global configuration of yara rules (relative to SAQ_HOME or absolute path) ; each subdirectory in this directory has any yara rules loaded in the scanner signature_dir = etc/yara ; how often to check the yara rules for changes (in seconds) update_frequency = 60 ; parameter to the socket.listen() function (how many connections to backlog) backlog = 50 ; the blacklist contains a list of rule names (one per line) to exclude from the results blacklist_path = etc/yara.blacklist ; a directory that contains all the files that fail to scan (relative to DATA_DIR) scan_failure_dir = scan_failures","title":"Yara Scanning Configuration"},{"location":"admin/yara_scanning_config/#yara-rules","text":"Yara rules are stored in sub directories inside of the directory specified by the signature_dir option. Each sub directory is scanned for yara rules (files that end with .yar or .yara ). Only files in the sub directory are included. Files in nested sub directories are not included. Any number of sub directories inside of the signature_dir can be used. Rules are automatically reloaded when they change. If the sub directory is a git repository, rules will only be reloaded when the HEAD commit of the repository is changed.","title":"Yara Rules"},{"location":"admin/yara_scanning_config/#blacklisting-yara-rules","text":"You can avoid loading certain yara rules by including the name of the yara rule in a file referenced by the blacklist_path configuration setting. This is useful if you have a repository of rules from an external source that changes often and you don't want to have to manage but you don't want to use all the rules.","title":"Blacklisting Yara Rules"},{"location":"admin/yara_scanning_config/#scan-failures","text":"At times the process actually performing the scanning my unexpectedly die. This may happen for a number of reasons including local system stability. target buffer size. rule quality. When scanning fails ACE makes a copy of the file that was being scanned available in the directory specified by the scan_failure_dir configuration setting. This should be periodically reviewed to determine the cause of the failures.","title":"Scan Failures"},{"location":"database/","text":"ACE Databases \u00b6 ACE uses multiple MySQL databases to operate. Each ACE cluster uses a different set of databases. Configuration \u00b6 Each database connection is defined in the configuration file in a section formatted as [database_NAME] where NAME is the unique name of the database. The following database names are supported. database_ace database_collection database_brocess database_email_archive Additional integrations may add more. All database configurations are formatted as follows. [database_NAME] hostname = unix_socket = database = username = password = ;ssl_key = ssl/mysql/client-key.pem ;ssl_cert = ssl/mysql/client-cert.pem ;ssl_ca = ssl/mysql/ca-cert.pem hostname is the DNS name of the host running the database. This can be localhost if the database is running locally. unix_socket is an optional setting the defines the full path to the unix socket connection to the database. This only applies when the hostname is set to localhost . database is the name of the database on the database server. username and password are the credentials used to access the database. NOTE that you cannot use encryption to store the database credentials because the encryption key is stored in the database . There are optional settings for ssl connections to the database. You should use SSL if the database is on a remote system. The ssl_ca should point to the certificate authority chain file. The ssl_key and ssl_cert should point to the key and certificate files. You must also configure MySQL for SSL before using these options. User Accounts \u00b6 By default the installation gives the user ace-user read/write access to all of the ACE-related databases. An additional user called ace-superuser is also created that has the same privilege levels as the database root user. This has a different password than the ace-user . Both users are allowed access from any host. MySQL Defaults Files \u00b6 MySQL defaults files are created for both users. You can use these files with the mysql command to access the database. etc/mysql_defaults : defaults for ace-user etc/mysql_defaults.root : defaults for ace-superuser Databases \u00b6 The following databases are used by ACE. ace brocess email_archive database_email_archive \u00b6 The email_archive database contains a recorded history of all emails received by either the zeek-based smtp collector or the office365-based email collector . database_collection \u00b6 The collection database points ACE at the database to use for executing collectors . If the entire ACE cluster runs on the same system, then these settings can be the same as the settings for [database_ace] . MySQL Considerations \u00b6 ACE currently uses a large number of database settings. Each ACE process could use from 20 to 50 connections. You may need to adjust the maximum number of MySQL connections to a much higher number until this issue is refactored out.","title":"ACE Databases"},{"location":"database/#ace-databases","text":"ACE uses multiple MySQL databases to operate. Each ACE cluster uses a different set of databases.","title":"ACE Databases"},{"location":"database/#configuration","text":"Each database connection is defined in the configuration file in a section formatted as [database_NAME] where NAME is the unique name of the database. The following database names are supported. database_ace database_collection database_brocess database_email_archive Additional integrations may add more. All database configurations are formatted as follows. [database_NAME] hostname = unix_socket = database = username = password = ;ssl_key = ssl/mysql/client-key.pem ;ssl_cert = ssl/mysql/client-cert.pem ;ssl_ca = ssl/mysql/ca-cert.pem hostname is the DNS name of the host running the database. This can be localhost if the database is running locally. unix_socket is an optional setting the defines the full path to the unix socket connection to the database. This only applies when the hostname is set to localhost . database is the name of the database on the database server. username and password are the credentials used to access the database. NOTE that you cannot use encryption to store the database credentials because the encryption key is stored in the database . There are optional settings for ssl connections to the database. You should use SSL if the database is on a remote system. The ssl_ca should point to the certificate authority chain file. The ssl_key and ssl_cert should point to the key and certificate files. You must also configure MySQL for SSL before using these options.","title":"Configuration"},{"location":"database/#user-accounts","text":"By default the installation gives the user ace-user read/write access to all of the ACE-related databases. An additional user called ace-superuser is also created that has the same privilege levels as the database root user. This has a different password than the ace-user . Both users are allowed access from any host.","title":"User Accounts"},{"location":"database/#mysql-defaults-files","text":"MySQL defaults files are created for both users. You can use these files with the mysql command to access the database. etc/mysql_defaults : defaults for ace-user etc/mysql_defaults.root : defaults for ace-superuser","title":"MySQL Defaults Files"},{"location":"database/#databases","text":"The following databases are used by ACE. ace brocess email_archive","title":"Databases"},{"location":"database/#database_email_archive","text":"The email_archive database contains a recorded history of all emails received by either the zeek-based smtp collector or the office365-based email collector .","title":"database_email_archive"},{"location":"database/#database_collection","text":"The collection database points ACE at the database to use for executing collectors . If the entire ACE cluster runs on the same system, then these settings can be the same as the settings for [database_ace] .","title":"database_collection"},{"location":"database/#mysql-considerations","text":"ACE currently uses a large number of database settings. Each ACE process could use from 20 to 50 connections. You may need to adjust the maximum number of MySQL connections to a much higher number until this issue is refactored out.","title":"MySQL Considerations"},{"location":"database/ace/","text":"ace Database \u00b6 The ace database contains most of the ace-related data objects including alerts . user settings. workload information. engine node status. observable and tag mappings. Database Table Documentation \u00b6 alerts \u00b6 The alerts table contains all of the alert meta data. Each row represents an alert . This database table is used as an index into the alert data and to keep track of state related to analyst dispositions . The authoritative source is currently the JSON data . comments \u00b6 This table holds any comments added by the analysts in the GUI . config \u00b6 This table holds various configuration data including encryption settings persistence data delayed_analysis \u00b6 Part of the analysis workload management queue tracking work that has been delayed . encrypted_passwords \u00b6 Storage location of encrypted passwords in the configuration file . incoming_workload \u00b6 Used by collectors to manage the incoming requests. locks \u00b6 Used by the [engine] as part of the workload management to synchronize access to work items. nodes \u00b6 Contains an entry for each engine in the cluster . These entries are populated by the engine and updated at a frequency specified by the node_status_update_frequency configuration in the [service_engine] section. node_modes \u00b6 Contains a listing of what analysis modes each node supports. Note that the nodes table contains a any_mode column which indicates that the engine supports any mode (this is the default). In this case the node would have no entries in this table, but could have entries in the node_modes_excluded table. node_modes_excluded \u00b6 Contains a list of what [analysis modes] (../design/analysis_mode.md) each node does not support. observables \u00b6 Contains an entry for each unique observable ever seen by ACE. observable_mapping \u00b6 Maps observables seen by ACE to each alert they have been seen in. observable_tag_index \u00b6 Unknown. observable_tag_mapping \u00b6 Unknown. persistence \u00b6 Contains the persistence data for this cluster . remediation \u00b6 Contains the remediation history for this cluster . tags \u00b6 Contains an entry for each unique tag that ACE has ever used. tag_mapping \u00b6 Maps tags used by ACE to each alert they have been used in. users \u00b6 Contains credentials and basic settings for all analysts (users) in ACE. work_distribution \u00b6 Used by collectors to manage the routing of submissions to engine clusters . workload \u00b6 The primary table for managing the workload assignment for the entire cluster .","title":"ace Database"},{"location":"database/ace/#ace-database","text":"The ace database contains most of the ace-related data objects including alerts . user settings. workload information. engine node status. observable and tag mappings.","title":"ace Database"},{"location":"database/ace/#database-table-documentation","text":"","title":"Database Table Documentation"},{"location":"database/ace/#alerts","text":"The alerts table contains all of the alert meta data. Each row represents an alert . This database table is used as an index into the alert data and to keep track of state related to analyst dispositions . The authoritative source is currently the JSON data .","title":"alerts"},{"location":"database/ace/#comments","text":"This table holds any comments added by the analysts in the GUI .","title":"comments"},{"location":"database/ace/#config","text":"This table holds various configuration data including encryption settings persistence data","title":"config"},{"location":"database/ace/#delayed_analysis","text":"Part of the analysis workload management queue tracking work that has been delayed .","title":"delayed_analysis"},{"location":"database/ace/#encrypted_passwords","text":"Storage location of encrypted passwords in the configuration file .","title":"encrypted_passwords"},{"location":"database/ace/#incoming_workload","text":"Used by collectors to manage the incoming requests.","title":"incoming_workload"},{"location":"database/ace/#locks","text":"Used by the [engine] as part of the workload management to synchronize access to work items.","title":"locks"},{"location":"database/ace/#nodes","text":"Contains an entry for each engine in the cluster . These entries are populated by the engine and updated at a frequency specified by the node_status_update_frequency configuration in the [service_engine] section.","title":"nodes"},{"location":"database/ace/#node_modes","text":"Contains a listing of what analysis modes each node supports. Note that the nodes table contains a any_mode column which indicates that the engine supports any mode (this is the default). In this case the node would have no entries in this table, but could have entries in the node_modes_excluded table.","title":"node_modes"},{"location":"database/ace/#node_modes_excluded","text":"Contains a list of what [analysis modes] (../design/analysis_mode.md) each node does not support.","title":"node_modes_excluded"},{"location":"database/ace/#observables","text":"Contains an entry for each unique observable ever seen by ACE.","title":"observables"},{"location":"database/ace/#observable_mapping","text":"Maps observables seen by ACE to each alert they have been seen in.","title":"observable_mapping"},{"location":"database/ace/#observable_tag_index","text":"Unknown.","title":"observable_tag_index"},{"location":"database/ace/#observable_tag_mapping","text":"Unknown.","title":"observable_tag_mapping"},{"location":"database/ace/#persistence","text":"Contains the persistence data for this cluster .","title":"persistence"},{"location":"database/ace/#remediation","text":"Contains the remediation history for this cluster .","title":"remediation"},{"location":"database/ace/#tags","text":"Contains an entry for each unique tag that ACE has ever used.","title":"tags"},{"location":"database/ace/#tag_mapping","text":"Maps tags used by ACE to each alert they have been used in.","title":"tag_mapping"},{"location":"database/ace/#users","text":"Contains credentials and basic settings for all analysts (users) in ACE.","title":"users"},{"location":"database/ace/#work_distribution","text":"Used by collectors to manage the routing of submissions to engine clusters .","title":"work_distribution"},{"location":"database/ace/#workload","text":"The primary table for managing the workload assignment for the entire cluster .","title":"workload"},{"location":"database/ace_remediation/","text":"Remediation Table \u00b6 The remediation table contains a complete history of all remediation requests whether they succeeded or failed. Each request records who did requested. when it was requested. what type of request it was. what was targetted. the results of the request.","title":"Remediation Table"},{"location":"database/ace_remediation/#remediation-table","text":"The remediation table contains a complete history of all remediation requests whether they succeeded or failed. Each request records who did requested. when it was requested. what type of request it was. what was targetted. the results of the request.","title":"Remediation Table"},{"location":"database/brocess/","text":"brocess Database \u00b6 The brocess database contains a recorded history of activity relevant to analysis including email delivery history. user proxy history.","title":"brocess Database"},{"location":"database/brocess/#brocess-database","text":"The brocess database contains a recorded history of activity relevant to analysis including email delivery history. user proxy history.","title":"brocess Database"},{"location":"database/email_archive/","text":"email-archive Database \u00b6 The email_archive database contains a recorded history of all emails received by either the zeek-based smtp collector or the office365-based email collector .","title":"email-archive Database"},{"location":"database/email_archive/#email-archive-database","text":"The email_archive database contains a recorded history of all emails received by either the zeek-based smtp collector or the office365-based email collector .","title":"email-archive Database"},{"location":"design/","text":"Philosophy \u00b6 The following topics cover some concepts (at a high level) that should be first understood if you're curious about where ACE comes from or the bigger picture of how ACE is meant to be used. I gave a talk on the development of the tool at BSides Cincinnati in 2015 which covers these topics in detail. You can watch his presentation here: Driving Behavior \u00b6 The metric this tool attempts to drive is called time to disposition , or, how long does it take to figure out you're wasting your time? Most people would phrase this as how long does it take to work an alert? but I've taken a different approach to how we look at alerts. Generally, analysts tend to open up whatever alert they're looking at and try to figure out if it's \"bad\" or not. And by \"bad\", they usually mean \"Is this an attack?\", or, \"Is this something I need to worry about?\" Over the years as an analyst, I came to realize that most of the alerts I worked ended up being dispositioned as false positive, with the following definition of what a false postive actually is. False Positive - An alert you're not going to respond to. \u00b6 I think many people define a false positive more in terms of mismatching something, such as a network-based packet alert that pattern matched something, but that pattern matched something it wasn't intended to match. This is obvious. But I would expand that definition even further. For example, suppose you get an alert from your IDS that a remote MySQL exploit was attempted against a server you protect, but that server does not run MySQL. How would you disposition the alert? It was an attack for sure, but it doesn't matter if the target of the attack is not running the software the attack is meant to exploit. It would matter even less if you didn't run MySQL at all. (Note that I would consider the intelligence gained from the attack as another matter entirely and not the focus of this topic.) So when I applied that definition of what a false positive is, I found that almost all of the alerts I worked ended up being false positive. In my presentation at the Cincinnati b-sides, I even stated that 99% of alerts are false positive . For the sake of argument here, we assume this is the case. This changes how alerts are analyzed. If 99% of them are false positive, then what is the chance that what is being analyzed is something to worry about? So rather than answer the question \"Is this bad?\" we answer the question \"Am I wasting my time?\" because 99% of the time that's exactly what is happening! This idea gets some resistance. I've seen many security organizations take the approach of always attempting to minimize the number of false positives their tools are generating, (rightfully) thinking their analysts waste their time working them, and thus the only alerts that should be generated are the true positive alerts that are accurately identifying an attack. I disagree with that and here is why. Algorithms and Metrics \u00b6 An analyst can work a certain number of alerts in a given day. How large that number is depends on a number of factors such as how skilled they are. how good their tools are. how focused they are. how much analysis paralysis has set in (aka alert burn-out.) Say we measure the number of alerts the analysts can work in a given day as measurement W . Now, there are a number of tools deployed that are generating alerts. Say we measure the number of alerts generated in a given day as measurement N . All of the signatures, rules, engines and algorithms they use to generate these alerts gives the network being protected coverage . We define coverage as \"this is all the places and things we're constantly monitoring for evidence of a compromise or attack.\" Say we measure this coverage as measurement C . So now we have tools generating N alerts giving us a coverage of C , with the analysts able to work W of the N alerts. The metric to drive here is C . The higher the value of C , the more difficult it is for an attacker to have a successful attack without being detected. The issue is that an increase in C usually results in an increase in N , but W stays the same. The goal is to always be increasing C while keeping N less than or equal to W . In English that would read as \"always increasing coverage while keeping the number of alerts manageable.\" This is accomplished by a continuous process of hunting, automation and tuning. Hunting is how C increases. You look somewhere you were not looking before. This can be literally anything that makes sense: a new snort rule, a new saved search in splunk, a new script that runs that checks some weird system you have. Anything that would generate new alerts to be analyzed by the analysts. Automation is how the increase to C is actually made permanent. This means running the hunt continuously, forever or until it doesn't make sense any more. For some tools this is natural, for example, a snort rule always runs after it's deployed. But you may need to build something to run that splunk search every so often, or to run that script on a cron job as certain times. Finally, tuning is how to manage the increase in N . This is the action of preventing the tools from generating false positive alerts. This is accomplished in one of two ways: modifying the hunt to avoid the false positives. For example, tweaking the signature to be more specific in the match, or adding additional boolean logic clauses to queries (such as NOT this AND NOT that AND NOT that.) turning off the alert entirely in the case where the hunt is either wrong or not tunable. At this point, we're constantly increasing C by following the process of hunting, automating and tuning, keeping the number of alerts N manageable to a team of analysts that can handle W alerts in a given day. So where does ACE come into play here? It drives the one metric not covered yet: W . ACE increases the number of alerts an analyst can work in a given day. The higher the value of W is, the more aggressive a team can get with C . Teams with a low value of W are easily overwhelmed by very small increases to C . Teams with a high value of W can handle large increases to C . If viewed on a chart over time, the value of N should look more like a sine wave, fluctuating as new hunts are automated and tuning is performed on the old hunts. The value of C should always be rising, even if only gradually. Finally, it's worth noting that in this scenario I'm describing the number of false positive alerts is very close to N , because 99% of all alerts are false positive (assuming our definition.) Thus, the effort to reduce false positive alerts is merely a function of the process, and not a goal in itself.","title":"Philosophy"},{"location":"design/#philosophy","text":"The following topics cover some concepts (at a high level) that should be first understood if you're curious about where ACE comes from or the bigger picture of how ACE is meant to be used. I gave a talk on the development of the tool at BSides Cincinnati in 2015 which covers these topics in detail. You can watch his presentation here:","title":"Philosophy"},{"location":"design/#driving-behavior","text":"The metric this tool attempts to drive is called time to disposition , or, how long does it take to figure out you're wasting your time? Most people would phrase this as how long does it take to work an alert? but I've taken a different approach to how we look at alerts. Generally, analysts tend to open up whatever alert they're looking at and try to figure out if it's \"bad\" or not. And by \"bad\", they usually mean \"Is this an attack?\", or, \"Is this something I need to worry about?\" Over the years as an analyst, I came to realize that most of the alerts I worked ended up being dispositioned as false positive, with the following definition of what a false postive actually is.","title":"Driving Behavior"},{"location":"design/#false-positive-an-alert-youre-not-going-to-respond-to","text":"I think many people define a false positive more in terms of mismatching something, such as a network-based packet alert that pattern matched something, but that pattern matched something it wasn't intended to match. This is obvious. But I would expand that definition even further. For example, suppose you get an alert from your IDS that a remote MySQL exploit was attempted against a server you protect, but that server does not run MySQL. How would you disposition the alert? It was an attack for sure, but it doesn't matter if the target of the attack is not running the software the attack is meant to exploit. It would matter even less if you didn't run MySQL at all. (Note that I would consider the intelligence gained from the attack as another matter entirely and not the focus of this topic.) So when I applied that definition of what a false positive is, I found that almost all of the alerts I worked ended up being false positive. In my presentation at the Cincinnati b-sides, I even stated that 99% of alerts are false positive . For the sake of argument here, we assume this is the case. This changes how alerts are analyzed. If 99% of them are false positive, then what is the chance that what is being analyzed is something to worry about? So rather than answer the question \"Is this bad?\" we answer the question \"Am I wasting my time?\" because 99% of the time that's exactly what is happening! This idea gets some resistance. I've seen many security organizations take the approach of always attempting to minimize the number of false positives their tools are generating, (rightfully) thinking their analysts waste their time working them, and thus the only alerts that should be generated are the true positive alerts that are accurately identifying an attack. I disagree with that and here is why.","title":"False Positive - An alert you're not going to respond to."},{"location":"design/#algorithms-and-metrics","text":"An analyst can work a certain number of alerts in a given day. How large that number is depends on a number of factors such as how skilled they are. how good their tools are. how focused they are. how much analysis paralysis has set in (aka alert burn-out.) Say we measure the number of alerts the analysts can work in a given day as measurement W . Now, there are a number of tools deployed that are generating alerts. Say we measure the number of alerts generated in a given day as measurement N . All of the signatures, rules, engines and algorithms they use to generate these alerts gives the network being protected coverage . We define coverage as \"this is all the places and things we're constantly monitoring for evidence of a compromise or attack.\" Say we measure this coverage as measurement C . So now we have tools generating N alerts giving us a coverage of C , with the analysts able to work W of the N alerts. The metric to drive here is C . The higher the value of C , the more difficult it is for an attacker to have a successful attack without being detected. The issue is that an increase in C usually results in an increase in N , but W stays the same. The goal is to always be increasing C while keeping N less than or equal to W . In English that would read as \"always increasing coverage while keeping the number of alerts manageable.\" This is accomplished by a continuous process of hunting, automation and tuning. Hunting is how C increases. You look somewhere you were not looking before. This can be literally anything that makes sense: a new snort rule, a new saved search in splunk, a new script that runs that checks some weird system you have. Anything that would generate new alerts to be analyzed by the analysts. Automation is how the increase to C is actually made permanent. This means running the hunt continuously, forever or until it doesn't make sense any more. For some tools this is natural, for example, a snort rule always runs after it's deployed. But you may need to build something to run that splunk search every so often, or to run that script on a cron job as certain times. Finally, tuning is how to manage the increase in N . This is the action of preventing the tools from generating false positive alerts. This is accomplished in one of two ways: modifying the hunt to avoid the false positives. For example, tweaking the signature to be more specific in the match, or adding additional boolean logic clauses to queries (such as NOT this AND NOT that AND NOT that.) turning off the alert entirely in the case where the hunt is either wrong or not tunable. At this point, we're constantly increasing C by following the process of hunting, automating and tuning, keeping the number of alerts N manageable to a team of analysts that can handle W alerts in a given day. So where does ACE come into play here? It drives the one metric not covered yet: W . ACE increases the number of alerts an analyst can work in a given day. The higher the value of W is, the more aggressive a team can get with C . Teams with a low value of W are easily overwhelmed by very small increases to C . Teams with a high value of W can handle large increases to C . If viewed on a chart over time, the value of N should look more like a sine wave, fluctuating as new hunts are automated and tuning is performed on the old hunts. The value of C should always be rising, even if only gradually. Finally, it's worth noting that in this scenario I'm describing the number of false positive alerts is very close to N , because 99% of all alerts are false positive (assuming our definition.) Thus, the effort to reduce false positive alerts is merely a function of the process, and not a goal in itself.","title":"Algorithms and Metrics"},{"location":"design/alert_data/","text":"Alert Data Structure \u00b6 Alert data is stored in the storage defined for alerts and is composed of three types of data. The main alert data. Analysis data. File observable data. Alert Data JSON \u00b6 Both Alerts and root analysis objects are stored as JSON formatted files named data.json inside the storage directory of the object. This JSON contains everything associated to the alert or root analysis except for analysis data and file observable data. The JSON contains references to the locations of these other types of data. This is done because analysis data can become very large. This allows ACE to load a root analysis -based object without having to load all the individual analysis JSON data. Analysis Data JSON \u00b6 Analysis data is stored in individual files inside of a hidden .ace subdirectory inside of the storage directory of the root analysis -based object. This analysis data is only loaded when it is requested. File Observable Data \u00b6 File observables represent file data. File data is stored in the storage directory of the root analysis -based object. The exact location of the data is stored as the value of the observable.","title":"Alert Data Structure"},{"location":"design/alert_data/#alert-data-structure","text":"Alert data is stored in the storage defined for alerts and is composed of three types of data. The main alert data. Analysis data. File observable data.","title":"Alert Data Structure"},{"location":"design/alert_data/#alert-data-json","text":"Both Alerts and root analysis objects are stored as JSON formatted files named data.json inside the storage directory of the object. This JSON contains everything associated to the alert or root analysis except for analysis data and file observable data. The JSON contains references to the locations of these other types of data. This is done because analysis data can become very large. This allows ACE to load a root analysis -based object without having to load all the individual analysis JSON data.","title":"Alert Data JSON"},{"location":"design/alert_data/#analysis-data-json","text":"Analysis data is stored in individual files inside of a hidden .ace subdirectory inside of the storage directory of the root analysis -based object. This analysis data is only loaded when it is requested.","title":"Analysis Data JSON"},{"location":"design/alert_data/#file-observable-data","text":"File observables represent file data. File data is stored in the storage directory of the root analysis -based object. The exact location of the data is stored as the value of the observable.","title":"File Observable Data"},{"location":"design/alert_storage/","text":"Alert Storage \u00b6 Alerts are stored in the data directory . The UUID and engine node name are used to create the path to the storage directory. This path is stored along with the alert and recorded in the database as the storage_dir property of the alert. The structure of the directory is as follows and is relative to the data directory . NODE_LOCATION/UUID[:3]/UUID/ NODE_LOCATION is the name of the engine node that hosts the alert. Typically this is the local node name. UUID[:3] is the first three characters of the UUID. UUID is the UUID of the alert. Inside the UUID directory are the data contents of the alert . Alert Storage vs Database Storage \u00b6 Although some alert data is stored in the database , the JSON data is the definitive source of data always.","title":"Alert Storage"},{"location":"design/alert_storage/#alert-storage","text":"Alerts are stored in the data directory . The UUID and engine node name are used to create the path to the storage directory. This path is stored along with the alert and recorded in the database as the storage_dir property of the alert. The structure of the directory is as follows and is relative to the data directory . NODE_LOCATION/UUID[:3]/UUID/ NODE_LOCATION is the name of the engine node that hosts the alert. Typically this is the local node name. UUID[:3] is the first three characters of the UUID. UUID is the UUID of the alert. Inside the UUID directory are the data contents of the alert .","title":"Alert Storage"},{"location":"design/alert_storage/#alert-storage-vs-database-storage","text":"Although some alert data is stored in the database , the JSON data is the definitive source of data always.","title":"Alert Storage vs Database Storage"},{"location":"design/alerts/","text":"Alerts \u00b6 In ACE an alert is a root analysis object that satisfies one of the two following conditions. It was submitted to ACE in correlation analysis mode . It was promoted during analysis when one or more detection points were added. Alerts show up to the analysts in the GUI for review.","title":"Alerts"},{"location":"design/alerts/#alerts","text":"In ACE an alert is a root analysis object that satisfies one of the two following conditions. It was submitted to ACE in correlation analysis mode . It was promoted during analysis when one or more detection points were added. Alerts show up to the analysts in the GUI for review.","title":"Alerts"},{"location":"design/analysis/","text":"Analysis \u00b6 An analysis is the output of the analysis of an observable . It consists of zero or more observables. a free form JSON formatted analysis output. The relationship between analysis and observable is always parent-child. Analysis Details \u00b6 The details of the analysis are stored as a free-form JSON-compatible blob of data. This can be any value. The interpretation of this value is up to the python classes that implement the analysis modules and analysis objects. These details are stored separately from the JSON of the main root analysis object. They are loaded as needed. A brief summary of the details are stored instead. Instances \u00b6 An analysis has an instance that matches the instance of the analysis module that created it. See instanced analysis modules .","title":"Analysis"},{"location":"design/analysis/#analysis","text":"An analysis is the output of the analysis of an observable . It consists of zero or more observables. a free form JSON formatted analysis output. The relationship between analysis and observable is always parent-child.","title":"Analysis"},{"location":"design/analysis/#analysis-details","text":"The details of the analysis are stored as a free-form JSON-compatible blob of data. This can be any value. The interpretation of this value is up to the python classes that implement the analysis modules and analysis objects. These details are stored separately from the JSON of the main root analysis object. They are loaded as needed. A brief summary of the details are stored instead.","title":"Analysis Details"},{"location":"design/analysis/#instances","text":"An analysis has an instance that matches the instance of the analysis module that created it. See instanced analysis modules .","title":"Instances"},{"location":"design/analysis_modes/","text":"Analysis Modes \u00b6 An analysis mode is a property of a root analysis object that determines what analysis modules an engine will run. if the root analysis should automatically become an alert . ACE has predefined analysis modes built in. Additional analysis modes can be added by modifying the configuration settings. Module Groups \u00b6 The engine uses analysis modes to select one or more analysis modules to execute in that mode. These are defined in the configuration file by creating a section with the format [module_group_NAME] where NAME is the name of the analysis mode. Each key in these configuration sections has the format analysis_module_NAME = boolean Where NAME is the name of the analysis module. Note that the value of the key is same as the value of configuration section that defines the module in the configuration. If the key is set to true then the analysis module becomes active for that group. If it is set to false, or not set at all, then that module is not used in that group. correlation mode \u00b6 Any root analysis that has the analysis mode set to correlation automatically becomes an alert . dispositioned mode \u00b6 When an analyst sets the disposition of an alert the analysis mode of the alert gets set to dispositioned and the alert gets re-inserted into the work queue for analysis.","title":"Analysis Modes"},{"location":"design/analysis_modes/#analysis-modes","text":"An analysis mode is a property of a root analysis object that determines what analysis modules an engine will run. if the root analysis should automatically become an alert . ACE has predefined analysis modes built in. Additional analysis modes can be added by modifying the configuration settings.","title":"Analysis Modes"},{"location":"design/analysis_modes/#module-groups","text":"The engine uses analysis modes to select one or more analysis modules to execute in that mode. These are defined in the configuration file by creating a section with the format [module_group_NAME] where NAME is the name of the analysis mode. Each key in these configuration sections has the format analysis_module_NAME = boolean Where NAME is the name of the analysis module. Note that the value of the key is same as the value of configuration section that defines the module in the configuration. If the key is set to true then the analysis module becomes active for that group. If it is set to false, or not set at all, then that module is not used in that group.","title":"Module Groups"},{"location":"design/analysis_modes/#correlation-mode","text":"Any root analysis that has the analysis mode set to correlation automatically becomes an alert .","title":"correlation mode"},{"location":"design/analysis_modes/#dispositioned-mode","text":"When an analyst sets the disposition of an alert the analysis mode of the alert gets set to dispositioned and the alert gets re-inserted into the work queue for analysis.","title":"dispositioned mode"},{"location":"design/analysis_module/","text":"Analysis Module \u00b6 An analysis module is what takes an observable and generates analysis as output. Analysis modules are what make up the core of ACE. They contain the custom analysis logic that does most of the work. The modules that are part of the base installation are documented here . Configuration \u00b6 An analysis module is defined in the configuration settings by sections with the format [analysis_module_NAME] where NAME is a unique name for the module in the configuration. This NAME is often used to reference the analysis module in the available command line tooling . The configuration settings support a number of base options as defined below. [analysis_module_NAME] module = class = instance = enabled = module_groups = ; deprecated exclude_UNIQUE_NAME = expect_UNIQUE_NAME = ; optional parameters priority = threaded = threaded_execution_frequency = The module and class values define the module-class specification which tells ACE what python class to load for the module. The optional instance value defines an instanced analysis module (see below.) The enabled boolean value enables and disables the module. A module that is disabled here is disabled for the entire engine . There are other ways to enable and disable analysis modules. Instanced Analysis Modules \u00b6 An analysis module can be instanced if it defines an instance property in the configuration section. An instance makes an analysis module more unique. This is useful if the analysis module is made up entirely of configuration data (see below.) Analysis that is generated by instanced analysis modules have a matching instance property. Analysis Modules Defined By Configuration \u00b6 You can define new analysis modules entirely by configuration by using instanced analysis modules. While all modules support being instanced, it only makes sense for ones that have a different behavior depending on the configuration. For example, an analysis module that can query a data lake could contain a configuration option that specifies the query to execute and uses variable interpolation to apply parameters to the query. By designing the module this way, new \"analysis modules\" with different queries can be created by simply creating new [analysis_module_] configuration sections and specifying the same module-class specification but a different instance value.","title":"Analysis Module"},{"location":"design/analysis_module/#analysis-module","text":"An analysis module is what takes an observable and generates analysis as output. Analysis modules are what make up the core of ACE. They contain the custom analysis logic that does most of the work. The modules that are part of the base installation are documented here .","title":"Analysis Module"},{"location":"design/analysis_module/#configuration","text":"An analysis module is defined in the configuration settings by sections with the format [analysis_module_NAME] where NAME is a unique name for the module in the configuration. This NAME is often used to reference the analysis module in the available command line tooling . The configuration settings support a number of base options as defined below. [analysis_module_NAME] module = class = instance = enabled = module_groups = ; deprecated exclude_UNIQUE_NAME = expect_UNIQUE_NAME = ; optional parameters priority = threaded = threaded_execution_frequency = The module and class values define the module-class specification which tells ACE what python class to load for the module. The optional instance value defines an instanced analysis module (see below.) The enabled boolean value enables and disables the module. A module that is disabled here is disabled for the entire engine . There are other ways to enable and disable analysis modules.","title":"Configuration"},{"location":"design/analysis_module/#instanced-analysis-modules","text":"An analysis module can be instanced if it defines an instance property in the configuration section. An instance makes an analysis module more unique. This is useful if the analysis module is made up entirely of configuration data (see below.) Analysis that is generated by instanced analysis modules have a matching instance property.","title":"Instanced Analysis Modules"},{"location":"design/analysis_module/#analysis-modules-defined-by-configuration","text":"You can define new analysis modules entirely by configuration by using instanced analysis modules. While all modules support being instanced, it only makes sense for ones that have a different behavior depending on the configuration. For example, an analysis module that can query a data lake could contain a configuration option that specifies the query to execute and uses variable interpolation to apply parameters to the query. By designing the module this way, new \"analysis modules\" with different queries can be created by simply creating new [analysis_module_] configuration sections and specifying the same module-class specification but a different instance value.","title":"Analysis Modules Defined By Configuration"},{"location":"design/analysis_overview/","text":"Analysis Overview \u00b6 ACE analyzes Observables which generates Analysis output which can in turn contain more Observables . Observable \u00b6 An Observable is an observation made during analysis. It has a type. a value. an optional time at which it was observed. Analysis \u00b6 An Analysis is the output of analysis of an observable. It has zero or more observables. free form analysis data. Analysis Module \u00b6 An Analysis Module is what is responsible for taking an observable and generating analysis. Analysis modules are loaded and executed by Engines .","title":"Analysis Overview"},{"location":"design/analysis_overview/#analysis-overview","text":"ACE analyzes Observables which generates Analysis output which can in turn contain more Observables .","title":"Analysis Overview"},{"location":"design/analysis_overview/#observable","text":"An Observable is an observation made during analysis. It has a type. a value. an optional time at which it was observed.","title":"Observable"},{"location":"design/analysis_overview/#analysis","text":"An Analysis is the output of analysis of an observable. It has zero or more observables. free form analysis data.","title":"Analysis"},{"location":"design/analysis_overview/#analysis-module","text":"An Analysis Module is what is responsible for taking an observable and generating analysis. Analysis modules are loaded and executed by Engines .","title":"Analysis Module"},{"location":"design/brocess/","text":"Brocess \u00b6 Brocess keeps track of two things for ACE. Who received what what emails from who? What urls have people visited? Design History \u00b6 Brocess is an incomplete thought ...","title":"Brocess"},{"location":"design/brocess/#brocess","text":"Brocess keeps track of two things for ACE. Who received what what emails from who? What urls have people visited?","title":"Brocess"},{"location":"design/brocess/#design-history","text":"Brocess is an incomplete thought ...","title":"Design History"},{"location":"design/collector/","text":"Collectors \u00b6 A collector is a service that is responsible for collecting things to submit to one or more ACE nodes. Concepts \u00b6 A collector takes something and decides if it should send it to a remote ACE node for analysis. That something can be anything. Some examples include raw emails files, raw log data, binary file submissions, or PCAP data just to name a few. The collector then turns that something into a request that ACE can understand and process. It does so by extracting observations into observables . formatting raw analysis data into JSON. The collector turns that something into a submission which contains all of the data required for analysis. The collector must also decide what analysis mode a submission should be submitted as. The analysis mode determine how ACE treats the submission. Submissions made with the analysis mode set to correlation automatically become alerts, while submissions set to analysis are not automatically alerts but may become so when ACE analyzes the submission. The list of built-in analysis modes can be found here , and administrators are free to create their own. How Collection Works \u00b6 Collection boils down to the following steps. Obtain the thing to collect and analyze. Extract observables and format the analysis data. Store the submission request to persistent storage. Queue the submission request. Retrieve the submission request from the queue. Perform the submission to the various node clusters. Delete the submission data once the submission satisfies all configured settings. Collection Groups \u00b6 Submissions are routed by collectors to remote ACE nodes using configuration settings called collection groups . A configuration section that starts with [collection_group_] is recognized by ACE as the definition of a collection group. Each collection group identifies an ACE cluster to send to. All collection groups that are enabled have submissions routed to them. The configuration settings of each group define how those submissions are handled. [collection_group_local] enabled = yes coverage = 100 full_delivery = yes database = ace company_id = 1 ; deprecated secondary_company_ids = 2,3 ; deprecated target_node_company_id = 1 ; deprecated target_nodes = LOCAL Coverage and Delivery Options \u00b6 The coverage setting controls what percentage of submissions should be sent to this group. The value should be between 0 and 100. Typically this value would be 100. A common use of this is to send some percentage of submissions to a development or QA cluster. The full_delivery setting is a boolean option that controls how ACE treats submission failures. Setting this value to yes will ensure that ACE continues to try to submit when submission fails. Setting the value to no will allow ACE to try to submit it once and only once. Target Cluster Identification \u00b6 The ACE node cluster is identified by providing the name of the database section to the database option. For example: [database_ace] username = ace password = ace ... [collection_group_local] database = ace The value ace is the ace part of [database_ace] and tells ACE to use those database settings to connect to that ACE cluster . Target Nodes \u00b6 By default a collector will submit to whatever node is available in a given cluster. The target_nodes setting is a comma-separated list of engine node names that can narrow down which nodes this collector submits to. The special value of LOCAL represents the local node regardless of the name. You may use this value if you have certain hardware dedicated to analyzing a particular type of submission. For example, if you ran email scanning on one particular system, you might configuration your email collectors to send their submissions only to that particular node. Target Node Names \u00b6 A node is an instance of an ACE engine. Each node can be identified by name which comes from the node configuration setting under the [global] section. Node Translation \u00b6 You can automatically translate one node address into another address. This is useful if the target node is behind a NAT or some type of reverse proxy. The [node_translation] configuration section contains zero or more key, value pairs where each value is formatted as follows. source_host : source_port , target_host , target_port Nodes with a target location of source_host : source_port will be remapped to target_host , target_port when the submission is made. [node_translation] ; here we map anything going to 10.1.1.2 port 443 to go to 192.168.1.2 port 443 example_mapping = 10.1.1.2:443,192.168.1.2:443 Node Submission \u00b6 A collector can submit requests to a local or remote ACE cluster. The ACE api submit call is used if the target node is remote. Local submission routines are used if the target node is local. This is much faster than remote submissions and should be use for high-volume collectors. Persistent Storage \u00b6 Collectors use persistent storage to maintain state. Some examples include what items have already been collected. when the last time a collection request was made. what the unique ID of the last collected item was. Collectors can use any type of persistent storage. ACE has built-in support for using the local file system and the database. Persistent Storage - Files \u00b6 One of the key requirements of collection is that submission data is not lost in the event of system failure. There are two types of submission storage available for collection. Collectors can use local file storage to help keep track of state. The base directory for this storage is defined by the configuration setting persistence_dir . Additional collection data such as external file attachments are stored in the directory defined by the incoming_dir configuration setting. [collection] ; contains various persistant information used by collectors (relative to DATA_DIR) persistence_dir = var/collection/persistence ; some a submission is presented to the saq.collectors.Collector it moves ; all the files attached to the submission into this directory ; (relative to DATA_DIR) incoming_dir = var/collection/incoming Persistent Storage - Database \u00b6 The database can be and is used to store persistance data . The Submission object itself is pickled and stored in the work field of the incoming_workload table. As an alternative to using the local file system to store stateful data for collection, the persistence and persistence_source tables are provided, along with tooling to manage the data .","title":"Collectors"},{"location":"design/collector/#collectors","text":"A collector is a service that is responsible for collecting things to submit to one or more ACE nodes.","title":"Collectors"},{"location":"design/collector/#concepts","text":"A collector takes something and decides if it should send it to a remote ACE node for analysis. That something can be anything. Some examples include raw emails files, raw log data, binary file submissions, or PCAP data just to name a few. The collector then turns that something into a request that ACE can understand and process. It does so by extracting observations into observables . formatting raw analysis data into JSON. The collector turns that something into a submission which contains all of the data required for analysis. The collector must also decide what analysis mode a submission should be submitted as. The analysis mode determine how ACE treats the submission. Submissions made with the analysis mode set to correlation automatically become alerts, while submissions set to analysis are not automatically alerts but may become so when ACE analyzes the submission. The list of built-in analysis modes can be found here , and administrators are free to create their own.","title":"Concepts"},{"location":"design/collector/#how-collection-works","text":"Collection boils down to the following steps. Obtain the thing to collect and analyze. Extract observables and format the analysis data. Store the submission request to persistent storage. Queue the submission request. Retrieve the submission request from the queue. Perform the submission to the various node clusters. Delete the submission data once the submission satisfies all configured settings.","title":"How Collection Works"},{"location":"design/collector/#collection-groups","text":"Submissions are routed by collectors to remote ACE nodes using configuration settings called collection groups . A configuration section that starts with [collection_group_] is recognized by ACE as the definition of a collection group. Each collection group identifies an ACE cluster to send to. All collection groups that are enabled have submissions routed to them. The configuration settings of each group define how those submissions are handled. [collection_group_local] enabled = yes coverage = 100 full_delivery = yes database = ace company_id = 1 ; deprecated secondary_company_ids = 2,3 ; deprecated target_node_company_id = 1 ; deprecated target_nodes = LOCAL","title":"Collection Groups"},{"location":"design/collector/#coverage-and-delivery-options","text":"The coverage setting controls what percentage of submissions should be sent to this group. The value should be between 0 and 100. Typically this value would be 100. A common use of this is to send some percentage of submissions to a development or QA cluster. The full_delivery setting is a boolean option that controls how ACE treats submission failures. Setting this value to yes will ensure that ACE continues to try to submit when submission fails. Setting the value to no will allow ACE to try to submit it once and only once.","title":"Coverage and Delivery Options"},{"location":"design/collector/#target-cluster-identification","text":"The ACE node cluster is identified by providing the name of the database section to the database option. For example: [database_ace] username = ace password = ace ... [collection_group_local] database = ace The value ace is the ace part of [database_ace] and tells ACE to use those database settings to connect to that ACE cluster .","title":"Target Cluster Identification"},{"location":"design/collector/#target-nodes","text":"By default a collector will submit to whatever node is available in a given cluster. The target_nodes setting is a comma-separated list of engine node names that can narrow down which nodes this collector submits to. The special value of LOCAL represents the local node regardless of the name. You may use this value if you have certain hardware dedicated to analyzing a particular type of submission. For example, if you ran email scanning on one particular system, you might configuration your email collectors to send their submissions only to that particular node.","title":"Target Nodes"},{"location":"design/collector/#target-node-names","text":"A node is an instance of an ACE engine. Each node can be identified by name which comes from the node configuration setting under the [global] section.","title":"Target Node Names"},{"location":"design/collector/#node-translation","text":"You can automatically translate one node address into another address. This is useful if the target node is behind a NAT or some type of reverse proxy. The [node_translation] configuration section contains zero or more key, value pairs where each value is formatted as follows. source_host : source_port , target_host , target_port Nodes with a target location of source_host : source_port will be remapped to target_host , target_port when the submission is made. [node_translation] ; here we map anything going to 10.1.1.2 port 443 to go to 192.168.1.2 port 443 example_mapping = 10.1.1.2:443,192.168.1.2:443","title":"Node Translation"},{"location":"design/collector/#node-submission","text":"A collector can submit requests to a local or remote ACE cluster. The ACE api submit call is used if the target node is remote. Local submission routines are used if the target node is local. This is much faster than remote submissions and should be use for high-volume collectors.","title":"Node Submission"},{"location":"design/collector/#persistent-storage","text":"Collectors use persistent storage to maintain state. Some examples include what items have already been collected. when the last time a collection request was made. what the unique ID of the last collected item was. Collectors can use any type of persistent storage. ACE has built-in support for using the local file system and the database.","title":"Persistent Storage"},{"location":"design/collector/#persistent-storage-files","text":"One of the key requirements of collection is that submission data is not lost in the event of system failure. There are two types of submission storage available for collection. Collectors can use local file storage to help keep track of state. The base directory for this storage is defined by the configuration setting persistence_dir . Additional collection data such as external file attachments are stored in the directory defined by the incoming_dir configuration setting. [collection] ; contains various persistant information used by collectors (relative to DATA_DIR) persistence_dir = var/collection/persistence ; some a submission is presented to the saq.collectors.Collector it moves ; all the files attached to the submission into this directory ; (relative to DATA_DIR) incoming_dir = var/collection/incoming","title":"Persistent Storage - Files"},{"location":"design/collector/#persistent-storage-database","text":"The database can be and is used to store persistance data . The Submission object itself is pickled and stored in the work field of the incoming_workload table. As an alternative to using the local file system to store stateful data for collection, the persistence and persistence_source tables are provided, along with tooling to manage the data .","title":"Persistent Storage - Database"},{"location":"design/command_tooling/","text":"ACE Command \u00b6 The ace command sits in the installation directory of ACE and it used to start, stop, configuration and manage the system. The command uses the sub-command pattern. The format of most commands is ace [options] sub-command [options] Every command supports the --help option.","title":"ACE Command"},{"location":"design/command_tooling/#ace-command","text":"The ace command sits in the installation directory of ACE and it used to start, stop, configuration and manage the system. The command uses the sub-command pattern. The format of most commands is ace [options] sub-command [options] Every command supports the --help option.","title":"ACE Command"},{"location":"design/config_dir/","text":"Configuration Directory \u00b6 All ACE configuration settings are stored in the etc directory inside the installation directory.","title":"Configuration Directory"},{"location":"design/config_dir/#configuration-directory","text":"All ACE configuration settings are stored in the etc directory inside the installation directory.","title":"Configuration Directory"},{"location":"design/configuration/","text":"Configuration \u00b6 ACE configuration settings are stored in multiple ini-format files in the SAQ_HOME /etc directory. The files are loaded in a certain order (detailed below). Each time another configuration file is loaded any settings it defines overrides the settings defined in previously loaded files. Load Order \u00b6 ACE loads configuration files in a particular order. There are two different sets of rules. One for normal ACE execution, and one for unit testing. Load Order (Normal) \u00b6 etc/saq.default.ini etc/saq.integrations.default.ini etc/saq.integrations.ini (if it exists) default integration files as configured in etc/saq.integrations.ini Typically these are saq.INTEGRATION_NAME.default.ini where INTEGRATION_NAME is the name of the integration . local integration files as configured in etc/saq.integrations.ini (if they exist) Typically these are saq.INTEGRATION_NAME.ini where INTEGRATION_NAME is the name of the integration . configuration files specified on the command line configuration files specified in the SAQ_CONFIG_PATHS environment variable etc/saq.ini Load Order (Unit Testing) \u00b6 etc/saq.default.ini etc/saq.integrations.default.ini etc/saq.integrations.ini (if it exists) default integration files as configured in etc/saq.integrations.ini local integration files as configured in etc/saq.integrations.ini (if they exist) etc/saq.unittest.default.ini etc/saq.unittest.ini Encrypted Passwords \u00b6 ACE supports encrypting passwords used by the configuration file. These passwords are referenced by using the following format as the value of the configuration option. some_option = encrypted:key The configuration system ACE uses is instrumented to recognize these specially formatted values. If the encryption key has been decrypted then these values are automatically decrypted and made available to ACE. See the admin guide for instructions on how to view and manage encrypted passwords.","title":"Configuration"},{"location":"design/configuration/#configuration","text":"ACE configuration settings are stored in multiple ini-format files in the SAQ_HOME /etc directory. The files are loaded in a certain order (detailed below). Each time another configuration file is loaded any settings it defines overrides the settings defined in previously loaded files.","title":"Configuration"},{"location":"design/configuration/#load-order","text":"ACE loads configuration files in a particular order. There are two different sets of rules. One for normal ACE execution, and one for unit testing.","title":"Load Order"},{"location":"design/configuration/#load-order-normal","text":"etc/saq.default.ini etc/saq.integrations.default.ini etc/saq.integrations.ini (if it exists) default integration files as configured in etc/saq.integrations.ini Typically these are saq.INTEGRATION_NAME.default.ini where INTEGRATION_NAME is the name of the integration . local integration files as configured in etc/saq.integrations.ini (if they exist) Typically these are saq.INTEGRATION_NAME.ini where INTEGRATION_NAME is the name of the integration . configuration files specified on the command line configuration files specified in the SAQ_CONFIG_PATHS environment variable etc/saq.ini","title":"Load Order (Normal)"},{"location":"design/configuration/#load-order-unit-testing","text":"etc/saq.default.ini etc/saq.integrations.default.ini etc/saq.integrations.ini (if it exists) default integration files as configured in etc/saq.integrations.ini local integration files as configured in etc/saq.integrations.ini (if they exist) etc/saq.unittest.default.ini etc/saq.unittest.ini","title":"Load Order (Unit Testing)"},{"location":"design/configuration/#encrypted-passwords","text":"ACE supports encrypting passwords used by the configuration file. These passwords are referenced by using the following format as the value of the configuration option. some_option = encrypted:key The configuration system ACE uses is instrumented to recognize these specially formatted values. If the encryption key has been decrypted then these values are automatically decrypted and made available to ACE. See the admin guide for instructions on how to view and manage encrypted passwords.","title":"Encrypted Passwords"},{"location":"design/data_dir/","text":"Data Directory \u00b6 The ACE data directory is a sub directory contained inside the ACE installation directory that contains all of the persistent data files that ACE uses locally. The location of this directory (relative to the installation directory) is defined in the configuration under the [global] section as data_dir . By default this is set to data . Directory Contents \u00b6 archive error_reports es_logs logs review scan_failures splunk_logs stats var vt_cache This directory also contains alert storage directories . These directories will be named after the name of the node .","title":"Data Directory"},{"location":"design/data_dir/#data-directory","text":"The ACE data directory is a sub directory contained inside the ACE installation directory that contains all of the persistent data files that ACE uses locally. The location of this directory (relative to the installation directory) is defined in the configuration under the [global] section as data_dir . By default this is set to data .","title":"Data Directory"},{"location":"design/data_dir/#directory-contents","text":"archive error_reports es_logs logs review scan_failures splunk_logs stats var vt_cache This directory also contains alert storage directories . These directories will be named after the name of the node .","title":"Directory Contents"},{"location":"design/detection_points/","text":"Detection Points \u00b6 A detection point represents something determined to be suspcious enough to warrent investigation. Only observables and analysis objects can have detection points, and in practice observables are usually the best place to put them. A entire analysis that has one or more detection points is considered by ACE to be an alert and thus has the analysis mode changed to correlation during analysis.","title":"Detection Points"},{"location":"design/detection_points/#detection-points","text":"A detection point represents something determined to be suspcious enough to warrent investigation. Only observables and analysis objects can have detection points, and in practice observables are usually the best place to put them. A entire analysis that has one or more detection points is considered by ACE to be an alert and thus has the analysis mode changed to correlation during analysis.","title":"Detection Points"},{"location":"design/directives/","text":"Directives \u00b6 A directive is an additional analysis instruction given to an observable . Directives are used by analysis modules to control how they are treated by the analysis module. An observable can have zero or more directives. ACE has a number of built-in directives specified in the saq/constants.py file.","title":"Directives"},{"location":"design/directives/#directives","text":"A directive is an additional analysis instruction given to an observable . Directives are used by analysis modules to control how they are treated by the analysis module. An observable can have zero or more directives. ACE has a number of built-in directives specified in the saq/constants.py file.","title":"Directives"},{"location":"design/disposition/","text":"Disposition of Alerts \u00b6 A disposition is the final assessment of an alert by an analyst. The value can be changed multiple times. Each time the disposition value changes the alert is re-analyzed by the engine allowing for further correlation based on the new value. See the user guide for a detailed explanation of the base dispositions.","title":"Disposition of Alerts"},{"location":"design/disposition/#disposition-of-alerts","text":"A disposition is the final assessment of an alert by an analyst. The value can be changed multiple times. Each time the disposition value changes the alert is re-analyzed by the engine allowing for further correlation based on the new value. See the user guide for a detailed explanation of the base dispositions.","title":"Disposition of Alerts"},{"location":"design/ecs/","text":"Encryption Cache Service \u00b6 The ecs service is responsible for caching the encryption password used by ACE for encrypting and decrypting sensitive data. This service makes the encryption password available to other processes by using a local unix socket. ACE automatically uses this service if the encryption is requested and has not been provided in any other way. See the admin guide on services for instructions on how to start and stop services. NOTE You will need to provide the -p option to the ace command when starting the ecs service.","title":"Encryption Cache Service"},{"location":"design/ecs/#encryption-cache-service","text":"The ecs service is responsible for caching the encryption password used by ACE for encrypting and decrypting sensitive data. This service makes the encryption password available to other processes by using a local unix socket. ACE automatically uses this service if the encryption is requested and has not been provided in any other way. See the admin guide on services for instructions on how to start and stop services. NOTE You will need to provide the -p option to the ace command when starting the ecs service.","title":"Encryption Cache Service"},{"location":"design/email_collector/","text":"Email Collector \u00b6 See the design guide for an overview of using ACE to scan emails received from Office365 journaling. Organizations that use Office365 can enable journaling to send a copy of every sent and received email to some external (to Microsoft) email address. This email contains the original email as an attachment. ACE has special support built to handle these types of email. This allows an orgranization to scan their emails using custom rules and logic. Architecture \u00b6","title":"Email Collector"},{"location":"design/email_collector/#email-collector","text":"See the design guide for an overview of using ACE to scan emails received from Office365 journaling. Organizations that use Office365 can enable journaling to send a copy of every sent and received email to some external (to Microsoft) email address. This email contains the original email as an attachment. ACE has special support built to handle these types of email. This allows an orgranization to scan their emails using custom rules and logic.","title":"Email Collector"},{"location":"design/email_collector/#architecture","text":"","title":"Architecture"},{"location":"design/email_logging/","text":"Email Logging \u00b6 ACE can log the meta data of analyzed emails to files that can be consumed by log aggregation tools such as ElasticSearch and Splunk.","title":"Email Logging"},{"location":"design/email_logging/#email-logging","text":"ACE can log the meta data of analyzed emails to files that can be consumed by log aggregation tools such as ElasticSearch and Splunk.","title":"Email Logging"},{"location":"design/email_remediation/","text":"Email Remediation \u00b6 ACE supports the removal of emails from inboxes for the following systems. Office 365 Microsoft Exchange","title":"Email Remediation"},{"location":"design/email_remediation/#email-remediation","text":"ACE supports the removal of emails from inboxes for the following systems. Office 365 Microsoft Exchange","title":"Email Remediation"},{"location":"design/email_scanning/","text":"Email Scanning \u00b6 ACE has support for generating alerts by scanning emails. ACE has three ways of receiving emails to scan. SMTP collection Office365 journaling Exchange or Office365 mailbox extraction Analysis Mode Email \u00b6 The analysis mode email is defined in the configuration settings. This mode has a group of analysis modules assigned to it that are design specifically for email scanning. Email Analyzer Email Conversation Attachment Analyzer Email Conversation Frequency Analyzer Email Conversation Frequency Link Analyzer Email Link Analyzer Encrypted Archive Analyzer analysis_module_mailbox_email_analyzer analysis_module_message_id_analyzer analysis_module_msoffice_encryption_analyzer analysis_module_smtp_stream_analyzer","title":"Email Scanning"},{"location":"design/email_scanning/#email-scanning","text":"ACE has support for generating alerts by scanning emails. ACE has three ways of receiving emails to scan. SMTP collection Office365 journaling Exchange or Office365 mailbox extraction","title":"Email Scanning"},{"location":"design/email_scanning/#analysis-mode-email","text":"The analysis mode email is defined in the configuration settings. This mode has a group of analysis modules assigned to it that are design specifically for email scanning. Email Analyzer Email Conversation Attachment Analyzer Email Conversation Frequency Analyzer Email Conversation Frequency Link Analyzer Email Link Analyzer Encrypted Archive Analyzer analysis_module_mailbox_email_analyzer analysis_module_message_id_analyzer analysis_module_msoffice_encryption_analyzer analysis_module_smtp_stream_analyzer","title":"Analysis Mode Email"},{"location":"design/encryption/","text":"Encryption \u00b6 See the admin guide for how to manage encrypted passwords. ACE uses AES-256 to encrypt the following sensitive information: archived emails passwords to connect to other systems API keys The encryption settings are stored in the config table of the database specified by the encrypted_passwords_db option in the [global] configuration section. Definitions \u00b6 USER PASSWORD : The password supplied by the user. This password is used when starting ACE. USER AES KEY : The 32 byte key used to encrypt and decrypt the Primary AES Key. VERIFICATION KEY : The 32 byte value used to check the validity of the provided password. PRIMARY AES KEY : The 32 byte key to by ACE to encrypt and decrypt data. How ACE Implements Encryption \u00b6 The user supplies the USER PASSWORD. ACE randomly generates the PRIMARY AES KEY. ACE uses the USER PASSWORD as input into the PBKDF2 key derivation function to generate a 64 byte value. The first 32 bytes are the USER AES KEY which is used to encrypt the key generated in step 2. The second 32 bytes is the VERIFICATION KEY which is used to verify the user supplied password. The USER AES KEY is used to encrypt the PRIMARY AES KEY. The results are stored in the config table in the database . Note that each ACE cluster shares a common encryption password (because it shares the database.) Loading and Using the Encryption Password \u00b6 The user provides the USER PASSWORD in one of the following ways. Using the -p option for the main ace command. Using the Encryption Cache Service . Use the SAQ_ENC environment variable. ACE computes the USER AES KEY and VERIFICATION KEY using the supplied password. ACE decrypts the PRIMARY AES KEY and makes it available globally as saq.ENCRYPTION_PASSWORD . NOTES \u00b6 You can set the PRIMARY AES KEY to the sha256 hash of a password by using the -k option of the ace enc set command.","title":"Encryption"},{"location":"design/encryption/#encryption","text":"See the admin guide for how to manage encrypted passwords. ACE uses AES-256 to encrypt the following sensitive information: archived emails passwords to connect to other systems API keys The encryption settings are stored in the config table of the database specified by the encrypted_passwords_db option in the [global] configuration section.","title":"Encryption"},{"location":"design/encryption/#definitions","text":"USER PASSWORD : The password supplied by the user. This password is used when starting ACE. USER AES KEY : The 32 byte key used to encrypt and decrypt the Primary AES Key. VERIFICATION KEY : The 32 byte value used to check the validity of the provided password. PRIMARY AES KEY : The 32 byte key to by ACE to encrypt and decrypt data.","title":"Definitions"},{"location":"design/encryption/#how-ace-implements-encryption","text":"The user supplies the USER PASSWORD. ACE randomly generates the PRIMARY AES KEY. ACE uses the USER PASSWORD as input into the PBKDF2 key derivation function to generate a 64 byte value. The first 32 bytes are the USER AES KEY which is used to encrypt the key generated in step 2. The second 32 bytes is the VERIFICATION KEY which is used to verify the user supplied password. The USER AES KEY is used to encrypt the PRIMARY AES KEY. The results are stored in the config table in the database . Note that each ACE cluster shares a common encryption password (because it shares the database.)","title":"How ACE Implements Encryption"},{"location":"design/encryption/#loading-and-using-the-encryption-password","text":"The user provides the USER PASSWORD in one of the following ways. Using the -p option for the main ace command. Using the Encryption Cache Service . Use the SAQ_ENC environment variable. ACE computes the USER AES KEY and VERIFICATION KEY using the supplied password. ACE decrypts the PRIMARY AES KEY and makes it available globally as saq.ENCRYPTION_PASSWORD .","title":"Loading and Using the Encryption Password"},{"location":"design/encryption/#notes","text":"You can set the PRIMARY AES KEY to the sha256 hash of a password by using the -k option of the ace enc set command.","title":"NOTES"},{"location":"design/engine/","text":"Engine \u00b6 An engine is a service that loads one or more analysis modules and analyzes observables generating analysis . One or more engines can make up a cluster . An engine by itself is referred to as a node . Engines register themselves as nodes in the nodes table of the ace database . Workloads \u00b6 An engine pulls work from the workload . The workload is contained in the workload table in the ace database . Work that is assigned to an engine creates a new entry inside the table. Workload items are then pulled by the workers running in the engine. A single workload is shared across engine nodes running in a cluster . Worker Processes \u00b6 An engine has one or more worker processes. These processes are what does most of the analysis work an engine does. Viewing the process hierarchy of a running engine shows a parent with multiple child copies of itself. Worker processes are members of an analysis pool. Analysis Pools \u00b6 Workers are grouped together into pools. Each pool can optionally be assigned a priority . When the worker pulls the next workload item it first checks for work with an analysis module that matches the priority if one is set. If none are available (or if no priority is set) then it pulls the oldest workload item. The number of analysis pools an engine uses is configurable. If these configuration options are not set (this is the default) then a default pool with a size equal to the number of CPU cores on the current system is created with no priority. To define analysis pools add options under the [service_engine] section with the following format. [service_engine] analysis_pool_size_MODE = int MODE is the priority analysis mode for the pool. The integer value is the size of the pool. See workloads and clustering to understand how work is managed in a clustered environment. Expiring Worker Processes \u00b6 Due to chaotic nature of the analysis work being performed by ACE, at times it becomes useful to release a worker and create a new one to replace it even if nothing seems wrong. This limits the impact of any hidden run-away resource issues caused by using so many different third-party applications, some of which may be in an alpha project status. The auto_refresh_frequency configuration option specifies how many seconds a worker is alive. Targeting Specific Analysis Modes \u00b6 An engine node can be configured to only process work set to specific analysis modes . The local_analysis_modes option is a comma separated list of modes this node will only process. By default this value is empty, which allows the engine to process work in any mode. You can also exclude analysis modes from processing by using the excluded_analysis_modes option. Note that you cannot use both local_analysis_modes and excluded_analysis_modes on the same node. Analysis Failure \u00b6 Analysis always fails at some point. In ACE a failure is represented as an uncaught python exception . ACE logs useful debugging information when this occurs. This saved in the error_reporting directory inside the data directory . The location of the error reporting directory is set by the error_reporting_dir option in the [global] configuration section. An engine can optionally save the current state of the entire analysis as a sub directory inside of the error reporting directory by setting the copy_analysis_on_error boolean option in the [service_engine] configuration section to yes . Work Directory \u00b6 ACE stores root analysis objects inside the work directory . By default ACE uses the data directory for alert storage . You can change the work directory by setting the work_dir configuration value in the [service_engine] section. This directory is relative to the installation directory . You might use this option to mount a faster, smaller disk to use for very high volume analysis work, such as scanning email data. When a root analysis becomes an alert , the data moves from the work directory to the alert storage directory . Alert Disposition Monitoring \u00b6 ACE stops analysis When an analyst dispositions an alert . This is useful when are large number of invalid alerts are submitted to the system by mistake, as it allows the system to recover without having to analyze the invalid submissions. This can also lend itself to analysts thinking something isn't working correctly.","title":"Engine"},{"location":"design/engine/#engine","text":"An engine is a service that loads one or more analysis modules and analyzes observables generating analysis . One or more engines can make up a cluster . An engine by itself is referred to as a node . Engines register themselves as nodes in the nodes table of the ace database .","title":"Engine"},{"location":"design/engine/#workloads","text":"An engine pulls work from the workload . The workload is contained in the workload table in the ace database . Work that is assigned to an engine creates a new entry inside the table. Workload items are then pulled by the workers running in the engine. A single workload is shared across engine nodes running in a cluster .","title":"Workloads"},{"location":"design/engine/#worker-processes","text":"An engine has one or more worker processes. These processes are what does most of the analysis work an engine does. Viewing the process hierarchy of a running engine shows a parent with multiple child copies of itself. Worker processes are members of an analysis pool.","title":"Worker Processes"},{"location":"design/engine/#analysis-pools","text":"Workers are grouped together into pools. Each pool can optionally be assigned a priority . When the worker pulls the next workload item it first checks for work with an analysis module that matches the priority if one is set. If none are available (or if no priority is set) then it pulls the oldest workload item. The number of analysis pools an engine uses is configurable. If these configuration options are not set (this is the default) then a default pool with a size equal to the number of CPU cores on the current system is created with no priority. To define analysis pools add options under the [service_engine] section with the following format. [service_engine] analysis_pool_size_MODE = int MODE is the priority analysis mode for the pool. The integer value is the size of the pool. See workloads and clustering to understand how work is managed in a clustered environment.","title":"Analysis Pools"},{"location":"design/engine/#expiring-worker-processes","text":"Due to chaotic nature of the analysis work being performed by ACE, at times it becomes useful to release a worker and create a new one to replace it even if nothing seems wrong. This limits the impact of any hidden run-away resource issues caused by using so many different third-party applications, some of which may be in an alpha project status. The auto_refresh_frequency configuration option specifies how many seconds a worker is alive.","title":"Expiring Worker Processes"},{"location":"design/engine/#targeting-specific-analysis-modes","text":"An engine node can be configured to only process work set to specific analysis modes . The local_analysis_modes option is a comma separated list of modes this node will only process. By default this value is empty, which allows the engine to process work in any mode. You can also exclude analysis modes from processing by using the excluded_analysis_modes option. Note that you cannot use both local_analysis_modes and excluded_analysis_modes on the same node.","title":"Targeting Specific Analysis Modes"},{"location":"design/engine/#analysis-failure","text":"Analysis always fails at some point. In ACE a failure is represented as an uncaught python exception . ACE logs useful debugging information when this occurs. This saved in the error_reporting directory inside the data directory . The location of the error reporting directory is set by the error_reporting_dir option in the [global] configuration section. An engine can optionally save the current state of the entire analysis as a sub directory inside of the error reporting directory by setting the copy_analysis_on_error boolean option in the [service_engine] configuration section to yes .","title":"Analysis Failure"},{"location":"design/engine/#work-directory","text":"ACE stores root analysis objects inside the work directory . By default ACE uses the data directory for alert storage . You can change the work directory by setting the work_dir configuration value in the [service_engine] section. This directory is relative to the installation directory . You might use this option to mount a faster, smaller disk to use for very high volume analysis work, such as scanning email data. When a root analysis becomes an alert , the data moves from the work directory to the alert storage directory .","title":"Work Directory"},{"location":"design/engine/#alert-disposition-monitoring","text":"ACE stops analysis When an analyst dispositions an alert . This is useful when are large number of invalid alerts are submitted to the system by mistake, as it allows the system to recover without having to analyze the invalid submissions. This can also lend itself to analysts thinking something isn't working correctly.","title":"Alert Disposition Monitoring"},{"location":"design/engine_cluster/","text":"Engine Cluster \u00b6 An engine cluster is one or more engines that share a common workload. Each engine in the cluster is referred to as a node . Each cluster uses a single database . Workloads and Clustering \u00b6 A engine running on one node can optionally pull work from another node that may be too busy to get to it. This is the default behavior. Work is pulled from other nodes using the ACE API. When this occurs the entire root analysis object and all the storage data is moved to the local node. Restricting Workloads To Specific Nodes \u00b6 You can limit what node an engine pulls work from by setting the target_nodes configuration value under the [service_engine] section. This value is a comma separated list of engine node names that this engine should pull work from. A special value of LOCAL is used to target the local system the engine is running on.","title":"Engine Cluster"},{"location":"design/engine_cluster/#engine-cluster","text":"An engine cluster is one or more engines that share a common workload. Each engine in the cluster is referred to as a node . Each cluster uses a single database .","title":"Engine Cluster"},{"location":"design/engine_cluster/#workloads-and-clustering","text":"A engine running on one node can optionally pull work from another node that may be too busy to get to it. This is the default behavior. Work is pulled from other nodes using the ACE API. When this occurs the entire root analysis object and all the storage data is moved to the local node.","title":"Workloads and Clustering"},{"location":"design/engine_cluster/#restricting-workloads-to-specific-nodes","text":"You can limit what node an engine pulls work from by setting the target_nodes configuration value under the [service_engine] section. This value is a comma separated list of engine node names that this engine should pull work from. A special value of LOCAL is used to target the local system the engine is running on.","title":"Restricting Workloads To Specific Nodes"},{"location":"design/file_system_layout/","text":"File System Layout \u00b6 aceapi : api server code ace_client_lib : DEPRECATED app : web application gui archive : ACE archive directory data : ACE data directory data_unittest : ACE unit testing data directory docker : docker -related files documentation : documentation source files etc : primary configuration file directory hunts : primary hunt ini file directory installer : installation scripts and configuration settings saq : ACE source code sql : ACE database schema files and upgrade paths ssl : SSL ca, key and certificate files test_data : various data files used for unit testing tests : pytest unit tests","title":"File System Layout"},{"location":"design/file_system_layout/#file-system-layout","text":"aceapi : api server code ace_client_lib : DEPRECATED app : web application gui archive : ACE archive directory data : ACE data directory data_unittest : ACE unit testing data directory docker : docker -related files documentation : documentation source files etc : primary configuration file directory hunts : primary hunt ini file directory installer : installation scripts and configuration settings saq : ACE source code sql : ACE database schema files and upgrade paths ssl : SSL ca, key and certificate files test_data : various data files used for unit testing tests : pytest unit tests","title":"File System Layout"},{"location":"design/hunting/","text":"Hunting \u00b6 ACE supports hunting through a service called hunter which executes various scripts, queries and API calls at variable frequencies. Hunt can generate work which can be or can become alerts . The hunter is another module-class spec style system that is easily extendible. Hunting Service \u00b6 The entire hunting system runs under the service hunter . See the adminstration guide for how to manage services. Hunting Managers \u00b6 A hunt manager is defined for each type of supported hunt. Managers are responsible for loading, monitoring and executing hunts. Hunts \u00b6 A single hunt is compromised of a configuration and an execution . The configuration defines the various aspects of the hunt such as what it should be identified as, what hunting system it's a part of, when it should run, and how the results should be interpreted. The execution is what the hunts does. For example a query-based hunts would execute a query against some kind of data store. The results of the execution then feed ACE with an analysis submission . The analysis mode of the submissions are configurable. So a hunt can introduce something to ACE which could potentially correlate into an alert . Or it could make the submission as an alert. Supported Hunting Systems \u00b6 The following hunting system are supported by default. Splunk IBM QRadar Configuration and Management \u00b6 See the administration guide for details on how to create, tune and manage hunts.","title":"Hunting"},{"location":"design/hunting/#hunting","text":"ACE supports hunting through a service called hunter which executes various scripts, queries and API calls at variable frequencies. Hunt can generate work which can be or can become alerts . The hunter is another module-class spec style system that is easily extendible.","title":"Hunting"},{"location":"design/hunting/#hunting-service","text":"The entire hunting system runs under the service hunter . See the adminstration guide for how to manage services.","title":"Hunting Service"},{"location":"design/hunting/#hunting-managers","text":"A hunt manager is defined for each type of supported hunt. Managers are responsible for loading, monitoring and executing hunts.","title":"Hunting Managers"},{"location":"design/hunting/#hunts","text":"A single hunt is compromised of a configuration and an execution . The configuration defines the various aspects of the hunt such as what it should be identified as, what hunting system it's a part of, when it should run, and how the results should be interpreted. The execution is what the hunts does. For example a query-based hunts would execute a query against some kind of data store. The results of the execution then feed ACE with an analysis submission . The analysis mode of the submissions are configurable. So a hunt can introduce something to ACE which could potentially correlate into an alert . Or it could make the submission as an alert.","title":"Hunts"},{"location":"design/hunting/#supported-hunting-systems","text":"The following hunting system are supported by default. Splunk IBM QRadar","title":"Supported Hunting Systems"},{"location":"design/hunting/#configuration-and-management","text":"See the administration guide for details on how to create, tune and manage hunts.","title":"Configuration and Management"},{"location":"design/integration/","text":"Integrations \u00b6 An integration is a grouping of configuration data that enables some kind of extra functionality that does not exist in the base system. You can enable and disable an integration which enables and disables anything that is a part of that integration such as analyis modules or collectors . See the admin guide for details on how to manage integrations.","title":"Integrations"},{"location":"design/integration/#integrations","text":"An integration is a grouping of configuration data that enables some kind of extra functionality that does not exist in the base system. You can enable and disable an integration which enables and disables anything that is a part of that integration such as analyis modules or collectors . See the admin guide for details on how to manage integrations.","title":"Integrations"},{"location":"design/logging/","text":"Logging \u00b6 ACE uses the standard logging library found in Python to record logs. ACE uses the following logging levels. DEBUG : very verbose debugging log data useful only for developers. INFO : events that have significance to the normal operation of ACE. WARNING : events that indicate something is wrong, misconfigured or failing. ERROR : used to log error data. CRITICAL : this is used very rarely to indicate some kind of catastrophic error condition that would prevent ACE from running. Logging Configuration \u00b6 ACE uses the python configuration file format for logging configuration. Logging configuration files are stored in the configuration directory . The default logging configuration is etc/console_logging.ini which sends all logging output to the console. A different logging configuration can be specified by using the -L or --logging-config-path command line option . Logging Level \u00b6 By default ACE operates at logging level INFO . This level can be modified using the --log-level command line option . Service Logging \u00b6 Services that execute in background (daemon) mode have special rules for loading logging configuration files. .","title":"Logging"},{"location":"design/logging/#logging","text":"ACE uses the standard logging library found in Python to record logs. ACE uses the following logging levels. DEBUG : very verbose debugging log data useful only for developers. INFO : events that have significance to the normal operation of ACE. WARNING : events that indicate something is wrong, misconfigured or failing. ERROR : used to log error data. CRITICAL : this is used very rarely to indicate some kind of catastrophic error condition that would prevent ACE from running.","title":"Logging"},{"location":"design/logging/#logging-configuration","text":"ACE uses the python configuration file format for logging configuration. Logging configuration files are stored in the configuration directory . The default logging configuration is etc/console_logging.ini which sends all logging output to the console. A different logging configuration can be specified by using the -L or --logging-config-path command line option .","title":"Logging Configuration"},{"location":"design/logging/#logging-level","text":"By default ACE operates at logging level INFO . This level can be modified using the --log-level command line option .","title":"Logging Level"},{"location":"design/logging/#service-logging","text":"Services that execute in background (daemon) mode have special rules for loading logging configuration files. .","title":"Service Logging"},{"location":"design/module_class_spec/","text":"Module-Class Specification \u00b6 ACE uses a design pattern called module-class specification in the configuration settings that allows you to specify a python class to load. Typically there is a module and a class configuration option. The module option refers to the python module that contains the class to load. How this is actually used depends on what is implementing it (such as how additional configuration options are passed to the class.) However, this pattern occurs enough to warrant documentation.","title":"Module-Class Specification"},{"location":"design/module_class_spec/#module-class-specification","text":"ACE uses a design pattern called module-class specification in the configuration settings that allows you to specify a python class to load. Typically there is a module and a class configuration option. The module option refers to the python module that contains the class to load. How this is actually used depends on what is implementing it (such as how additional configuration options are passed to the class.) However, this pattern occurs enough to warrant documentation.","title":"Module-Class Specification"},{"location":"design/network_semaphore/","text":"Network Semaphore \u00b6 The network semaphore service provides shared semaphore objects referenced by name to any ACE node over TCP/IP sockets. These semaphores are used to limit the usage of shared resources. For example an installation that queries some shared system can limit the total number of concurrent queries by defining a network semaphore for the resource and then enforcing the use of the semaphore when the resource is required. Semaphores are created with a limit set. When a client makes a request for a semaphore it attempts to increase the usage counter for that semaphore. If the usage counter would exceed the limit, the request is blocked until an existing request is completed. Requests are resolved in FIFO order. Clients can optionally request a timeout when attempting to access a shared resource. This system uses a custom protocol over TCP/IP. Fallback Semaphores \u00b6 When a request is made to lock a network semaphore and the service is unavailable, then a local version of the semaphore is used. This provides default behavior for single-node installations of ACE, and also provides a backup capability in the event of system or network failure. Configuration and management is documented in the administration guide . Defined and Undefined Semaphores \u00b6 A defined semaphore is one that is defined by the configuration settings. Defined semaphores are assigned a limit value which represents how many concurrent access connections can be active at a time. An undefined semaphore is one that is defined on-the-fly by requesting a semaphore that does not already exist. These types of semaphores are assigned limit value of 1. Undefined semaphores can be used to coordinate access to resources which are numerous and have unknown quantities. For example, you could use this to limit access to endpoints by using the name or ip address of the endpoint as an undefined semaphore name.","title":"Network Semaphore"},{"location":"design/network_semaphore/#network-semaphore","text":"The network semaphore service provides shared semaphore objects referenced by name to any ACE node over TCP/IP sockets. These semaphores are used to limit the usage of shared resources. For example an installation that queries some shared system can limit the total number of concurrent queries by defining a network semaphore for the resource and then enforcing the use of the semaphore when the resource is required. Semaphores are created with a limit set. When a client makes a request for a semaphore it attempts to increase the usage counter for that semaphore. If the usage counter would exceed the limit, the request is blocked until an existing request is completed. Requests are resolved in FIFO order. Clients can optionally request a timeout when attempting to access a shared resource. This system uses a custom protocol over TCP/IP.","title":"Network Semaphore"},{"location":"design/network_semaphore/#fallback-semaphores","text":"When a request is made to lock a network semaphore and the service is unavailable, then a local version of the semaphore is used. This provides default behavior for single-node installations of ACE, and also provides a backup capability in the event of system or network failure. Configuration and management is documented in the administration guide .","title":"Fallback Semaphores"},{"location":"design/network_semaphore/#defined-and-undefined-semaphores","text":"A defined semaphore is one that is defined by the configuration settings. Defined semaphores are assigned a limit value which represents how many concurrent access connections can be active at a time. An undefined semaphore is one that is defined on-the-fly by requesting a semaphore that does not already exist. These types of semaphores are assigned limit value of 1. Undefined semaphores can be used to coordinate access to resources which are numerous and have unknown quantities. For example, you could use this to limit access to endpoints by using the name or ip address of the endpoint as an undefined semaphore name.","title":"Defined and Undefined Semaphores"},{"location":"design/observable/","text":"Observables \u00b6 An observable represents an observation made during the course of analysis. It always has a type and a value , and optionally has a time at which the observation was made. If the observable is missing the time then the time is assumed to be the time of the entire event that is being analyzed. Observables are always children of analysis based objects. Observables are analyzed by analysis modules which generate analysis as output. The newly created analysis can also contain more observables. Observables are unique according to their type, value and time. If an observable with the same type, value and time as another existing observable is added, it references the existing observable instead of creating a new one. Note that observables are unique by time. You can optionally group them together by time if you need to. Tagging \u00b6 An observable can have zero or more tags . Tagging is used to tie some concept, idea or grouping property to the observables. Tagging can also be used to automatically add detection points . Directives \u00b6 An observable can have zero or more directives . Directives are used to give analysis modules some additional instructions on how to handle the observable. Redirection \u00b6 An observable can include a redirection which points to another observable. Redirections are often used when extracting artifacts from files. They give ACE the ability to say \"This file actually came from this other file.\" A common example usage of this feature is determining which file to send to a sandboxing system. If a file was generated as part of an analysis, redirection can be used to point to the original file, giving the sandbox analysis module the correct file to analyze. By default redirection points to itself. Linking \u00b6 An observable can be linked to another observable. Any tags applied to the original observable are also applied to the linked observable. Limited Analysis \u00b6 An observable can limit what analysis modules are executed against it by specifying one or more analysis modules as the limited analysis for the observable. Only modules in the list will be executed against it, and only if the module accepts it. Excluded Analysis \u00b6 An observable can restrict what analysis modules are executed against it by specifying one or more analysis modules that will be excluded from analysis. These modules will not execute against the observable regardless of any other condition. Relationships \u00b6 An observable can have a relationship to another observable. This has meaning only to analysis modules that work with relationships. A current list of valid relationships can be found in saq/constants.py in the # relationships section. Grouping by Time \u00b6 Observables can be grouped together for analysis purposes by time. This allows multiple observations over some time period to be treated as a single observation. For example, if the same IP address was observed 50 times over 5 seconds, they can be grouped into a single observation over that 5 second time period. This works in conjunction with the observation_grouping_time_range configuration option of analysis modules . Analysis \u00b6 An observable can have zero or more analysis objects attached to it. These represent the analysis performed by analysis modules .","title":"Observables"},{"location":"design/observable/#observables","text":"An observable represents an observation made during the course of analysis. It always has a type and a value , and optionally has a time at which the observation was made. If the observable is missing the time then the time is assumed to be the time of the entire event that is being analyzed. Observables are always children of analysis based objects. Observables are analyzed by analysis modules which generate analysis as output. The newly created analysis can also contain more observables. Observables are unique according to their type, value and time. If an observable with the same type, value and time as another existing observable is added, it references the existing observable instead of creating a new one. Note that observables are unique by time. You can optionally group them together by time if you need to.","title":"Observables"},{"location":"design/observable/#tagging","text":"An observable can have zero or more tags . Tagging is used to tie some concept, idea or grouping property to the observables. Tagging can also be used to automatically add detection points .","title":"Tagging"},{"location":"design/observable/#directives","text":"An observable can have zero or more directives . Directives are used to give analysis modules some additional instructions on how to handle the observable.","title":"Directives"},{"location":"design/observable/#redirection","text":"An observable can include a redirection which points to another observable. Redirections are often used when extracting artifacts from files. They give ACE the ability to say \"This file actually came from this other file.\" A common example usage of this feature is determining which file to send to a sandboxing system. If a file was generated as part of an analysis, redirection can be used to point to the original file, giving the sandbox analysis module the correct file to analyze. By default redirection points to itself.","title":"Redirection"},{"location":"design/observable/#linking","text":"An observable can be linked to another observable. Any tags applied to the original observable are also applied to the linked observable.","title":"Linking"},{"location":"design/observable/#limited-analysis","text":"An observable can limit what analysis modules are executed against it by specifying one or more analysis modules as the limited analysis for the observable. Only modules in the list will be executed against it, and only if the module accepts it.","title":"Limited Analysis"},{"location":"design/observable/#excluded-analysis","text":"An observable can restrict what analysis modules are executed against it by specifying one or more analysis modules that will be excluded from analysis. These modules will not execute against the observable regardless of any other condition.","title":"Excluded Analysis"},{"location":"design/observable/#relationships","text":"An observable can have a relationship to another observable. This has meaning only to analysis modules that work with relationships. A current list of valid relationships can be found in saq/constants.py in the # relationships section.","title":"Relationships"},{"location":"design/observable/#grouping-by-time","text":"Observables can be grouped together for analysis purposes by time. This allows multiple observations over some time period to be treated as a single observation. For example, if the same IP address was observed 50 times over 5 seconds, they can be grouped into a single observation over that 5 second time period. This works in conjunction with the observation_grouping_time_range configuration option of analysis modules .","title":"Grouping by Time"},{"location":"design/observable/#analysis","text":"An observable can have zero or more analysis objects attached to it. These represent the analysis performed by analysis modules .","title":"Analysis"},{"location":"design/persistence/","text":"Persistence Data \u00b6 ACE has built-in support for storing persistance data in the database table persistence of the ace database . This is used to maintain state over time and across reboots of the system. See the admin guide for how to view and manage persistence data. Persistence Sources \u00b6 A persistence source is a logical grouping of persistence data. A collector may define a persistence source for itself so that it can easily keep track of and manage the persistence data it generates. All persistence data is tied to a source. Permanent vs Volatile Persistence \u00b6 Persistence data can be permanent or volatile . Permanent persistence is (usually) not deleted over time. Volatile data is. Permanent data is typically state tracking data of particular things or properties of things, such as recording the last time a query was made. Volatile data is usually tracking multiple things there the number of thing is large and grows. For example a collector might record the unique ID of collected data so that the same thing is not collected twice. Over time these records will become stale and useless, at which point they can be expired from the database. There is tooling available to manage volatile data. Volatile Data Timestamps \u00b6 When volatile data is added a timestamp is recorded. Every time the volatile data is read it also updates the timestamp . This allows the system to expire volatile data that is no longer being referenced.","title":"Persistence Data"},{"location":"design/persistence/#persistence-data","text":"ACE has built-in support for storing persistance data in the database table persistence of the ace database . This is used to maintain state over time and across reboots of the system. See the admin guide for how to view and manage persistence data.","title":"Persistence Data"},{"location":"design/persistence/#persistence-sources","text":"A persistence source is a logical grouping of persistence data. A collector may define a persistence source for itself so that it can easily keep track of and manage the persistence data it generates. All persistence data is tied to a source.","title":"Persistence Sources"},{"location":"design/persistence/#permanent-vs-volatile-persistence","text":"Persistence data can be permanent or volatile . Permanent persistence is (usually) not deleted over time. Volatile data is. Permanent data is typically state tracking data of particular things or properties of things, such as recording the last time a query was made. Volatile data is usually tracking multiple things there the number of thing is large and grows. For example a collector might record the unique ID of collected data so that the same thing is not collected twice. Over time these records will become stale and useless, at which point they can be expired from the database. There is tooling available to manage volatile data.","title":"Permanent vs Volatile Persistence"},{"location":"design/persistence/#volatile-data-timestamps","text":"When volatile data is added a timestamp is recorded. Every time the volatile data is read it also updates the timestamp . This allows the system to expire volatile data that is no longer being referenced.","title":"Volatile Data Timestamps"},{"location":"design/recursive_analysis/","text":"Recursive Analysis \u00b6 Analysis is performed on observables by analysis modules generating an output called analysis . The analysis output can contain zero or more observables . These new observables are then analyzed in the same way. This type of data is best represented as a tree showing the parent-child relationships between the data elements. The tree is self-referencing . If an observable is added that already exists somewhere in the existing analysis tree, then the existing observable is used instead of adding a new one.","title":"Recursive Analysis"},{"location":"design/recursive_analysis/#recursive-analysis","text":"Analysis is performed on observables by analysis modules generating an output called analysis . The analysis output can contain zero or more observables . These new observables are then analyzed in the same way. This type of data is best represented as a tree showing the parent-child relationships between the data elements. The tree is self-referencing . If an observable is added that already exists somewhere in the existing analysis tree, then the existing observable is used instead of adding a new one.","title":"Recursive Analysis"},{"location":"design/remediation/","text":"Remediation \u00b6 ACE has a system defined for remediation purposes. Remediation requests are made to the remediation system , which may either immediately execute the request, or queue it for later execution. It also supports the unavailability of the resource targeted for remediation. Remediation Systems \u00b6 Remediation requests are assigned a type value which defines what kind of remediation request it is. Each type of supported remediation is handled by a remediation system which is defined in the configuration settings. The following types are currently supported. email remediation Remediation Request Types \u00b6 All remediation systems support two requests: remove and restore . A remove requests removes the object from the target, while a restore request restores a previously removed object back to the target. It is important that both types of requests are supported and function correctly. Remediation History \u00b6 A history of all remediation requests taken, regardless of success, are recorded in the [remediation] table in the ace database .","title":"Remediation"},{"location":"design/remediation/#remediation","text":"ACE has a system defined for remediation purposes. Remediation requests are made to the remediation system , which may either immediately execute the request, or queue it for later execution. It also supports the unavailability of the resource targeted for remediation.","title":"Remediation"},{"location":"design/remediation/#remediation-systems","text":"Remediation requests are assigned a type value which defines what kind of remediation request it is. Each type of supported remediation is handled by a remediation system which is defined in the configuration settings. The following types are currently supported. email remediation","title":"Remediation Systems"},{"location":"design/remediation/#remediation-request-types","text":"All remediation systems support two requests: remove and restore . A remove requests removes the object from the target, while a restore request restores a previously removed object back to the target. It is important that both types of requests are supported and function correctly.","title":"Remediation Request Types"},{"location":"design/remediation/#remediation-history","text":"A history of all remediation requests taken, regardless of success, are recorded in the [remediation] table in the ace database .","title":"Remediation History"},{"location":"design/root_analysis/","text":"Root Analysis \u00b6 The root analysis object is the root of all analysis and thus the root of all parent-child relationships. A root analysis is an analysis object. Thus is can contain zero or more observables and zero or more tags . During analysis by an engine the root analysis is always available by reference to the analysis modules . A root analysis can automically become an alert if one or more detection points are added during the course of analysis. An alert is also considered a root analysis object.","title":"Root Analysis"},{"location":"design/root_analysis/#root-analysis","text":"The root analysis object is the root of all analysis and thus the root of all parent-child relationships. A root analysis is an analysis object. Thus is can contain zero or more observables and zero or more tags . During analysis by an engine the root analysis is always available by reference to the analysis modules . A root analysis can automically become an alert if one or more detection points are added during the course of analysis. An alert is also considered a root analysis object.","title":"Root Analysis"},{"location":"design/saq_home/","text":"SAQ_HOME \u00b6 SAQ_HOME refers to the installation directory of ACE. Typically this is /opt/ace but any directory can be used. ACE determines SAQ_HOME by loading it from the environment variable SAQ_HOME. obtaining it from a command line option. defaulting directory that contains the ace command. This value must be an absolute path. Most functionality makes relative directory paths absolute by prefixing them with SAQ_HOME.","title":"SAQ_HOME"},{"location":"design/saq_home/#saq_home","text":"SAQ_HOME refers to the installation directory of ACE. Typically this is /opt/ace but any directory can be used. ACE determines SAQ_HOME by loading it from the environment variable SAQ_HOME. obtaining it from a command line option. defaulting directory that contains the ace command. This value must be an absolute path. Most functionality makes relative directory paths absolute by prefixing them with SAQ_HOME.","title":"SAQ_HOME"},{"location":"design/service/","text":"Service \u00b6 A service is a process that typically runs in the background as part of the ACE ecosystem. Special command line tooling exists to manage the starting, stopping and getting the status of services. Execution Modes \u00b6 A service can be ran in one of three execution modes. debug mode foreground mode background (daemon) mode debug mode \u00b6 A service that runs in debug mode typically runs as a single thread on a single process. This makes it easier for the developer to drop into debugging shells. To start a service in debug mode execute ace service start --debug Use Control-C to stop a service running in debug mode. foregorund mode \u00b6 A service that runs in foreground mode runs normally but waits for the service to exit before returning control to the shell. To start a service in foreground mode execute ace service start --foreground . Use Control-C to stop a service running in the foreground. background (daemon) mode \u00b6 A service that runs in background (daemon) mode detaches itself from the foreground and executes as a forked process in the background. This is the default mode if no other mode is specified. Management and Configuration \u00b6 Service management and configuration is covered in the administration guide.","title":"Service"},{"location":"design/service/#service","text":"A service is a process that typically runs in the background as part of the ACE ecosystem. Special command line tooling exists to manage the starting, stopping and getting the status of services.","title":"Service"},{"location":"design/service/#execution-modes","text":"A service can be ran in one of three execution modes. debug mode foreground mode background (daemon) mode","title":"Execution Modes"},{"location":"design/service/#debug-mode","text":"A service that runs in debug mode typically runs as a single thread on a single process. This makes it easier for the developer to drop into debugging shells. To start a service in debug mode execute ace service start --debug Use Control-C to stop a service running in debug mode.","title":"debug mode"},{"location":"design/service/#foregorund-mode","text":"A service that runs in foreground mode runs normally but waits for the service to exit before returning control to the shell. To start a service in foreground mode execute ace service start --foreground . Use Control-C to stop a service running in the foreground.","title":"foregorund mode"},{"location":"design/service/#background-daemon-mode","text":"A service that runs in background (daemon) mode detaches itself from the foreground and executes as a forked process in the background. This is the default mode if no other mode is specified.","title":"background (daemon) mode"},{"location":"design/service/#management-and-configuration","text":"Service management and configuration is covered in the administration guide.","title":"Management and Configuration"},{"location":"design/smtp_collector/","text":"SMTP Collector \u00b6 See the admin guide for installation and configuration instructions. The SMTP collector uses zeek to extract raw SMTP session data and parses it for emails. These emails are submitted to ACE for analysis. Note that this was written when zeek was called bro and still has references to the name bro in places. This SMTP collector is part of the bro integration . Architecture \u00b6 Zeek scripts stored in the bro directory are used to extract SMTP session data into files. Each file is named after the zeek connection ID . Most (not all) of the SMTP session data is recorded to the file. The SMTP session data file has the following format. originating IP address originating port epoch network time data Each SMTP transaction request is formatted as follows. > command argument Each SMTP transaction response is formatted as follows. < command code message SMTP session DATA is written to the file with a single line-feed appended. Once the session has completed, terminated or errorred out another file with file name of NAME.ready is created. NAME is the name of the SMTP session data file. The SMTP collection sees the .ready file as the signal to process the SMTP session data file. This file is parsed for RFC 822 formatted email data. For each extracted RFC 822 formatted email file the SMTP session data file, email file, and session meta data such as MAIL FROM and RCPT TO values are submitted to ACE engine clusters for analysis. Thus if a single SMTP session contains multiple emails then multiple analysis requests will be issued. If a single SMTP session contains a single email that contains multiple RCPT TO (recipients), only a single analysis request is issued for the email that was delivered to multiple addresses.","title":"SMTP Collector"},{"location":"design/smtp_collector/#smtp-collector","text":"See the admin guide for installation and configuration instructions. The SMTP collector uses zeek to extract raw SMTP session data and parses it for emails. These emails are submitted to ACE for analysis. Note that this was written when zeek was called bro and still has references to the name bro in places. This SMTP collector is part of the bro integration .","title":"SMTP Collector"},{"location":"design/smtp_collector/#architecture","text":"Zeek scripts stored in the bro directory are used to extract SMTP session data into files. Each file is named after the zeek connection ID . Most (not all) of the SMTP session data is recorded to the file. The SMTP session data file has the following format. originating IP address originating port epoch network time data Each SMTP transaction request is formatted as follows. > command argument Each SMTP transaction response is formatted as follows. < command code message SMTP session DATA is written to the file with a single line-feed appended. Once the session has completed, terminated or errorred out another file with file name of NAME.ready is created. NAME is the name of the SMTP session data file. The SMTP collection sees the .ready file as the signal to process the SMTP session data file. This file is parsed for RFC 822 formatted email data. For each extracted RFC 822 formatted email file the SMTP session data file, email file, and session meta data such as MAIL FROM and RCPT TO values are submitted to ACE engine clusters for analysis. Thus if a single SMTP session contains multiple emails then multiple analysis requests will be issued. If a single SMTP session contains a single email that contains multiple RCPT TO (recipients), only a single analysis request is issued for the email that was delivered to multiple addresses.","title":"Architecture"},{"location":"design/ssl/","text":"SSL \u00b6 ACE has a number of uses for SSL. GUI encryption API encryption database encryption The [SSL] configuration section defines a ca_chain_path setting that points to a file relative to SAQ_HOME that contains the chain of certificate authorities used to sign the certificates used by ACE.","title":"SSL"},{"location":"design/ssl/#ssl","text":"ACE has a number of uses for SSL. GUI encryption API encryption database encryption The [SSL] configuration section defines a ca_chain_path setting that points to a file relative to SAQ_HOME that contains the chain of certificate authorities used to sign the certificates used by ACE.","title":"SSL"},{"location":"design/submissions/","text":"Submissions \u00b6 ACE receives analysis work via the api in the format of a submission. This object is a simple subset of an entire root analysis object. JSON Schema \u00b6 The submission is a JSON object with the following schema. { \"analysis\": { \"analysis_mode\": analysis_mode, \"tool\": tool, \"tool_instance\": tool_instance, \"type\": type, \"company_id\": company_id, \"description\": description, \"event_time\": formatted_event_time, \"details\": details, \"observables\": observables, \"tags\": tags, \"queue\": queue, \"instructions\": instructions } } Submission Filtering \u00b6 Yara rules are used to filter out matching submissions. See here for details of how this works and how to manage these yara rules.","title":"Submissions"},{"location":"design/submissions/#submissions","text":"ACE receives analysis work via the api in the format of a submission. This object is a simple subset of an entire root analysis object.","title":"Submissions"},{"location":"design/submissions/#json-schema","text":"The submission is a JSON object with the following schema. { \"analysis\": { \"analysis_mode\": analysis_mode, \"tool\": tool, \"tool_instance\": tool_instance, \"type\": type, \"company_id\": company_id, \"description\": description, \"event_time\": formatted_event_time, \"details\": details, \"observables\": observables, \"tags\": tags, \"queue\": queue, \"instructions\": instructions } }","title":"JSON Schema"},{"location":"design/submissions/#submission-filtering","text":"Yara rules are used to filter out matching submissions. See here for details of how this works and how to manage these yara rules.","title":"Submission Filtering"},{"location":"design/tags/","text":"Tags \u00b6 Tagging is a way to add additional information or context to analysis data. Only observables and analysis can be tagged, and in practice observables are usually what get tagged. Tagging shows up in the GUI as labels of varying colors. The value of a tag is any UTF8 encoded string. Relationships \u00b6 A tag adds a relationships between alerts. ACE keeps track of what tags alerts contain. So quick correlational queries can be performed from the database. Tag Severity Levels \u00b6 Tags can be assigned severity levels in the configuration settings under the [tags] section. By assigning tags severity levels you can control the color of the tag in the GUI. adding detection points to what got tagged. The format of the keys in the [tags] are as follows. [tags] tag_name = value tag_name is the value of the tag to assign the severity to. value is one of the following values. hidden - hides the tags in the GUI . special - displayed as a black-on-white tag. fp - green info - gray warning - yellow and adds a detection point alert - red and adds a detection point critical - dark red and adds a detection point The visual display (color) of the tags is controlled by [tag_css_class] section which associates a tag severity level to a CSS class to use to display the tag.","title":"Tags"},{"location":"design/tags/#tags","text":"Tagging is a way to add additional information or context to analysis data. Only observables and analysis can be tagged, and in practice observables are usually what get tagged. Tagging shows up in the GUI as labels of varying colors. The value of a tag is any UTF8 encoded string.","title":"Tags"},{"location":"design/tags/#relationships","text":"A tag adds a relationships between alerts. ACE keeps track of what tags alerts contain. So quick correlational queries can be performed from the database.","title":"Relationships"},{"location":"design/tags/#tag-severity-levels","text":"Tags can be assigned severity levels in the configuration settings under the [tags] section. By assigning tags severity levels you can control the color of the tag in the GUI. adding detection points to what got tagged. The format of the keys in the [tags] are as follows. [tags] tag_name = value tag_name is the value of the tag to assign the severity to. value is one of the following values. hidden - hides the tags in the GUI . special - displayed as a black-on-white tag. fp - green info - gray warning - yellow and adds a detection point alert - red and adds a detection point critical - dark red and adds a detection point The visual display (color) of the tags is controlled by [tag_css_class] section which associates a tag severity level to a CSS class to use to display the tag.","title":"Tag Severity Levels"},{"location":"design/whitelisting/","text":"Whitelisting \u00b6 ACE has a number of different ways that various things can be whitelisted. Analyst (User) Whitelisting \u00b6 Analysts are able to whitelist observables from the analysis tree in the GUI . Observables which are whitelisted in this way are not analyzed by the engine . They are still displayed in the analysis tree with a whitelisted tag. An observable can be un-whitelisted from the same menu item. The current set of whitelisted observables cannot be viewed or managed. Limiting Analyst (User) Whitelisting \u00b6 Certain observable types should not be whitelisted. The configuration setting whitelist_excluded_observable_types in the [gui] configuration settings contains a comma separated list of observable types that cannot be whitelisted in this way. [gui] whitelist_excluded_observable_types = ipv4_full_conversation,yara_rule,indicator,snort_sig Note that these can still be whitelisted by analysis. Analysis and Observable Whitelisting \u00b6 Both anaylsis and observable objects can be whitelisted by one of three ways. Adding a tag with a value of whitelisted . Adding the directive DIRECTIVE_WHITELISTED . Calling mark_as_whitelisted() . Note that calling mark_as_whitelisted() also adds the whitelisted tag . adds the DIRECTIVE_WHITELISTED directive . sets the root analysis whitelisted boolean property to true . Just adding the observable or directive does not whitelist the root analysis . Root Analysis Whitelisting \u00b6 Root analysis can be marked as whitelisted by setting the boolean whitelisted property to true. This prevents a root analysis from becoming an alert . Email Whitelisting \u00b6 There is special support for whitelisting emails based on sender, recipient and subject. The EmailAnalyzer module uses this support to apply whitelisting to the current root analysis object during analysis. This method is refered to as \"Brotex Whitelisting\" because it came from another project called Brotex. See here for details about this whitelisting method. Crawlphish and Cloudphish Whitelisting and Blacklisting \u00b6 The crawlphish and cloudphish analysis modules support another filtering scheme designed for URLs. See here for details about this whitelisting method. Submission Filtering \u00b6 ACE receives work in the form of submissions . A filtering system is available that is specific to the format of that data.","title":"Whitelisting"},{"location":"design/whitelisting/#whitelisting","text":"ACE has a number of different ways that various things can be whitelisted.","title":"Whitelisting"},{"location":"design/whitelisting/#analyst-user-whitelisting","text":"Analysts are able to whitelist observables from the analysis tree in the GUI . Observables which are whitelisted in this way are not analyzed by the engine . They are still displayed in the analysis tree with a whitelisted tag. An observable can be un-whitelisted from the same menu item. The current set of whitelisted observables cannot be viewed or managed.","title":"Analyst (User) Whitelisting"},{"location":"design/whitelisting/#limiting-analyst-user-whitelisting","text":"Certain observable types should not be whitelisted. The configuration setting whitelist_excluded_observable_types in the [gui] configuration settings contains a comma separated list of observable types that cannot be whitelisted in this way. [gui] whitelist_excluded_observable_types = ipv4_full_conversation,yara_rule,indicator,snort_sig Note that these can still be whitelisted by analysis.","title":"Limiting Analyst (User) Whitelisting"},{"location":"design/whitelisting/#analysis-and-observable-whitelisting","text":"Both anaylsis and observable objects can be whitelisted by one of three ways. Adding a tag with a value of whitelisted . Adding the directive DIRECTIVE_WHITELISTED . Calling mark_as_whitelisted() . Note that calling mark_as_whitelisted() also adds the whitelisted tag . adds the DIRECTIVE_WHITELISTED directive . sets the root analysis whitelisted boolean property to true . Just adding the observable or directive does not whitelist the root analysis .","title":"Analysis and Observable Whitelisting"},{"location":"design/whitelisting/#root-analysis-whitelisting","text":"Root analysis can be marked as whitelisted by setting the boolean whitelisted property to true. This prevents a root analysis from becoming an alert .","title":"Root Analysis Whitelisting"},{"location":"design/whitelisting/#email-whitelisting","text":"There is special support for whitelisting emails based on sender, recipient and subject. The EmailAnalyzer module uses this support to apply whitelisting to the current root analysis object during analysis. This method is refered to as \"Brotex Whitelisting\" because it came from another project called Brotex. See here for details about this whitelisting method.","title":"Email Whitelisting"},{"location":"design/whitelisting/#crawlphish-and-cloudphish-whitelisting-and-blacklisting","text":"The crawlphish and cloudphish analysis modules support another filtering scheme designed for URLs. See here for details about this whitelisting method.","title":"Crawlphish and Cloudphish Whitelisting and Blacklisting"},{"location":"design/whitelisting/#submission-filtering","text":"ACE receives work in the form of submissions . A filtering system is available that is specific to the format of that data.","title":"Submission Filtering"},{"location":"design/yara/","text":"Yara \u00b6 ACE depends on yara for a number of different use cases including file scanning for detection and analysis purposes. submission filtering . routing when scanning emails. ACE uses the yara scanner project as a wrapper around yara scanning. This gives ACE the capability to fully utilize the CPU resources to scan yara files. use meta fields to target specific files. ACE runs a yara scanning service to facilitate fast yara scanning.","title":"Yara"},{"location":"design/yara/#yara","text":"ACE depends on yara for a number of different use cases including file scanning for detection and analysis purposes. submission filtering . routing when scanning emails. ACE uses the yara scanner project as a wrapper around yara scanning. This gives ACE the capability to fully utilize the CPU resources to scan yara files. use meta fields to target specific files. ACE runs a yara scanning service to facilitate fast yara scanning.","title":"Yara"},{"location":"design/yara_scanner_service/","text":"Yara Scanner Service \u00b6 The yara scanner service is an ACE service that runs a local-only distributed yara scanner across a configurable number of processes. There are two reasons for this. Yara rules can take a long time to compile. Yara supports pre-compiled rules. Rather than loading pre-compiled rules every time ACE wants to scan something, it just keeps the \"yara scanner\" in memory and re-uses it. Yara scanning is also single-threaded. THe yara scanner service runs yara on multiple processes allowing ACE to fully utilize the resources of the system. Yara Rules \u00b6 ACE loads yara rules when the yara scanner service starts up. It then monitors the source of the yara rules for changes and then recompiles the rules as needed. ACE also uses a specific format of meta tagging to control which files a given rule should fire on. The functionality of the yara scanning service comes from the yara scanner project . See this project of details on how yara scanning works.","title":"Yara Scanner Service"},{"location":"design/yara_scanner_service/#yara-scanner-service","text":"The yara scanner service is an ACE service that runs a local-only distributed yara scanner across a configurable number of processes. There are two reasons for this. Yara rules can take a long time to compile. Yara supports pre-compiled rules. Rather than loading pre-compiled rules every time ACE wants to scan something, it just keeps the \"yara scanner\" in memory and re-uses it. Yara scanning is also single-threaded. THe yara scanner service runs yara on multiple processes allowing ACE to fully utilize the resources of the system.","title":"Yara Scanner Service"},{"location":"design/yara_scanner_service/#yara-rules","text":"ACE loads yara rules when the yara scanner service starts up. It then monitors the source of the yara rules for changes and then recompiles the rules as needed. ACE also uses a specific format of meta tagging to control which files a given rule should fire on. The functionality of the yara scanning service comes from the yara scanner project . See this project of details on how yara scanning works.","title":"Yara Rules"},{"location":"development/","text":"Development Guide \u00b6 A guide to help with ACE development. Collectors \u00b6 Parsing Data \u00b6 ACE has a couple built in classes that can aid you in parsing and transforming data into a format that can be used to submit an alert to your collector. RegexObservableParserGroup The RegexObservableParserGroup can be used to add various regular expressions ``` {.sourceCode .python} from saq.constants import ( F_URL, F_IPV4, F_IPV4_CONVERSATION, F_IPV4_FULL_CONVERSATION, DIRECTIVE_CRAWL, ) from saq.util import RegexObservableParserGroup sample_log = \"dst_ip: 10.0.0.2, port: 8080, src_ip: 10.0.0.1, port: 5555, url: https://hello.local/malicious/payload.exe\\n\" Tags you add here will be applied to all observables you find unless \u00b6 you override the tags value at the individual parser being added. \u00b6 parser_group = RegexObservableParserGroup(tags=['my_custom_parser']) This will capture ALL matching strings. If any are duplicates, \u00b6 they will be filtered out when you generate the observables. \u00b6 parser_group.add(r'ip: ([0-9.]+)', F_IPV4) You can also add multiple capture groups and determine in which \u00b6 Order the items are extracted. \u00b6 For example, the source IP comes second in our test string, but the \u00b6 F_IPV4_CONVERSATION is in this format: src-ip_dst-ip. \u00b6 Note the capture groups are in reverse order to accomodate: \u00b6 parser_group.add(r'ip: ([0-9.]+).*ip: ([0-9.]+)', F_IPV4_CONVERSATION, capture_groups=[2, 1]) You can also change the delimiter for how all the capture groups \u00b6 are joined together. For example, the F_IPV4_FULL_CONVERSATION \u00b6 is delimited by colons. \u00b6 parser_group.add( r'dst_ip: ([0-9.]+), port: ([0-9]+), src_ip: ([0-9.]+), port: ([0-9]+)', F_IPV4_FULL_CONVERSATION, delimiter=':', capture_groups=[3, 4, 1, 2] ) You can also add directives to control analysis/actions taken on your \u00b6 observable. \u00b6 parser_group.add(r'url: ([^\\n]+)', F_URL, directives=[DIRECTIVE_CRAWL]) Once you've added your parsers, you can parse the data: ``` {.sourceCode .python} parser_group.parse_content(sample_log) Then you can access the observables: ``` {.sourceCode .python} observables = parser_group.observables The extractions from this example would create the following list of dictionairies which are in an appropriate format to be submitted to the collector: ``` {.sourceCode .python} [ { \"type\": \"ipv4\", \"value\": \"10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4\", \"value\": \"10.0.0.1\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_conversation\", \"value\": \"10.0.0.1_10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_full_conversation\", \"value\": \"10.0.0.1:5555:10.0.0.2:8080\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"url\", \"value\": \"https://hello.local/malicious/payload.exe\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [ \"crawl\" ] } ] What if you've created your own observable type? Or maybe you want to change the way the parser groups work. You can make subclass the saq.util.RegexObservableParser , override the parsing logic, and then pass it into the parser group: ``` {.sourceCode .python} from saq.constants import F_CUSTOM_TYPE from saq.util import RegexObservableParserGroup, RegexObservableParser class MyParser(RegexObservableParser): def init (self, args, kwargs): super(). init ( args, **kwargs) # override the RegexObservableParser.parse() method def parse(self, text): # My custom parsing logic pass parser_group = RegexObservableParserGroup() parser_group.add(r'ip: ([0-9.]+)', override_class=MyParser) parser_group.parse_content(my_log) When you're ready to submit to the collector: ``` {.sourceCode .python} observables = parser_group.observables from saq.collectors import Submission from saq.constanants import ANALYSIS_MODE_CORRELATION submission = Submission( description=\"My custom alert\", analysis_mode=ANALYSIS_MODE_CORRELATION, tool = 'my custom tool', tool_instance = 'my custom tool instance', type = 'custom_type', event_time = 'datetime_from_alert', details = [], observables = observables, tags=[], files=[], )","title":"Index"},{"location":"development/#development-guide","text":"A guide to help with ACE development.","title":"Development Guide"},{"location":"development/#collectors","text":"","title":"Collectors"},{"location":"development/#parsing-data","text":"ACE has a couple built in classes that can aid you in parsing and transforming data into a format that can be used to submit an alert to your collector. RegexObservableParserGroup The RegexObservableParserGroup can be used to add various regular expressions ``` {.sourceCode .python} from saq.constants import ( F_URL, F_IPV4, F_IPV4_CONVERSATION, F_IPV4_FULL_CONVERSATION, DIRECTIVE_CRAWL, ) from saq.util import RegexObservableParserGroup sample_log = \"dst_ip: 10.0.0.2, port: 8080, src_ip: 10.0.0.1, port: 5555, url: https://hello.local/malicious/payload.exe\\n\"","title":"Parsing Data"},{"location":"development/#tags-you-add-here-will-be-applied-to-all-observables-you-find-unless","text":"","title":"Tags you add here will be applied to all observables you find unless"},{"location":"development/#you-override-the-tags-value-at-the-individual-parser-being-added","text":"parser_group = RegexObservableParserGroup(tags=['my_custom_parser'])","title":"you override the tags value at the individual parser being added."},{"location":"development/#this-will-capture-all-matching-strings-if-any-are-duplicates","text":"","title":"This will capture ALL matching strings. If any are duplicates,"},{"location":"development/#they-will-be-filtered-out-when-you-generate-the-observables","text":"parser_group.add(r'ip: ([0-9.]+)', F_IPV4)","title":"they will be filtered out when you generate the observables."},{"location":"development/#you-can-also-add-multiple-capture-groups-and-determine-in-which","text":"","title":"You can also add multiple capture groups and determine in which"},{"location":"development/#order-the-items-are-extracted","text":"","title":"Order the items are extracted."},{"location":"development/#for-example-the-source-ip-comes-second-in-our-test-string-but-the","text":"","title":"For example, the source IP comes second in our test string, but the"},{"location":"development/#f_ipv4_conversation-is-in-this-format-src-ip_dst-ip","text":"","title":"F_IPV4_CONVERSATION is in this format: src-ip_dst-ip."},{"location":"development/#note-the-capture-groups-are-in-reverse-order-to-accomodate","text":"parser_group.add(r'ip: ([0-9.]+).*ip: ([0-9.]+)', F_IPV4_CONVERSATION, capture_groups=[2, 1])","title":"Note the capture groups are in reverse order to accomodate:"},{"location":"development/#you-can-also-change-the-delimiter-for-how-all-the-capture-groups","text":"","title":"You can also change the delimiter for how all the capture groups"},{"location":"development/#are-joined-together-for-example-the-f_ipv4_full_conversation","text":"","title":"are joined together. For example, the F_IPV4_FULL_CONVERSATION"},{"location":"development/#is-delimited-by-colons","text":"parser_group.add( r'dst_ip: ([0-9.]+), port: ([0-9]+), src_ip: ([0-9.]+), port: ([0-9]+)', F_IPV4_FULL_CONVERSATION, delimiter=':', capture_groups=[3, 4, 1, 2] )","title":"is delimited by colons."},{"location":"development/#you-can-also-add-directives-to-control-analysisactions-taken-on-your","text":"","title":"You can also add directives to control analysis/actions taken on your"},{"location":"development/#observable","text":"parser_group.add(r'url: ([^\\n]+)', F_URL, directives=[DIRECTIVE_CRAWL]) Once you've added your parsers, you can parse the data: ``` {.sourceCode .python} parser_group.parse_content(sample_log) Then you can access the observables: ``` {.sourceCode .python} observables = parser_group.observables The extractions from this example would create the following list of dictionairies which are in an appropriate format to be submitted to the collector: ``` {.sourceCode .python} [ { \"type\": \"ipv4\", \"value\": \"10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4\", \"value\": \"10.0.0.1\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_conversation\", \"value\": \"10.0.0.1_10.0.0.2\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"ipv4_full_conversation\", \"value\": \"10.0.0.1:5555:10.0.0.2:8080\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [] }, { \"type\": \"url\", \"value\": \"https://hello.local/malicious/payload.exe\", \"tags\": [ \"my_custom_parser\" ], \"directives\": [ \"crawl\" ] } ] What if you've created your own observable type? Or maybe you want to change the way the parser groups work. You can make subclass the saq.util.RegexObservableParser , override the parsing logic, and then pass it into the parser group: ``` {.sourceCode .python} from saq.constants import F_CUSTOM_TYPE from saq.util import RegexObservableParserGroup, RegexObservableParser class MyParser(RegexObservableParser): def init (self, args, kwargs): super(). init ( args, **kwargs) # override the RegexObservableParser.parse() method def parse(self, text): # My custom parsing logic pass parser_group = RegexObservableParserGroup() parser_group.add(r'ip: ([0-9.]+)', override_class=MyParser) parser_group.parse_content(my_log) When you're ready to submit to the collector: ``` {.sourceCode .python} observables = parser_group.observables from saq.collectors import Submission from saq.constanants import ANALYSIS_MODE_CORRELATION submission = Submission( description=\"My custom alert\", analysis_mode=ANALYSIS_MODE_CORRELATION, tool = 'my custom tool', tool_instance = 'my custom tool instance', type = 'custom_type', event_time = 'datetime_from_alert', details = [], observables = observables, tags=[], files=[], )","title":"observable."},{"location":"development/api/","text":"ACE API Examples \u00b6 Let's go through a few examples using the ACE API. We will specifically use the ace_api python library. Connect to a Server \u00b6 By default, the ace_api library will attempt to connect to localhost. Use the ace_api.set_default_remote_host function to have the library connect to a different server. The OS's certificate store is used to validate the server. See ace_api.set_default_ssl_ca_path to change this behavior. >>> import ace_api >>> server = 'ace.integraldefense.com' >>> ace_api.set_default_remote_host(server) >>> ace_api.ping() {'result': 'pong'} You can over-ride this default in the ace_api.Analysis class with the ace_api.Analysis.set_remote_host method and you can also manually specify a remote host with any submit. >>> analysis = ace_api.Analysis('this is the analysis description') >>> analysis.remote_host 'ace.integraldefense.com' >>> analysis.set_remote_host('something.else.com').remote_host 'something.else.com' >>> ace_api.default_remote_host 'ace.integraldefense.com' If your ACE instance is listening on a port other than 443, specify it like so: >>> ace_api.set_default_remote_host('ace.integraldefense.com:24443') >>> ace_api.default_remote_host 'ace.integraldefense.com:24443' Submitting data to ACE \u00b6 You should submit data to ace by first creating an Analysis object and loading it with the data you want to submit for analysis and/or correlation. The below examples show how to perform some common submissions. Submit a File \u00b6 Say we have a suspect file in our current working director named \"Business.doc\" that we want to submit to ACE. First, we create an analysis object and then we pass the path to the file to the ace_api.Analysis.add_file method. We will also include some tags and check the status (ace_api.Analysis.status) of the analysis as ACE works on the submission. >>> path_to_file = 'Business.doc' >>> analysis.add_file(path_to_file) <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.add_tag('Business.doc').add_tag('suspicious doc') <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.submit() <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.status 'NEW' >>> analysis.status 'ANALYZING' >>> analysis.status 'COMPLETE (Alerted with 8 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=137842ac-9d53-4a25-8066-ad2a1f6cfa17 Submit a URL \u00b6 Two examples of submitting a URL to ACE follow. The first example shows how to submit a URL by adding the URL as an observable to an Analysis object. This also allows us to demontrate the use of directives. The second example shows how simple it is to submit a URL for analysis directly to Cloudphish. As an observable \u00b6 You can submit as many observables <observable> as you desire in a submission to ACE, but they won't neccessarily get passed to every analysis module that can work on them by default. This is the case for URL observables, which by themselves, require the crawl directive to tell ACE you want to download the conent from the URL for further analysis. Submititing a request for a suspicious URL to be analyzed, note the use of the crawl directive and how to get a list of the valid directives. >>> suspicious_url = 'http://davidcizek.cz/Invoice/ifKgg-jrzA_PvC-a7' >>> analysis = ace_api.Analysis('Suspicious URL') >>> analysis.add_tag('suspicious_url') <ace_api.Analysis object at 0x7f23d57e7588> >>> for d in ace_api.get_valid_directives()['result']: ... if d['name'] == 'crawl': ... print(d['description']) ... crawl the URL >>> analysis.add_url(suspicious_url, directives=['crawl']).submit() <ace_api.Analysis object at 0x7f23d57e7588> >>> analysis.status 'COMPLETE (Alerted with 9 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=de66b2d3-f273-4bdd-a05b-771ecf5c8a76 Using Cloudphish \u00b6 If you just want ACE to analyze a single URL, it's best to submit directly to Cloudphish. In this example, a URL is submitted to cloudphish that cloudphish has never seen before and a 'NEW' status is returned. After cloudphish has finished analyzing the URL, the status changes to 'ANALYZED' and the analysis_result tells us at least one detection was found (as we alerted). >>> another_url = 'http://medicci.ru/myATT/tu8794_QcbkoEsv_Xw20pYh7ij' >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'NEW' >>> # Query again, a moment later: ... >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'ANALYZED' >>> cp_result['analysis_result'] 'ALERT' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(ace_api.default_remote_host, cp_result['uuid']) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=732ec396-ce20-463f-82b0-6b043b07f941 Forcing Alert Creation \u00b6 By default, ACE alerts are only created if an detection is made in the initially submitted analysis. You can force alert creation by changing the default analysis mode from analysis to correlation . This is accomplished like so: >>> analysis = ace_api.Analysis('This is an analysis with no detections', analysis_mode='correlation') >>> analysis.submit() <ace_api.Analysis object at 0x7fbe81af66a0> >>> analysis.status 'COMPLETE (Alerted with 0 detections)' Downloading Cloudphish Results \u00b6 Cloudphish keeps a cache of the URL content it downloads. In this example we will download the results of the URL submitted in the previous example, which in this case is a malicious word document. >>> ace_api.cloudphish_download(another_url, output_path='cp_result.raw') True >>> os.path.exists('cp_result.raw') True Downloading an Alert \u00b6 You can use the ace_api.download function to download an entire Alert. Below, we download an entire Alert and have it written to a directory named by the Alert's UUID.: >>> uuid = cp_result['uuid'] >>> >>> uuid '732ec396-ce20-463f-82b0-6b043b07f941' >>> ace_api.download(uuid, target_dir=uuid) Now, there is a new directory named '732ec396-ce20-463f-82b0-6b043b07f941' in our current working directory that contians all of the files and data from the alert with uuid 732ec396-ce20-463f-82b0-6b043b07f941. Use the ace_api.load_analysis function to load an alert into a new Analysis object. ACE API \u00b6 Python Library \u00b6 A python library exits for intereacting with the ACE API. You can install it wil pip: pip3 install ace_api . Common API \u00b6 Alert API \u00b6 submit \u00b6 Submits a new alert to ACE. These go directly into the correlation engine for analysis and show up to analysts as alerts. Parameters: alert - JSON dict with the following schema : { 'tool': tool_name, 'tool_instance': tool_instance_name, 'type': alert_type, 'description': alert description, 'event_time': time of the alert/event (in %Y-%m-%dT%H:%M:%S.%f%z format), 'details': free-form JSON dict of anything you want to include, 'observables': (see below), 'tags': a list of tags to add to the alert, } The observables field is a list of zero or more dicts with the following format : { 'type': The type of the observable, 'value': The value of the observable, 'time': The optional time of the observable (can be null), 'tags': Optional list of tags to add to the observable, 'directives': Optional list of directives to add to the observable, } To attach files to the alert use the field named file .","title":"Api"},{"location":"development/api/#ace-api-examples","text":"Let's go through a few examples using the ACE API. We will specifically use the ace_api python library.","title":"ACE API Examples"},{"location":"development/api/#connect-to-a-server","text":"By default, the ace_api library will attempt to connect to localhost. Use the ace_api.set_default_remote_host function to have the library connect to a different server. The OS's certificate store is used to validate the server. See ace_api.set_default_ssl_ca_path to change this behavior. >>> import ace_api >>> server = 'ace.integraldefense.com' >>> ace_api.set_default_remote_host(server) >>> ace_api.ping() {'result': 'pong'} You can over-ride this default in the ace_api.Analysis class with the ace_api.Analysis.set_remote_host method and you can also manually specify a remote host with any submit. >>> analysis = ace_api.Analysis('this is the analysis description') >>> analysis.remote_host 'ace.integraldefense.com' >>> analysis.set_remote_host('something.else.com').remote_host 'something.else.com' >>> ace_api.default_remote_host 'ace.integraldefense.com' If your ACE instance is listening on a port other than 443, specify it like so: >>> ace_api.set_default_remote_host('ace.integraldefense.com:24443') >>> ace_api.default_remote_host 'ace.integraldefense.com:24443'","title":"Connect to a Server"},{"location":"development/api/#submitting-data-to-ace","text":"You should submit data to ace by first creating an Analysis object and loading it with the data you want to submit for analysis and/or correlation. The below examples show how to perform some common submissions.","title":"Submitting data to ACE"},{"location":"development/api/#submit-a-file","text":"Say we have a suspect file in our current working director named \"Business.doc\" that we want to submit to ACE. First, we create an analysis object and then we pass the path to the file to the ace_api.Analysis.add_file method. We will also include some tags and check the status (ace_api.Analysis.status) of the analysis as ACE works on the submission. >>> path_to_file = 'Business.doc' >>> analysis.add_file(path_to_file) <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.add_tag('Business.doc').add_tag('suspicious doc') <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.submit() <ace_api.Analysis object at 0x7f23d57e74e0> >>> analysis.status 'NEW' >>> analysis.status 'ANALYZING' >>> analysis.status 'COMPLETE (Alerted with 8 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=137842ac-9d53-4a25-8066-ad2a1f6cfa17","title":"Submit a File"},{"location":"development/api/#submit-a-url","text":"Two examples of submitting a URL to ACE follow. The first example shows how to submit a URL by adding the URL as an observable to an Analysis object. This also allows us to demontrate the use of directives. The second example shows how simple it is to submit a URL for analysis directly to Cloudphish.","title":"Submit a URL"},{"location":"development/api/#as-an-observable","text":"You can submit as many observables <observable> as you desire in a submission to ACE, but they won't neccessarily get passed to every analysis module that can work on them by default. This is the case for URL observables, which by themselves, require the crawl directive to tell ACE you want to download the conent from the URL for further analysis. Submititing a request for a suspicious URL to be analyzed, note the use of the crawl directive and how to get a list of the valid directives. >>> suspicious_url = 'http://davidcizek.cz/Invoice/ifKgg-jrzA_PvC-a7' >>> analysis = ace_api.Analysis('Suspicious URL') >>> analysis.add_tag('suspicious_url') <ace_api.Analysis object at 0x7f23d57e7588> >>> for d in ace_api.get_valid_directives()['result']: ... if d['name'] == 'crawl': ... print(d['description']) ... crawl the URL >>> analysis.add_url(suspicious_url, directives=['crawl']).submit() <ace_api.Analysis object at 0x7f23d57e7588> >>> analysis.status 'COMPLETE (Alerted with 9 detections)' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(analysis.remote_host, analysis.uuid) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=de66b2d3-f273-4bdd-a05b-771ecf5c8a76","title":"As an observable"},{"location":"development/api/#using-cloudphish","text":"If you just want ACE to analyze a single URL, it's best to submit directly to Cloudphish. In this example, a URL is submitted to cloudphish that cloudphish has never seen before and a 'NEW' status is returned. After cloudphish has finished analyzing the URL, the status changes to 'ANALYZED' and the analysis_result tells us at least one detection was found (as we alerted). >>> another_url = 'http://medicci.ru/myATT/tu8794_QcbkoEsv_Xw20pYh7ij' >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'NEW' >>> # Query again, a moment later: ... >>> cp_result = ace_api.cloudphish_submit(another_url) >>> cp_result['status'] 'ANALYZED' >>> cp_result['analysis_result'] 'ALERT' >>> result_url = 'https://{}/ace/analysis?direct={}'.format(ace_api.default_remote_host, cp_result['uuid']) >>> print(\"\\nThe results of this submission can be viewed here: {}\".format(result_url)) The results of this submission can be viewed here: https://ace.integraldefense.com/ace/analysis?direct=732ec396-ce20-463f-82b0-6b043b07f941","title":"Using Cloudphish"},{"location":"development/api/#forcing-alert-creation","text":"By default, ACE alerts are only created if an detection is made in the initially submitted analysis. You can force alert creation by changing the default analysis mode from analysis to correlation . This is accomplished like so: >>> analysis = ace_api.Analysis('This is an analysis with no detections', analysis_mode='correlation') >>> analysis.submit() <ace_api.Analysis object at 0x7fbe81af66a0> >>> analysis.status 'COMPLETE (Alerted with 0 detections)'","title":"Forcing Alert Creation"},{"location":"development/api/#downloading-cloudphish-results","text":"Cloudphish keeps a cache of the URL content it downloads. In this example we will download the results of the URL submitted in the previous example, which in this case is a malicious word document. >>> ace_api.cloudphish_download(another_url, output_path='cp_result.raw') True >>> os.path.exists('cp_result.raw') True","title":"Downloading Cloudphish Results"},{"location":"development/api/#downloading-an-alert","text":"You can use the ace_api.download function to download an entire Alert. Below, we download an entire Alert and have it written to a directory named by the Alert's UUID.: >>> uuid = cp_result['uuid'] >>> >>> uuid '732ec396-ce20-463f-82b0-6b043b07f941' >>> ace_api.download(uuid, target_dir=uuid) Now, there is a new directory named '732ec396-ce20-463f-82b0-6b043b07f941' in our current working directory that contians all of the files and data from the alert with uuid 732ec396-ce20-463f-82b0-6b043b07f941. Use the ace_api.load_analysis function to load an alert into a new Analysis object.","title":"Downloading an Alert"},{"location":"development/api/#ace-api","text":"","title":"ACE API"},{"location":"development/api/#python-library","text":"A python library exits for intereacting with the ACE API. You can install it wil pip: pip3 install ace_api .","title":"Python Library"},{"location":"development/api/#common-api","text":"","title":"Common API"},{"location":"development/api/#alert-api","text":"","title":"Alert API"},{"location":"development/api/#submit","text":"Submits a new alert to ACE. These go directly into the correlation engine for analysis and show up to analysts as alerts. Parameters: alert - JSON dict with the following schema : { 'tool': tool_name, 'tool_instance': tool_instance_name, 'type': alert_type, 'description': alert description, 'event_time': time of the alert/event (in %Y-%m-%dT%H:%M:%S.%f%z format), 'details': free-form JSON dict of anything you want to include, 'observables': (see below), 'tags': a list of tags to add to the alert, } The observables field is a list of zero or more dicts with the following format : { 'type': The type of the observable, 'value': The value of the observable, 'time': The optional time of the observable (can be null), 'tags': Optional list of tags to add to the observable, 'directives': Optional list of directives to add to the observable, } To attach files to the alert use the field named file .","title":"submit"},{"location":"development/history/","text":"Developer README \u00b6 This document explains the reasons behind some of the stranger design decisions made for this project. SAQ = ACE \u00b6 When the project first started we called it the Simple Alert Queue (SAQ). It was later renamed to the Analysis Correlation Engine (ACE). There are still a lot of references to SAQ left, including the name of the core library ( import saq ) and the SAQ_HOME environment variable. Eveything was initially command line driven. \u00b6 The original UI of the project was CLI. So there's still a lot of that left. Most of what you can do can also be done via the command line, including full analysis of observables. Along those lines, it was also meant to be able to be executed from any directory. This is probably no longer true, but there are a number of times where the code assumes it is running in some other directory. This was an internal project. \u00b6 There's a number of basic things that you would expect would exist that don't. For example, there's no way to manage users from the GUI. It must be done from the command line. And even then, there's no support to delete a user. We didn't have any turnover for 5 years so this was never a requirement. And the along those lines there's little effort put into account security internally. There are no \"roles\" or \"administrators\". The database came later. \u00b6 Very little of the analysis data is stored in the database. From the beginning of the project I wanted the data to be stored in a schema-less JSON structure on the filesystem. This would allow analysts to simply grep the files for whatever they were looking for. I (reluctantly) looked at MongoDB as a way to index the data and speed up the searches. This was quickly abandoned (it was slowing down development for various reasons.) Later when the GUI was added to the project we started storing data in MySQL. I knew that we would be modifying this system a lot. So trying to create a database schema that encompassed everything we would ever want to do was not realisitic. Making major changes to large database schemas is no easy task. Today the database is used to manage the workload of the collectors and engines, and to provide the GUI (and API) for the analysts. The data.json JSON files that hold the results of the analysis are actually the official records of the analysis. The database is kept in sync with these files. At some point it would make sense to index these JSON files in a system like Elasticsearch. Unit testing. \u00b6 My one regret with this project was not creating unit tests as I went. I didn't start adding unit tests until we were \\~4 years into the project. Unit test coverage is not what it shoud be, but I would expect that to improve over time. Final words. \u00b6 I think it's worth noting that this project was created to enable and improve our analysts. We were not designing a product . We were also moving as quickly as we saw threat actors change tactics. As soon as we saw a new techique being used, we would quickly implement a feature to ACE that would allow us to detect that. So there's a number of places where the code looks hastily thrown together. Hopefully this file helps to explain some of the oddness you may see in the code.","title":"History"},{"location":"development/history/#developer-readme","text":"This document explains the reasons behind some of the stranger design decisions made for this project.","title":"Developer README"},{"location":"development/history/#saq-ace","text":"When the project first started we called it the Simple Alert Queue (SAQ). It was later renamed to the Analysis Correlation Engine (ACE). There are still a lot of references to SAQ left, including the name of the core library ( import saq ) and the SAQ_HOME environment variable.","title":"SAQ = ACE"},{"location":"development/history/#eveything-was-initially-command-line-driven","text":"The original UI of the project was CLI. So there's still a lot of that left. Most of what you can do can also be done via the command line, including full analysis of observables. Along those lines, it was also meant to be able to be executed from any directory. This is probably no longer true, but there are a number of times where the code assumes it is running in some other directory.","title":"Eveything was initially command line driven."},{"location":"development/history/#this-was-an-internal-project","text":"There's a number of basic things that you would expect would exist that don't. For example, there's no way to manage users from the GUI. It must be done from the command line. And even then, there's no support to delete a user. We didn't have any turnover for 5 years so this was never a requirement. And the along those lines there's little effort put into account security internally. There are no \"roles\" or \"administrators\".","title":"This was an internal project."},{"location":"development/history/#the-database-came-later","text":"Very little of the analysis data is stored in the database. From the beginning of the project I wanted the data to be stored in a schema-less JSON structure on the filesystem. This would allow analysts to simply grep the files for whatever they were looking for. I (reluctantly) looked at MongoDB as a way to index the data and speed up the searches. This was quickly abandoned (it was slowing down development for various reasons.) Later when the GUI was added to the project we started storing data in MySQL. I knew that we would be modifying this system a lot. So trying to create a database schema that encompassed everything we would ever want to do was not realisitic. Making major changes to large database schemas is no easy task. Today the database is used to manage the workload of the collectors and engines, and to provide the GUI (and API) for the analysts. The data.json JSON files that hold the results of the analysis are actually the official records of the analysis. The database is kept in sync with these files. At some point it would make sense to index these JSON files in a system like Elasticsearch.","title":"The database came later."},{"location":"development/history/#unit-testing","text":"My one regret with this project was not creating unit tests as I went. I didn't start adding unit tests until we were \\~4 years into the project. Unit test coverage is not what it shoud be, but I would expect that to improve over time.","title":"Unit testing."},{"location":"development/history/#final-words","text":"I think it's worth noting that this project was created to enable and improve our analysts. We were not designing a product . We were also moving as quickly as we saw threat actors change tactics. As soon as we saw a new techique being used, we would quickly implement a feature to ACE that would allow us to detect that. So there's a number of places where the code looks hastily thrown together. Hopefully this file helps to explain some of the oddness you may see in the code.","title":"Final words."},{"location":"installation/","text":"Installation + Adding Data \u00b6 Super fast How-To \u00b6 Clean Ubuntu 18 install. Create username/group ace/ace. Add ace to sudo. Login as user ace. sudo mkdir /opt/ace && sudo chown ace:ace /opt/ace && cd /opt/ace git clone https://github.com/ace-ecosystem/ACE.git . ./installer/source_install source load_environment ./ace add-user username email_address Goto https://127.0.0.1/ace/ or whatever IP address you're using. If you run into certificate / SSL issues, see the Troubleshooting and Help section below. Detailed Installation \u00b6 Install Ubuntu Server 18.04 LST \u00b6 The size specifications for your server need to be based on your needs. At a minimum, the server should have 4 GB RAM and 20 GB storage drive. When installing the server, all of the default configurations are fine. Getting Everything Ready \u00b6 The ace User \u00b6 $ sudo adduser ace $ sudo adduser ace sudo $ sudo su - ace $ sudo chown ace:ace /opt Cloning ACE \u00b6 As the ace user you previously created, cd into /opt and git clone the ace-ecosystem ACE master branch: https://github.com/ace-ecosystem/ACE.git : $ cd /opt $ git clone https://github.com/ace-ecosystem/ACE.git ace Run the Installer \u00b6 With everything ready <get-enviro-ready>, you can now run the ACE installer. Run the installer as the ace user. You will be prompted for the password when certain things are run using sudo. This will take a little while to complete. $ cd /opt/ace $ ./installer/source_install Set Up Environment \u00b6 Next, you will need to load the default environment variables ACE depends on. This load needs to be sourced from bash with the following command: $ source load_environment This should already be added to the ace account bashrc, so the next login should automatically load it. Create Users \u00b6 Users are managed from the ACE command line with the following ace commands: add-user Add a new user to the system. modify-user Modifies an existing user on the system. delete-user Deletes an existing user from the system. Create your first user so that you can log into the ACE GUI: ./ace add-user <username> <email_address> Log into the GUI \u00b6 You should now be able to browse to https://your_ip/ace/ and log into ACE with the user you previously created. Troubleshooting & Help \u00b6 There are a couple snags and gotchas that you can run into when installing ACE. This section will detail a few, but it's still a work in process. SSL Errors \u00b6 You may run into an SSL error that will include the following text: Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),)) This error is less common when running on a local VM; However, it is fairly common when ACE is installed on a server with a domain. For example, AWS EC2 assigns a hostname such as ip-10-10-10-10.ec2.internal to their EC2 instances. Two quick options to fix this error if you are planning on using your ACE machine with the default installation: Add the FQDN of your host as the ServerName in /opt/ace/etc/saq_apache.conf Add the FQDN of your host as a ServerAlias in /opt/ace/etc/saq_apache.conf Then, restart the apache service (authentication required): $ service apache2 restart Example 1: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ip-10-10-10-10.ec2.internal # Rest of the config... Example 2: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ace.local ServerAlias ip-10-10-10-10.ec2.internal # Rest of the config... No Web GUI? \u00b6 Make sure apache2 is running and the /etc/apache2/sites-enabled/ace.conf configuration is loaded. The ace.conf should be a symlink in /etc/apache2/sites-available that points to /opt/ace/etc/saq_apache.conf . Alerts staying in 'NEW' status? \u00b6 Make sure the ACE engine is running. You can do this by running the following: cd /opt/ace && ace service start engine --daemon Start ACE \u00b6 You should now have a working installation, but you need to start the correlation engine. This is accomplished with the ace service start engine --daemon command. Getting Data into ACE \u00b6 A bare-bones ACE install is not going to be very effective without data. You can use the Manual Analysis section to submit observables to ACE. However, you're going to want to turn on some of the additional Integrations and Modules that come with ACE by default. Manual Analysis \u00b6 Via the Manual Analysis page, an analyst can submit an observable for ACE to analyze. Observables can be submitted for analysis via the Manual Analysis page By default, the Insert Date is set to the current time, and the Description is set to 'Manual Correlation'. You can change the description to something meaningful. The Target Company will also be set to default, which should be fine for most ACE installations. Select the type of observable you wish to correlate and then provide the value. Click the Add button to correlate more than one observable type and/or value at a time. Shortly after you've submitted your observable(s) for correlation, you will see your alert appear on the Manage Alerts page with the description you provided. The alert status will change to 'Complete' once ACE is finished performing its analysis. You must currently refresh the Manage Alerts page to see the alert status updates. Using the API \u00b6 ACE has an API that makes it simple to submit data to ACE for analysis and/or correlation. Check out the ACE API Examples and ACE API section for more information.","title":"Index"},{"location":"installation/#installation-adding-data","text":"","title":"Installation + Adding Data"},{"location":"installation/#super-fast-how-to","text":"Clean Ubuntu 18 install. Create username/group ace/ace. Add ace to sudo. Login as user ace. sudo mkdir /opt/ace && sudo chown ace:ace /opt/ace && cd /opt/ace git clone https://github.com/ace-ecosystem/ACE.git . ./installer/source_install source load_environment ./ace add-user username email_address Goto https://127.0.0.1/ace/ or whatever IP address you're using. If you run into certificate / SSL issues, see the Troubleshooting and Help section below.","title":"Super fast How-To"},{"location":"installation/#detailed-installation","text":"","title":"Detailed Installation"},{"location":"installation/#install-ubuntu-server-1804-lst","text":"The size specifications for your server need to be based on your needs. At a minimum, the server should have 4 GB RAM and 20 GB storage drive. When installing the server, all of the default configurations are fine.","title":"Install Ubuntu Server 18.04 LST"},{"location":"installation/#getting-everything-ready","text":"","title":"Getting Everything Ready"},{"location":"installation/#the-ace-user","text":"$ sudo adduser ace $ sudo adduser ace sudo $ sudo su - ace $ sudo chown ace:ace /opt","title":"The ace User"},{"location":"installation/#cloning-ace","text":"As the ace user you previously created, cd into /opt and git clone the ace-ecosystem ACE master branch: https://github.com/ace-ecosystem/ACE.git : $ cd /opt $ git clone https://github.com/ace-ecosystem/ACE.git ace","title":"Cloning ACE"},{"location":"installation/#run-the-installer","text":"With everything ready <get-enviro-ready>, you can now run the ACE installer. Run the installer as the ace user. You will be prompted for the password when certain things are run using sudo. This will take a little while to complete. $ cd /opt/ace $ ./installer/source_install","title":"Run the Installer"},{"location":"installation/#set-up-environment","text":"Next, you will need to load the default environment variables ACE depends on. This load needs to be sourced from bash with the following command: $ source load_environment This should already be added to the ace account bashrc, so the next login should automatically load it.","title":"Set Up Environment"},{"location":"installation/#create-users","text":"Users are managed from the ACE command line with the following ace commands: add-user Add a new user to the system. modify-user Modifies an existing user on the system. delete-user Deletes an existing user from the system. Create your first user so that you can log into the ACE GUI: ./ace add-user <username> <email_address>","title":"Create Users"},{"location":"installation/#log-into-the-gui","text":"You should now be able to browse to https://your_ip/ace/ and log into ACE with the user you previously created.","title":"Log into the GUI"},{"location":"installation/#troubleshooting-help","text":"There are a couple snags and gotchas that you can run into when installing ACE. This section will detail a few, but it's still a work in process.","title":"Troubleshooting &amp; Help"},{"location":"installation/#ssl-errors","text":"You may run into an SSL error that will include the following text: Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),)) This error is less common when running on a local VM; However, it is fairly common when ACE is installed on a server with a domain. For example, AWS EC2 assigns a hostname such as ip-10-10-10-10.ec2.internal to their EC2 instances. Two quick options to fix this error if you are planning on using your ACE machine with the default installation: Add the FQDN of your host as the ServerName in /opt/ace/etc/saq_apache.conf Add the FQDN of your host as a ServerAlias in /opt/ace/etc/saq_apache.conf Then, restart the apache service (authentication required): $ service apache2 restart Example 1: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ip-10-10-10-10.ec2.internal # Rest of the config... Example 2: # /opt/ace/etc/saq_apache.conf <VirtualHost *:443> ServerName ace.local ServerAlias ip-10-10-10-10.ec2.internal # Rest of the config...","title":"SSL Errors"},{"location":"installation/#no-web-gui","text":"Make sure apache2 is running and the /etc/apache2/sites-enabled/ace.conf configuration is loaded. The ace.conf should be a symlink in /etc/apache2/sites-available that points to /opt/ace/etc/saq_apache.conf .","title":"No Web GUI?"},{"location":"installation/#alerts-staying-in-new-status","text":"Make sure the ACE engine is running. You can do this by running the following: cd /opt/ace && ace service start engine --daemon","title":"Alerts staying in 'NEW' status?"},{"location":"installation/#start-ace","text":"You should now have a working installation, but you need to start the correlation engine. This is accomplished with the ace service start engine --daemon command.","title":"Start ACE"},{"location":"installation/#getting-data-into-ace","text":"A bare-bones ACE install is not going to be very effective without data. You can use the Manual Analysis section to submit observables to ACE. However, you're going to want to turn on some of the additional Integrations and Modules that come with ACE by default.","title":"Getting Data into ACE"},{"location":"installation/#manual-analysis","text":"Via the Manual Analysis page, an analyst can submit an observable for ACE to analyze. Observables can be submitted for analysis via the Manual Analysis page By default, the Insert Date is set to the current time, and the Description is set to 'Manual Correlation'. You can change the description to something meaningful. The Target Company will also be set to default, which should be fine for most ACE installations. Select the type of observable you wish to correlate and then provide the value. Click the Add button to correlate more than one observable type and/or value at a time. Shortly after you've submitted your observable(s) for correlation, you will see your alert appear on the Manage Alerts page with the description you provided. The alert status will change to 'Complete' once ACE is finished performing its analysis. You must currently refresh the Manage Alerts page to see the alert status updates.","title":"Manual Analysis"},{"location":"installation/#using-the-api","text":"ACE has an API that makes it simple to submit data to ACE for analysis and/or correlation. Check out the ACE API Examples and ACE API section for more information.","title":"Using the API"},{"location":"modules/","text":"Analysis Modules \u00b6 ACE comes with a number of built-in analysis modules .","title":"Analysis Modules"},{"location":"modules/#analysis-modules","text":"ACE comes with a number of built-in analysis modules .","title":"Analysis Modules"},{"location":"modules/email_analyzer/","text":"Email Analyzer \u00b6 This module recursively scans RFC822 formatted files extracting observables, attachments, meta data and embedded emails. The Email Analyzer can be used as a detection engine for scanning emails. The default configuration defines an analysis mode called email which has all email-related analysis modules assigned to it. The analyzer also has special support for scanning Office365 journaled emails. This analysis module can be used in conjunction with the smtp collector or the email collector to scan emails. Analysis \u00b6 The Email Analyzer accepts file observables and performs the following actions. extracts and targets the embedded email inside of Office365 journaled emails. determines the delivery date of the email based on Received headers. determines who the email was actually delivered to using either tagging performed by the smtp collector or headers added by Office365. extract various observables from the headers. decodes RFC 2822 encoded header values extracts attachments adding them to the analysis work queue. generates and stores logging data . The analyzer separates the headers of the email from the rest and saves this file as NAME.headers where NAME is the name of the file observable of the email. This file contains the headers alone and is analyzed like any other file. This gives analysts a way to target just the headers of an email with signature matching analysis such as yara . Configuration \u00b6 [analysis_module_email_analyzer] module = saq.modules.email class = EmailAnalyzer enabled = yes ; relative path to the brotex custom whitelist file whitelist_path = etc/brotex.whitelist ; office365 journaling will cause outbound emails to also get journaled ; set this to no to scan outbound office365 emails scan_inbound_only = no ; When only scanning inbound emails from office365, scan the following outbound emails ; found in outbound_exceptions. Comma separated list! outbound_exceptions = whitelist_path points to the brotex whitelist configuration file that is used to whitelist email scanning. When scanning emails received by Office365 journaling the scan_inbound_only boolean option can be used to only scan emails received by the local organization. This may be a requirement for legal reasons. In this case the outbound_exceptions option contains a comma separated list of email addresses in the To: field of emails to match against. Only emails with To: addresses that match one of these values are scanned.","title":"Email Analyzer"},{"location":"modules/email_analyzer/#email-analyzer","text":"This module recursively scans RFC822 formatted files extracting observables, attachments, meta data and embedded emails. The Email Analyzer can be used as a detection engine for scanning emails. The default configuration defines an analysis mode called email which has all email-related analysis modules assigned to it. The analyzer also has special support for scanning Office365 journaled emails. This analysis module can be used in conjunction with the smtp collector or the email collector to scan emails.","title":"Email Analyzer"},{"location":"modules/email_analyzer/#analysis","text":"The Email Analyzer accepts file observables and performs the following actions. extracts and targets the embedded email inside of Office365 journaled emails. determines the delivery date of the email based on Received headers. determines who the email was actually delivered to using either tagging performed by the smtp collector or headers added by Office365. extract various observables from the headers. decodes RFC 2822 encoded header values extracts attachments adding them to the analysis work queue. generates and stores logging data . The analyzer separates the headers of the email from the rest and saves this file as NAME.headers where NAME is the name of the file observable of the email. This file contains the headers alone and is analyzed like any other file. This gives analysts a way to target just the headers of an email with signature matching analysis such as yara .","title":"Analysis"},{"location":"modules/email_analyzer/#configuration","text":"[analysis_module_email_analyzer] module = saq.modules.email class = EmailAnalyzer enabled = yes ; relative path to the brotex custom whitelist file whitelist_path = etc/brotex.whitelist ; office365 journaling will cause outbound emails to also get journaled ; set this to no to scan outbound office365 emails scan_inbound_only = no ; When only scanning inbound emails from office365, scan the following outbound emails ; found in outbound_exceptions. Comma separated list! outbound_exceptions = whitelist_path points to the brotex whitelist configuration file that is used to whitelist email scanning. When scanning emails received by Office365 journaling the scan_inbound_only boolean option can be used to only scan emails received by the local organization. This may be a requirement for legal reasons. In this case the outbound_exceptions option contains a comma separated list of email addresses in the To: field of emails to match against. Only emails with To: addresses that match one of these values are scanned.","title":"Configuration"},{"location":"modules/email_logging_analyzer/","text":"Email Logging Analyzer \u00b6 This modules takes the analysis generated by the Email Analyzer and generates logging suitable for various log consumption tools. It currently supports splunk and elastic search. This module also updates the smtplog table of the brocess database table . Configuration \u00b6 [analysis_module_email_logger] module = saq.modules.email class = EmailLoggingAnalyzer enabled = yes ; set this to yes to enable log formatted for splunk splunk_log_enabled = yes ; the subdirectory inside of splunk_log_dir (see [splunk_logging]) that contains the logs splunk_log_subdir = smtp ; set this to yes to update the smtplog table of the brocess database update_brocess = yes ; elasticsearch JSON logging json_log_enabled = yes ; relative file path of the generated JSON logs ; the file name is passed through strftime ; https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior ; {pid} is replaced with the process ID of the current executing process ; so that we don't have multiple processes writing to the same file json_log_path_format = email/email-{pid}-%%Y%%m%%d.json","title":"Email Logging Analyzer"},{"location":"modules/email_logging_analyzer/#email-logging-analyzer","text":"This modules takes the analysis generated by the Email Analyzer and generates logging suitable for various log consumption tools. It currently supports splunk and elastic search. This module also updates the smtplog table of the brocess database table .","title":"Email Logging Analyzer"},{"location":"modules/email_logging_analyzer/#configuration","text":"[analysis_module_email_logger] module = saq.modules.email class = EmailLoggingAnalyzer enabled = yes ; set this to yes to enable log formatted for splunk splunk_log_enabled = yes ; the subdirectory inside of splunk_log_dir (see [splunk_logging]) that contains the logs splunk_log_subdir = smtp ; set this to yes to update the smtplog table of the brocess database update_brocess = yes ; elasticsearch JSON logging json_log_enabled = yes ; relative file path of the generated JSON logs ; the file name is passed through strftime ; https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior ; {pid} is replaced with the process ID of the current executing process ; so that we don't have multiple processes writing to the same file json_log_path_format = email/email-{pid}-%%Y%%m%%d.json","title":"Configuration"},{"location":"user/","text":"User Guide \u00b6 Keep this in mind when working ACE alerts: ACE is meant to enable the analyst to QUICKLY disposition false positive alerts and recognize true positives. For convenience, here is a video recording that provides a tour of the ACE GUI and demonstrates how to work some ACE alerts. Many of the concepts in this orientation are covered. Quick Concept Touchpoint \u00b6 There are two core concepts an analyst must be familiar with when working ACE alerts: Observables and Dispositioning. Observables \u00b6 Observables are anything an analyst might \"observe\" or take note of during an investigation or when performing Alert Triage. For instance, an IP address is an observable, and a file name is a different type of observable. Some more observable types are: URLs, domain names, usernames, file hashes, file names, file paths, email addresses, and Yara signatures. ACE knows what kind of analysis to perform for a given observable type and how to correlate the value of an observable across all available data sources. In the process of correlating observables with other data sources, ACE will discover more observables to analyze and correlate. When an ACE alert is created from an initial detection point, the alert's 'root' level observables are found in the output of that initial detection. ACE then gets to work on those root observables. An ACE alert's status is complete when ACE is finished with its recursive analysis, correlation, discovery, and relational combination of observables. The result is an ACE alert with intuitive context ready for the analyst's consumption. The figure below is meant to give a visual representation ACE's recursive observable analysis and correlation. Recursive Observable Analysis ACE\u2019s recursive analysis of observables reduces and simplifies the analyst\u2019s workload by providing the analyst with as much available context as reasonably possible. A complete list of currently defined observable types can be viewed in the table below. Alert Dispositioning \u00b6 When investigating an alert, there is a categorization model for analysts to follow called dispositioning. No matter if an alert requires a response or not, analysts need to disposition them correctly. Sometimes, especially for true positive alerts that get escalated, more information may lead a change in an alert\u2019s disposition. The disposition model that ACE uses is based on Lockheed Martin's Cyber Kill Chain model for identifying and describing the stages of an adversary\u2019s attack. The table below describes each of the different dispositions used by ACE. +----------+-----------------------------------------------------------+ | Disposit | Description / Example | | ion | | +==========+===========================================================+ | FALSE\\_P | Something matched a detection signature, but that | | OSITIVE | something turned out to be nothing malicious. | | | | | | - A signature was designed to detect something | | | specific, and this wasn't it. | | | - A signature was designed in a broad manner and, after | | | analysis, what it detected turned out to be benign. | | | - A response is not required. | +----------+-----------------------------------------------------------+ | IGNORE | This alert should have never fired. A match was made on | | | something a detection was looking for but it was expected | | | or an error. | | | | | | - Security information was being transferred | | | - An error occurred in the detection software | | | generating invalid alerts | | | - Someone on the security team was testing something or | | | working on something | | | | | | It is important to make the distinction between | | | FALSE\\_POSITIVE and IGNORE dispositions, as alerts marked | | | FALSE\\_POSITIVE are used to tune detection signatures, | | | while alerts marked as IGNORE are not. IGNORE alerts are | | | deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | UNKNOWN | Not enough information is available to make a good | | | decision because of a lack of visibility. | +----------+-----------------------------------------------------------+ | REVIEWED | This is a special disposition to be used for alerts that | | | were manually generated for analysis or serve an | | | informational purpose. For example, if someone uploaded a | | | malware sample from a third party to ACE, you would set | | | the disposition to REVIEWED after reviewing the analysis | | | results. Alerts set to REVIEWED do not count for metrics | | | and are not deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | GRAYWARE | Software that is not inherently malicious but exhibits | | | potentially unwanted or obtrusive behavior. | | | | | | - Adware | | | - Spyware | | | | | | [If desired, this disposition can be used to categorize s | | | pam emails.]{.strike} | +----------+-----------------------------------------------------------+ | POLICY\\_ | In the course of an investigation, general risky user | | VIOLATIO | behavior or behavior against an official policy or | | N | standard is discovered. | | | | | | - Installing unsupported software | | | - Connecting a USB drive with pirated software | | | - Browsing to pornographic sites | +----------+-----------------------------------------------------------+ | RECONNAI | Catching the adversary planning, gathering intel, or | | SSANCE | researching what attacks may work against you. | | | | | | - Vulnerability and port scanning | | | - Attempts to establish trust with a user | +----------+-----------------------------------------------------------+ | WEAPONIZ | The detection of an attempt to build a cyber attack | | ATION | weapon. | | | | | | - Detecting an advesary building a malicious document | | | using VT threat hunting | +----------+-----------------------------------------------------------+ | DELIVERY | An attack was attempted, and the attack's destination was | | | reached. Even with no indication the attack worked. | | | | | | - A user browsed to an exploit kit | | | - A phish was delivered to the email inbox | | | - AV detected and remediated malware after the malware | | | was written to disk | +----------+-----------------------------------------------------------+ | EXPLOITA | An attack was DELIVERED and there is evidence that the | | TION | EXPLOITATION worked in whole or in part. | | | | | | - A user clicked on a malicious link from a phish | | | - A user opened and ran a malicious email attachment | | | - A user hit an exploit kit, a Flash exploit was | | | attempted | +----------+-----------------------------------------------------------+ | INSTALLA | An attack was DELIVERED and the attack resulted in the | | TION | INSTALLATION of something to maintain persistence on an | | | asset/endpoint/system. | | | | | | - A user browsed to an exploit kit and got malware | | | installed on their system | | | - A user executed a malicious email attachment and | | | malware was installed | | | - Malware executed off a USB and installed persistence | | | on an endpoint | +----------+-----------------------------------------------------------+ | COMMAND\\ | An attacker was able to communicate between their control | | _AND\\_CO | system and a compromised asset. The adversary has been | | NTROL | able to establish a control channel with an asset. | | | | | | Example Scenario: A phish is DELIVERED to an inbox, and a | | | user opens a malicious Word document that was attached. | | | The Word document EXPLOITS a vulnerability and leads to | | | the INSTALLATION of malware. The malware is able to | | | communicate back to the attackers COMMAND\\_AND\\_CONTROL | | | server. | +----------+-----------------------------------------------------------+ | EXFIL | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. EXFIL indicates the | | | loss of something important. | | | | | | - Adversaries steals information by uploading files to | | | their control server | | | - A user submits login credentials to a phishing | | | website | +----------+-----------------------------------------------------------+ | DAMAGE | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. DAMAGE indicates that | | | damage or disruption was made to an asset, the network, | | | the company, or business operations. | | | | | | - An attacker steals money by tricking an employee to | | | change the bank account number of a customer | | | - Ransomware encrypts multiple files on an asset | | | - PLC code is modified and warehouse equipment is | | | broken | | | - Process Control Systems are tampered with and a | | | facility must shutdown until repairs are made | | | - A public facing website is compromised and defaced or | | | serves malware to other victums | +----------+-----------------------------------------------------------+ | INSIDER\\ | Employee has the data and is attempting to send it out of | | _DATA\\_C | the bank. | | ONTROL | | +----------+-----------------------------------------------------------+ | INSIDER\\ | Sensitive data leaves the bank below privacy impact | | _DATA\\_E | threshold. | | XFIL | | +----------+-----------------------------------------------------------+ GUI Overview \u00b6 Analysts interact with ACE through its graphical interface and specifically use the Manage Alerts page. After you're logged into ACE (Assuming you already have an account), you'll see a navigation bar that looks like the following image. A a simple breakdown of each page on that navigation bar is provided below. Page Function ---- --------- Overview General ACE information, performance, statistics, etc. Manual Analysis Where analysts can manually upload or submit observables for ACE to analyze Manage Alerts The alert queue - where the magic happens Events Where events are managed Metrics For creating and tracking metrics from the data ACE generates Working Alerts \u00b6 This section covers the basics for working and managing ACE alerts. If you're comfortable, skip ahead to the Examples section to find a walkthrough of a few ACE alerts being worked. The Manage Alerts Page \u00b6 ACE alerts will queue up on the Manage Alerts page. By default, only alerts that are open (not dispositioned ) and not owned by another analyst are displayed. When working an alert, analysts should take ownership of it to prevent other analysts from starting to work on the same alert. This prevents re-work and saves analyst time. You can take ownership of one or more alerts on the Manage Alerts page by selecting alert checkboxes and clicking the 'Take Ownership' button. You can also take ownership when viewing an individual alert. Below is an example of the Manage Alerts page with 32 open and unowned alerts. Manage Alerts page Viewing Observable Summary \u00b6 On the Manage Alerts page, each alert can be expanded via its dropdown button. Once expanded, all the observables in the alert can be viewed. The observables are grouped and listed by their observable type. The numbers in parentheses show a count of how many times ACE has seen that observable. Each observable is clickable, and when clicked, ACE will add that observable to the current alert filter. You don't need to worry about alert filtering to work alerts, however, the Filtering and Grouping <filtering and grouping> section covers Alert filtering. Expand/Collapse Observables - email_address - fakeuser@fakecompany.com (21) - tfry@kennyross.com (2) - email_conversation - tfry@kennyross.com|fakeuser@fakecompany.com (1) - file - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/1LSZPI0TG6C82HTABETK.temp (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/Kenny_Ross_Inquiry.LNK (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/index.dat (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/urlref_httpvezopilan.comtstindex.phpl_soho7.tkn_.Split (1) - Kenny_Ross_Inquiry.doc (9) - Kenny_Ross_Inquiry.doc.officeparser/iYzcZYMdfv.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/oUDOGruwp.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_10_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_11_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_12_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_13_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_14_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_15_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_16_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_17_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_18_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_19_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_1_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIK1.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIKManager.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/_pPOR/WXRIKManager.lrA.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/[Content_Types].lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/_pPOR/.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_3_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_4_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_5_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_8_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_9_0.dat (2) - Kenny_Ross_Inquiry.doc.olevba/macro_0.bas (2) - Kenny_Ross_Inquiry.doc.olevba/macro_1.bas (2) - Kenny_Ross_Inquiry.doc.pcode.bas (2) - email.rfc822 (37952) - email.rfc822.headers (37949) - email.rfc822.unknown_text_html_000 (3229) - email.rfc822.unknown_text_html_000_000.png (2482) - email.rfc822.unknown_text_plain_000 (37354) - filename.PNG (11) - indicator - 55c36786bcb87f2d54cf15da (369) - 57ffd02cbcb87fbb1464b1ce (88) - 58c9708aad951d7387c65be2 (274) - 58e3e8dfad951d49aabb1622 (384) - 58ee209dad951d09a1ee3860 (92) - 58ee221dad951d09a0b13e99 (92) - 5937f5d4ad951d4fe8787c63 (672) - 599db056ad951d5cb2c4768b (302) - 599dd8abad951d5cb3204569 (155) - 59a7fcc7ad951d522eeef8ed (380) - ipv4 - 104.118.208.249 (24) - md5 - 2307a1a403c6326509d4d9546e5f32ab (2) - 267b1bd0ae8194781c373f93c9df02fa (2) - 39ee938f6fa351f94a2cbf8835bb454f (2) - 5c4c76cbb739c04fb3838aff5b2c25bb (2) - 65811d8f7c6a1b94eab03ba1072a3a7e (2) - b3b8bf4ed2c5cb26883661911487d642 (2) - d8a7ea6ba4ab9541e628452e2ad6014a (2) - message_id - <8de41f6eb57ac01b2a90d3466890b0a1@127.0.0.1> (1) - sha1 - 03484a568871d494ad144ac9597e9717a2ae5601 (2) - 2e3b95bb9b0beb5db3487646d772363004505df6 (2) - 33b9d3de33adc5bd5954c1e9f9e48f10eabe7c49 (2) - 62837876eb5ec321e6d8dbd6babd0d5789230b60 (2) - b3024c6f598b1745ca352ac3a24cc3603b814cad (2) - cfe4f07fbf042b4f7dce44f9e6e3f449e02c123a (2) - fa47ebc1026bbe8952f129480f38a011f9faf47d (2) - sha256 - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98 (2) - 50aef060b9192d5230be21df821acb4495f7dc90416b2edfd68ebebde40562be (2) - 62be2fe5e5ad79f62671ba4b846a63352d324bb693ee7c0f663f488e25f05fe0 (2) - 8159227eb654ef2f60eb4c575f4a218bb76919ea15fdd625c2d01d151e4973f3 (2) - 9c7e06164ec59e76d6f3e01fa0129607be1d98af270a09fd0f126ee8e16da306 (2) - ae67f33b6ff45aecf91ff6cac71b290c27f791ccbe4829be44bd64468cbe3f5d (2) - ca797ec10341aebaed1130c4dbf9a5b036945f17dd94d71d46f2f81d9937504f (2) - url - http://schemas.openxmlformats.org/drawingml/2006/main (3796) - user - fake_user_id (17) - yara_rule - CRITS_EmailContent (4478) - CRITS_StringOffice (1685) - CRITS_StringVBS (6592) - CRITS_StringWindowsShell (1770) - macro_code_snippet (1013) - macro_overused_legit_functions (82) Above, you can click to expand a text based example of an alerts observable structure when expanded on the Manage Alerts page. Filtering and Grouping \u00b6 On the manage-alerts-page, alerts are filtered by default to show open alerts that are not currently owned by any other analysts. The current filter state is always displayed at the top of the page, in a human readable format. You can select 'Edit Filters' to modify the alert filter and display alerts based on several different conditions. For example, you can change the filters to see alerts dispositioned as DELIVERY over the past seven days by a specific analyst. Alerts can also be filtered by observables. Conveniently, when viewing an alert's Observable Summary <observable-summary> on the Manage Alerts page, you can click any of those observables to add it to the currently defined alert filter. So, with the default filter applied, if you clicked on an MD5 observable with value 10EFE4369EA344308416FB59051D5947 then the page would refresh and you'd see that the new filter became: filter: open alerts AND not owned by others AND with observable type md5 value b'10EFE4369EA344308416FB59051D5947'` The Alert Page \u00b6 Once an alert is opened, the full analysis results will be displayed. It's usually a good idea to go ahead and view <analysis-views> all of the alert's analysis. Views \u00b6 There are two different modes in which you can view ACE alerts: 'Critical' and 'All'. By default, ACE alerts will be displayed in critical mode. Critical mode will only display 'root' level alert observable analysis. This is helpful for alerts with a lot of observables, although it's generally helpful to view all alert analysis. At the top right of every alert you will see a button to \"View All Analysis\" or \"View Critical Analysis\". Whichever mode you have enabled will be persistent across your ACE session. Be mindful of these different views, as it's possible for an analyst to miss helpful information if viewing an alert in critical mode compared to all mode. Analysis Overview \u00b6 Each standard ACE alert will have analysis overview section where the analysis results for every Observable <observable> will be found. The observables displayed at the 'root' level are the ones that were directly discovered in the data provided to ACE at the time of the alert's creation. Underneath each observable you will find the analysis results for that respective observable. You may also find new observables that were added to the alert from the recursive analysis of other observables. This observable nesting on the alert page provides a visual representation of how alert observables are related. The figure below shows the analysis overview section of an ACE Mailbox (email) alert. You can see that a user observable of value 'fake-user-id' was discovered from the analysis results of the email_address Observable. Alert Tags \u00b6 ACE has a tagging system by which observables and analysis are tagged for the purpose of providing additional context. If you review the previous figure of manage-alerts-page, you will notice tags such as phish, new_sender, and frequent_conversation associated to various alerts. All observable tags get associated with their respective alert and show up on the alert management page. Any observable can be tagged and can have any number of tags. For instance, an email conversation between two addresses that ACE has seen a lot will be tagged as 'frequent_conversation'. Tags can also be added directly to alerts from the Manage Alerts page. This can be helpful for Filtering and Grouping alerts if an analyst needs a way to group alerts that don\u2019t otherwise have a commonly shared tag or observable. Examples \u00b6 The following are examples of a snarky analyst working ACE alerts. Think about the first intuition you get from what you see in these alerts. Check out this Email Alert \u00b6 We just got this alert in the queue. Huh, looks like this email might be related to a potentially malicious zip file. Let's open the alert and look at the Analysis Overview section to see the results ACE brought us. In the case of email alerts like this one, the 'email.rfc882' file is what ACE was given when told to create this alert. Under that email.rfc882 file observable you will see the output of the Email Analysis module, and underneath Email Analysis you will see where ACE discovered more observables, such as the email addresses. With respect to this alert, ACE conveniently rendered us a visual image of the email's HTML body. That rendering lets us quickly see that the sender is thanking the recipient for their purchase. It seems doubtful to me that the user really purchased anything, so this email seems awfully suspicious. Note that we can also view or download that 'email.rfc822.unknown_text_html_000' file by using the dropdown next to it. Scrolling down on the same alert from the example above, we see the \u2018URL Extraction Analysis\u2019 found some URL observables. Moreover, we see that ACE found additional observables in the analysis output of those URL observables. Specifically, ACE downloaded that '66524177012457.zip' file and extracted it to reveal an executable named '66524177012457.exe'. Hm, this email doesn't seem friendly at all. Perhaps that malicious tag was onto something... where did that tag come from? Oh, it's next to the MD5 observable of the file, which I know ACE checks VirusTotal for, and one of the analysis results under that MD5 observable shows the VT result summary. Got it. Definitely malicious. Someone should do something about this. We got another Email Alert \u00b6 We just got this email alert. Judging by the tags, I'm assuming an office document is attached. It's probably an open xml office document too, since the zip tag is present. I\u2019m assuming this because I know open xml office documents are zipped up. Of course, there could be a stand-alone zip file in the email too. Let's look and see. When we open the alert, we see the alert header at the top. Hmm, this email alert only has one detection. Either this is really good phish and something we barely catch, or it's a false positive. Let's scroll down and find that single detection. Oh, I just noticed that we're only viewing this alert's critical analysis. We could click on the \"View All Analysis\" button if we wanted to view all of its analysis results. However, for this alert, the critical view makes it easy to find the single detection. Detections are marked by a little red flame icon. Here we see that the flame is highlighting a yara rule that detected something in the analysis of the \"Glenn Resume.docx\" file. Speaking of that file, we were right about assuming it was an open xml office document. Look at this, ACE tagged the rels_remote_references yara rule with high_fp_frequency. That tells us that this specific yara rule has a high frequency of showing up in false positive alerts. Below the rule, we see that the \"Malicious Frequency Analysis\" module found the rels_remote_references yara rule only appeared in four true-positive alerts out of two hundred and ten! I don't know about you, but my gut is telling me this email alert is a false positive. Let's make sure though and click to view the \"Yara Scan Results\". Above we can see what the yara rule detected in this docx file. And what do we see? A target reference to a file, and when looking closer we see that the file being referenced was named \"Resume Template.dotm\". I bet this dotm file is a leftover artifact from Glenn using a resume template when creating this \"Glenn Resume.docx\" file. I'm already clicking the \"Disposition\" button and marking this alert FALSE_POSITIVE. Now that we've reviewed this email alert, I want to harp on how QUICKLY we should be able to disposition it. If you\u2019re curious, the rels_remote_references yara rule was created to detect references to URLs or files in an open xml document's template. Such references can and have been malicious. An example would be a Microsoft Word document that references a URL and causes word to display an authentication dialog to the end-user for the purpose of harvesting the user\u2019s credentials. This repository contains a GO script that makes it easy to do that very thing: https://github.com/ryhanson/phishery","title":"User Guide"},{"location":"user/#user-guide","text":"Keep this in mind when working ACE alerts: ACE is meant to enable the analyst to QUICKLY disposition false positive alerts and recognize true positives. For convenience, here is a video recording that provides a tour of the ACE GUI and demonstrates how to work some ACE alerts. Many of the concepts in this orientation are covered.","title":"User Guide"},{"location":"user/#quick-concept-touchpoint","text":"There are two core concepts an analyst must be familiar with when working ACE alerts: Observables and Dispositioning.","title":"Quick Concept Touchpoint"},{"location":"user/#observables","text":"Observables are anything an analyst might \"observe\" or take note of during an investigation or when performing Alert Triage. For instance, an IP address is an observable, and a file name is a different type of observable. Some more observable types are: URLs, domain names, usernames, file hashes, file names, file paths, email addresses, and Yara signatures. ACE knows what kind of analysis to perform for a given observable type and how to correlate the value of an observable across all available data sources. In the process of correlating observables with other data sources, ACE will discover more observables to analyze and correlate. When an ACE alert is created from an initial detection point, the alert's 'root' level observables are found in the output of that initial detection. ACE then gets to work on those root observables. An ACE alert's status is complete when ACE is finished with its recursive analysis, correlation, discovery, and relational combination of observables. The result is an ACE alert with intuitive context ready for the analyst's consumption. The figure below is meant to give a visual representation ACE's recursive observable analysis and correlation. Recursive Observable Analysis ACE\u2019s recursive analysis of observables reduces and simplifies the analyst\u2019s workload by providing the analyst with as much available context as reasonably possible. A complete list of currently defined observable types can be viewed in the table below.","title":"Observables"},{"location":"user/#alert-dispositioning","text":"When investigating an alert, there is a categorization model for analysts to follow called dispositioning. No matter if an alert requires a response or not, analysts need to disposition them correctly. Sometimes, especially for true positive alerts that get escalated, more information may lead a change in an alert\u2019s disposition. The disposition model that ACE uses is based on Lockheed Martin's Cyber Kill Chain model for identifying and describing the stages of an adversary\u2019s attack. The table below describes each of the different dispositions used by ACE. +----------+-----------------------------------------------------------+ | Disposit | Description / Example | | ion | | +==========+===========================================================+ | FALSE\\_P | Something matched a detection signature, but that | | OSITIVE | something turned out to be nothing malicious. | | | | | | - A signature was designed to detect something | | | specific, and this wasn't it. | | | - A signature was designed in a broad manner and, after | | | analysis, what it detected turned out to be benign. | | | - A response is not required. | +----------+-----------------------------------------------------------+ | IGNORE | This alert should have never fired. A match was made on | | | something a detection was looking for but it was expected | | | or an error. | | | | | | - Security information was being transferred | | | - An error occurred in the detection software | | | generating invalid alerts | | | - Someone on the security team was testing something or | | | working on something | | | | | | It is important to make the distinction between | | | FALSE\\_POSITIVE and IGNORE dispositions, as alerts marked | | | FALSE\\_POSITIVE are used to tune detection signatures, | | | while alerts marked as IGNORE are not. IGNORE alerts are | | | deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | UNKNOWN | Not enough information is available to make a good | | | decision because of a lack of visibility. | +----------+-----------------------------------------------------------+ | REVIEWED | This is a special disposition to be used for alerts that | | | were manually generated for analysis or serve an | | | informational purpose. For example, if someone uploaded a | | | malware sample from a third party to ACE, you would set | | | the disposition to REVIEWED after reviewing the analysis | | | results. Alerts set to REVIEWED do not count for metrics | | | and are not deleted by cleanup routines. | +----------+-----------------------------------------------------------+ | GRAYWARE | Software that is not inherently malicious but exhibits | | | potentially unwanted or obtrusive behavior. | | | | | | - Adware | | | - Spyware | | | | | | [If desired, this disposition can be used to categorize s | | | pam emails.]{.strike} | +----------+-----------------------------------------------------------+ | POLICY\\_ | In the course of an investigation, general risky user | | VIOLATIO | behavior or behavior against an official policy or | | N | standard is discovered. | | | | | | - Installing unsupported software | | | - Connecting a USB drive with pirated software | | | - Browsing to pornographic sites | +----------+-----------------------------------------------------------+ | RECONNAI | Catching the adversary planning, gathering intel, or | | SSANCE | researching what attacks may work against you. | | | | | | - Vulnerability and port scanning | | | - Attempts to establish trust with a user | +----------+-----------------------------------------------------------+ | WEAPONIZ | The detection of an attempt to build a cyber attack | | ATION | weapon. | | | | | | - Detecting an advesary building a malicious document | | | using VT threat hunting | +----------+-----------------------------------------------------------+ | DELIVERY | An attack was attempted, and the attack's destination was | | | reached. Even with no indication the attack worked. | | | | | | - A user browsed to an exploit kit | | | - A phish was delivered to the email inbox | | | - AV detected and remediated malware after the malware | | | was written to disk | +----------+-----------------------------------------------------------+ | EXPLOITA | An attack was DELIVERED and there is evidence that the | | TION | EXPLOITATION worked in whole or in part. | | | | | | - A user clicked on a malicious link from a phish | | | - A user opened and ran a malicious email attachment | | | - A user hit an exploit kit, a Flash exploit was | | | attempted | +----------+-----------------------------------------------------------+ | INSTALLA | An attack was DELIVERED and the attack resulted in the | | TION | INSTALLATION of something to maintain persistence on an | | | asset/endpoint/system. | | | | | | - A user browsed to an exploit kit and got malware | | | installed on their system | | | - A user executed a malicious email attachment and | | | malware was installed | | | - Malware executed off a USB and installed persistence | | | on an endpoint | +----------+-----------------------------------------------------------+ | COMMAND\\ | An attacker was able to communicate between their control | | _AND\\_CO | system and a compromised asset. The adversary has been | | NTROL | able to establish a control channel with an asset. | | | | | | Example Scenario: A phish is DELIVERED to an inbox, and a | | | user opens a malicious Word document that was attached. | | | The Word document EXPLOITS a vulnerability and leads to | | | the INSTALLATION of malware. The malware is able to | | | communicate back to the attackers COMMAND\\_AND\\_CONTROL | | | server. | +----------+-----------------------------------------------------------+ | EXFIL | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. EXFIL indicates the | | | loss of something important. | | | | | | - Adversaries steals information by uploading files to | | | their control server | | | - A user submits login credentials to a phishing | | | website | +----------+-----------------------------------------------------------+ | DAMAGE | A form of **action on objectives** where an objective is | | | an adversaries goal for attacking. DAMAGE indicates that | | | damage or disruption was made to an asset, the network, | | | the company, or business operations. | | | | | | - An attacker steals money by tricking an employee to | | | change the bank account number of a customer | | | - Ransomware encrypts multiple files on an asset | | | - PLC code is modified and warehouse equipment is | | | broken | | | - Process Control Systems are tampered with and a | | | facility must shutdown until repairs are made | | | - A public facing website is compromised and defaced or | | | serves malware to other victums | +----------+-----------------------------------------------------------+ | INSIDER\\ | Employee has the data and is attempting to send it out of | | _DATA\\_C | the bank. | | ONTROL | | +----------+-----------------------------------------------------------+ | INSIDER\\ | Sensitive data leaves the bank below privacy impact | | _DATA\\_E | threshold. | | XFIL | | +----------+-----------------------------------------------------------+","title":"Alert Dispositioning"},{"location":"user/#gui-overview","text":"Analysts interact with ACE through its graphical interface and specifically use the Manage Alerts page. After you're logged into ACE (Assuming you already have an account), you'll see a navigation bar that looks like the following image. A a simple breakdown of each page on that navigation bar is provided below. Page Function ---- --------- Overview General ACE information, performance, statistics, etc. Manual Analysis Where analysts can manually upload or submit observables for ACE to analyze Manage Alerts The alert queue - where the magic happens Events Where events are managed Metrics For creating and tracking metrics from the data ACE generates","title":"GUI Overview"},{"location":"user/#working-alerts","text":"This section covers the basics for working and managing ACE alerts. If you're comfortable, skip ahead to the Examples section to find a walkthrough of a few ACE alerts being worked.","title":"Working Alerts"},{"location":"user/#the-manage-alerts-page","text":"ACE alerts will queue up on the Manage Alerts page. By default, only alerts that are open (not dispositioned ) and not owned by another analyst are displayed. When working an alert, analysts should take ownership of it to prevent other analysts from starting to work on the same alert. This prevents re-work and saves analyst time. You can take ownership of one or more alerts on the Manage Alerts page by selecting alert checkboxes and clicking the 'Take Ownership' button. You can also take ownership when viewing an individual alert. Below is an example of the Manage Alerts page with 32 open and unowned alerts. Manage Alerts page","title":"The Manage Alerts Page"},{"location":"user/#viewing-observable-summary","text":"On the Manage Alerts page, each alert can be expanded via its dropdown button. Once expanded, all the observables in the alert can be viewed. The observables are grouped and listed by their observable type. The numbers in parentheses show a count of how many times ACE has seen that observable. Each observable is clickable, and when clicked, ACE will add that observable to the current alert filter. You don't need to worry about alert filtering to work alerts, however, the Filtering and Grouping <filtering and grouping> section covers Alert filtering. Expand/Collapse Observables - email_address - fakeuser@fakecompany.com (21) - tfry@kennyross.com (2) - email_conversation - tfry@kennyross.com|fakeuser@fakecompany.com (1) - file - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/1LSZPI0TG6C82HTABETK.temp (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/Kenny_Ross_Inquiry.LNK (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/index.dat (1) - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98.vxstream/dropped/urlref_httpvezopilan.comtstindex.phpl_soho7.tkn_.Split (1) - Kenny_Ross_Inquiry.doc (9) - Kenny_Ross_Inquiry.doc.officeparser/iYzcZYMdfv.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/oUDOGruwp.bas (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_10_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_11_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_12_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_13_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_14_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_15_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_16_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_17_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_18_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_19_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_1_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIK1.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/WXRIKManager.lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/WXRIK/WXRIK/_pPOR/WXRIKManager.lrA.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/[Content_Types].lrA (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_2_0.dat.extracted/_pPOR/.pPOR (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_3_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_4_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_5_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_8_0.dat (2) - Kenny_Ross_Inquiry.doc.officeparser/stream_9_0.dat (2) - Kenny_Ross_Inquiry.doc.olevba/macro_0.bas (2) - Kenny_Ross_Inquiry.doc.olevba/macro_1.bas (2) - Kenny_Ross_Inquiry.doc.pcode.bas (2) - email.rfc822 (37952) - email.rfc822.headers (37949) - email.rfc822.unknown_text_html_000 (3229) - email.rfc822.unknown_text_html_000_000.png (2482) - email.rfc822.unknown_text_plain_000 (37354) - filename.PNG (11) - indicator - 55c36786bcb87f2d54cf15da (369) - 57ffd02cbcb87fbb1464b1ce (88) - 58c9708aad951d7387c65be2 (274) - 58e3e8dfad951d49aabb1622 (384) - 58ee209dad951d09a1ee3860 (92) - 58ee221dad951d09a0b13e99 (92) - 5937f5d4ad951d4fe8787c63 (672) - 599db056ad951d5cb2c4768b (302) - 599dd8abad951d5cb3204569 (155) - 59a7fcc7ad951d522eeef8ed (380) - ipv4 - 104.118.208.249 (24) - md5 - 2307a1a403c6326509d4d9546e5f32ab (2) - 267b1bd0ae8194781c373f93c9df02fa (2) - 39ee938f6fa351f94a2cbf8835bb454f (2) - 5c4c76cbb739c04fb3838aff5b2c25bb (2) - 65811d8f7c6a1b94eab03ba1072a3a7e (2) - b3b8bf4ed2c5cb26883661911487d642 (2) - d8a7ea6ba4ab9541e628452e2ad6014a (2) - message_id - <8de41f6eb57ac01b2a90d3466890b0a1@127.0.0.1> (1) - sha1 - 03484a568871d494ad144ac9597e9717a2ae5601 (2) - 2e3b95bb9b0beb5db3487646d772363004505df6 (2) - 33b9d3de33adc5bd5954c1e9f9e48f10eabe7c49 (2) - 62837876eb5ec321e6d8dbd6babd0d5789230b60 (2) - b3024c6f598b1745ca352ac3a24cc3603b814cad (2) - cfe4f07fbf042b4f7dce44f9e6e3f449e02c123a (2) - fa47ebc1026bbe8952f129480f38a011f9faf47d (2) - sha256 - 308591a9db1d3b8739e53feaf3dd5ba069f7191125cf3bb7e2c849bad2182e98 (2) - 50aef060b9192d5230be21df821acb4495f7dc90416b2edfd68ebebde40562be (2) - 62be2fe5e5ad79f62671ba4b846a63352d324bb693ee7c0f663f488e25f05fe0 (2) - 8159227eb654ef2f60eb4c575f4a218bb76919ea15fdd625c2d01d151e4973f3 (2) - 9c7e06164ec59e76d6f3e01fa0129607be1d98af270a09fd0f126ee8e16da306 (2) - ae67f33b6ff45aecf91ff6cac71b290c27f791ccbe4829be44bd64468cbe3f5d (2) - ca797ec10341aebaed1130c4dbf9a5b036945f17dd94d71d46f2f81d9937504f (2) - url - http://schemas.openxmlformats.org/drawingml/2006/main (3796) - user - fake_user_id (17) - yara_rule - CRITS_EmailContent (4478) - CRITS_StringOffice (1685) - CRITS_StringVBS (6592) - CRITS_StringWindowsShell (1770) - macro_code_snippet (1013) - macro_overused_legit_functions (82) Above, you can click to expand a text based example of an alerts observable structure when expanded on the Manage Alerts page.","title":"Viewing Observable Summary"},{"location":"user/#filtering-and-grouping","text":"On the manage-alerts-page, alerts are filtered by default to show open alerts that are not currently owned by any other analysts. The current filter state is always displayed at the top of the page, in a human readable format. You can select 'Edit Filters' to modify the alert filter and display alerts based on several different conditions. For example, you can change the filters to see alerts dispositioned as DELIVERY over the past seven days by a specific analyst. Alerts can also be filtered by observables. Conveniently, when viewing an alert's Observable Summary <observable-summary> on the Manage Alerts page, you can click any of those observables to add it to the currently defined alert filter. So, with the default filter applied, if you clicked on an MD5 observable with value 10EFE4369EA344308416FB59051D5947 then the page would refresh and you'd see that the new filter became: filter: open alerts AND not owned by others AND with observable type md5 value b'10EFE4369EA344308416FB59051D5947'`","title":"Filtering and Grouping"},{"location":"user/#the-alert-page","text":"Once an alert is opened, the full analysis results will be displayed. It's usually a good idea to go ahead and view <analysis-views> all of the alert's analysis.","title":"The Alert Page"},{"location":"user/#views","text":"There are two different modes in which you can view ACE alerts: 'Critical' and 'All'. By default, ACE alerts will be displayed in critical mode. Critical mode will only display 'root' level alert observable analysis. This is helpful for alerts with a lot of observables, although it's generally helpful to view all alert analysis. At the top right of every alert you will see a button to \"View All Analysis\" or \"View Critical Analysis\". Whichever mode you have enabled will be persistent across your ACE session. Be mindful of these different views, as it's possible for an analyst to miss helpful information if viewing an alert in critical mode compared to all mode.","title":"Views"},{"location":"user/#analysis-overview","text":"Each standard ACE alert will have analysis overview section where the analysis results for every Observable <observable> will be found. The observables displayed at the 'root' level are the ones that were directly discovered in the data provided to ACE at the time of the alert's creation. Underneath each observable you will find the analysis results for that respective observable. You may also find new observables that were added to the alert from the recursive analysis of other observables. This observable nesting on the alert page provides a visual representation of how alert observables are related. The figure below shows the analysis overview section of an ACE Mailbox (email) alert. You can see that a user observable of value 'fake-user-id' was discovered from the analysis results of the email_address Observable.","title":"Analysis Overview"},{"location":"user/#alert-tags","text":"ACE has a tagging system by which observables and analysis are tagged for the purpose of providing additional context. If you review the previous figure of manage-alerts-page, you will notice tags such as phish, new_sender, and frequent_conversation associated to various alerts. All observable tags get associated with their respective alert and show up on the alert management page. Any observable can be tagged and can have any number of tags. For instance, an email conversation between two addresses that ACE has seen a lot will be tagged as 'frequent_conversation'. Tags can also be added directly to alerts from the Manage Alerts page. This can be helpful for Filtering and Grouping alerts if an analyst needs a way to group alerts that don\u2019t otherwise have a commonly shared tag or observable.","title":"Alert Tags"},{"location":"user/#examples","text":"The following are examples of a snarky analyst working ACE alerts. Think about the first intuition you get from what you see in these alerts.","title":"Examples"},{"location":"user/#check-out-this-email-alert","text":"We just got this alert in the queue. Huh, looks like this email might be related to a potentially malicious zip file. Let's open the alert and look at the Analysis Overview section to see the results ACE brought us. In the case of email alerts like this one, the 'email.rfc882' file is what ACE was given when told to create this alert. Under that email.rfc882 file observable you will see the output of the Email Analysis module, and underneath Email Analysis you will see where ACE discovered more observables, such as the email addresses. With respect to this alert, ACE conveniently rendered us a visual image of the email's HTML body. That rendering lets us quickly see that the sender is thanking the recipient for their purchase. It seems doubtful to me that the user really purchased anything, so this email seems awfully suspicious. Note that we can also view or download that 'email.rfc822.unknown_text_html_000' file by using the dropdown next to it. Scrolling down on the same alert from the example above, we see the \u2018URL Extraction Analysis\u2019 found some URL observables. Moreover, we see that ACE found additional observables in the analysis output of those URL observables. Specifically, ACE downloaded that '66524177012457.zip' file and extracted it to reveal an executable named '66524177012457.exe'. Hm, this email doesn't seem friendly at all. Perhaps that malicious tag was onto something... where did that tag come from? Oh, it's next to the MD5 observable of the file, which I know ACE checks VirusTotal for, and one of the analysis results under that MD5 observable shows the VT result summary. Got it. Definitely malicious. Someone should do something about this.","title":"Check out this Email Alert"},{"location":"user/#we-got-another-email-alert","text":"We just got this email alert. Judging by the tags, I'm assuming an office document is attached. It's probably an open xml office document too, since the zip tag is present. I\u2019m assuming this because I know open xml office documents are zipped up. Of course, there could be a stand-alone zip file in the email too. Let's look and see. When we open the alert, we see the alert header at the top. Hmm, this email alert only has one detection. Either this is really good phish and something we barely catch, or it's a false positive. Let's scroll down and find that single detection. Oh, I just noticed that we're only viewing this alert's critical analysis. We could click on the \"View All Analysis\" button if we wanted to view all of its analysis results. However, for this alert, the critical view makes it easy to find the single detection. Detections are marked by a little red flame icon. Here we see that the flame is highlighting a yara rule that detected something in the analysis of the \"Glenn Resume.docx\" file. Speaking of that file, we were right about assuming it was an open xml office document. Look at this, ACE tagged the rels_remote_references yara rule with high_fp_frequency. That tells us that this specific yara rule has a high frequency of showing up in false positive alerts. Below the rule, we see that the \"Malicious Frequency Analysis\" module found the rels_remote_references yara rule only appeared in four true-positive alerts out of two hundred and ten! I don't know about you, but my gut is telling me this email alert is a false positive. Let's make sure though and click to view the \"Yara Scan Results\". Above we can see what the yara rule detected in this docx file. And what do we see? A target reference to a file, and when looking closer we see that the file being referenced was named \"Resume Template.dotm\". I bet this dotm file is a leftover artifact from Glenn using a resume template when creating this \"Glenn Resume.docx\" file. I'm already clicking the \"Disposition\" button and marking this alert FALSE_POSITIVE. Now that we've reviewed this email alert, I want to harp on how QUICKLY we should be able to disposition it. If you\u2019re curious, the rels_remote_references yara rule was created to detect references to URLs or files in an open xml document's template. Such references can and have been malicious. An example would be a Microsoft Word document that references a URL and causes word to display an authentication dialog to the end-user for the purpose of harvesting the user\u2019s credentials. This repository contains a GO script that makes it easy to do that very thing: https://github.com/ryhanson/phishery","title":"We got another Email Alert"},{"location":"user/hunters_guide/","text":"Hunters Guide \u00b6 A hunt can be added, modified or deleted while the service is running. ACE detects these changes and reloads the hunts as needed. A hunt can be manually executed by using the execute sub-command of the hunt command. ace hunt execute --help For example, to execute the query_stuff hunt in the splunk hunting system you would issue the following command. ace hunt execute -s 04/17/2020:00:00:00 -e /04/18/2020:00:00:00 -z US/Eastern splunk:query_stuff By default what gets displayed is a list of the alerts that would have been generated. There are additional options to display more details of the alerts.","title":"Hunters Guide"},{"location":"user/hunters_guide/#hunters-guide","text":"A hunt can be added, modified or deleted while the service is running. ACE detects these changes and reloads the hunts as needed. A hunt can be manually executed by using the execute sub-command of the hunt command. ace hunt execute --help For example, to execute the query_stuff hunt in the splunk hunting system you would issue the following command. ace hunt execute -s 04/17/2020:00:00:00 -e /04/18/2020:00:00:00 -z US/Eastern splunk:query_stuff By default what gets displayed is a list of the alerts that would have been generated. There are additional options to display more details of the alerts.","title":"Hunters Guide"}]}